\chapter{Procedimiento de optimización de redes neuronales}

\section{Procedimiento}

A continuación, se explican los pasos seguidos que permitieron obtener los resultados de la sección \ref{sec:modelic}. Los pasos se siguen como en \cite{tutos}.

\subsection{Conversión a ONNX}

Los modelos del proyecto se encuentran en diferentes formatos, existen integraciones para utilizar los modelos directamente de TensorFlow y de PyTorch. Sin embargo, se ha optado por usar exclusivamente el formato \acrshort{onnx}. Su principal desventaja frente a otros formatos son la incompatibilidad de ciertas capas de redes neuronales. En cambio, es la solución que otorga el mayor rendimiento. Según el formato se han aplicado las siguientes herramientas en la exportación:

\begin{itemize}
    \item \textbf{PyTorch}: la API de Python ya proporciona una herramienta (torch.onnx.export).
    \item \textbf{TensorFlow}: se usa la librería de Python \textbf{tf2onnx}.
\end{itemize}

\subsection{Conversión a TensorRT engine}
Los archivos \textit{engine} de TensorRT (extensión .trt o .engine) representan los modelos finales optimizados por dicha herramienta. El proceso de conversión se puede realizar a través de la API de Python o C++ o mediante el ejecutable \textit{trtexec}, TODO: que además devuelve resultados del \gls{profiling} de los modelos. Según la distribución del software,  el ejecutable \textit{trtexec} se encuentra en rutas distintas o ni siquiera se encuentra instalado, por lo que una forma más cómoda y general es usar la API.

\input{contenido/codes/trtshapes.tex}

En \ref{coud:trtshapes} se muestran las características de un modelo transformado en un \textit{engine} de TensorRT. La aparición de un -1 en la primera dimensión de los tensores indica que el modelo soporta \textit{\glspl{batch}} de tamaño dinámico.

\subsection{Códigos de inferencia}

El último paso es emplear los modelos obtenidos. La estructura de los códigos parte de los ya creados en \cite{andrew} para los modelos de este proyecto, salvo por el paso de transferir el trabajo a la \acrshort{gpu}. PyCUDA ofrece una API que facilita gran parte de la interacción con CUDA, aun así, se necesita gestionar en el código temas como la creación de contextos y reserva de la memoria.

TODO: comentar el código.

\subsection{Características de los modelos}
Hablar de los 4 modelos que usamos

\subsection{Compilación de OpenCV con soporte para CUDA}

En \cite{andrew} se utiliza una versión del modelo YuNet otorgada por la librería OpenCV, esta versión abstrae al programador de implementar las fases de pre y postprocesado de dicho modelo, que para este caso resultan muy complejas de implementar. Como ya se comentó en la sección \ref{sec:sw}, OpenCV dispone de soporte para CUDA, aunque es necesario compilar el programa desde el fuente. El proceso de compilación seguido es el mismo que en \cite{OpenCVCUDA}.

TODO: algo más.