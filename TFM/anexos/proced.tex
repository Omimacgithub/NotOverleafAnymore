\chapter{Flujos de trabajo de TensorRT}
\label{chap:procedure}

\lettrine{E}{n} este apéndice se comentan detalles acerca del procedimiento para la conversión de los modelos utilizados a TensorRT.

\section{Introducción}

A la hora de realizar inferencias en el \textit{framework} de TensorRT, existen diferentes integraciones o flujos de trabajo (\textit{workflows}) que se exponen a continuación:

\begin{description}
    \item[TensorFlow/PyTorch-TensorRT] Las integraciones de TensorRT con PyTorch (Torch-TensorRT) y TensorFlow (TF-TRT) permiten aprovechar directamente las funcionalidades de TensorRT a través de los frameworks mencionados. Ambas integraciones parten el grafo generado del modelo en varios subgrafos para su optimización y ejecución en TensorRT. Los subgrafos generados que no son compatibles con TensorRT pasan a ejecutarse en los frameworks correspondientes. Esta aproximación no permite exprimir al máximo las capacidades del hardware por lo que, aunque es una solución sencilla, no resulta del todo eficiente.
    \item[\acrshort{onnx}/\acrshort{onnx} Runtime-TensorRT] Esta aproximación requiere de la exportación de los modelos al formato \acrshort{onnx} para su uso con la \acrshort{api} de TensorRT o \acrshort{onnx} Runtime.  Si se emplea la \acrshort{api} de TensorRT, es necesario reservar la memoria de los dispositivos y planificar la ejecución del trabajo en la \acrshort{gpu} por parte del programador \cite{trtworkflows}. En cambio, \acrshort{onnx} Runtime ofrece una mayor abstracción de este proceso, ya que dicha \acrshort{api} interacciona directamente con TensorRT por medio de los \textbf{Execution Providers (EP)}, que hace uso de su \gls{rt} para ejecutar los subgrafos en el hardware \cite{EP,trtworkflows}.
\end{description}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.75\linewidth]{imagenes/trtworkflows.png}
    \caption{Flujos según el \textit{workflow} de TensorRT, figura extraída de \cite{trtworkflows}.}
    \label{fig:trtworkflows}
\end{figure}

La figura \ref{fig:trtworkflows} muestra el flujo seguido para ejecutar inferencias en cada uno de los \textit{workflows} comentados. Se ha optado por usar el \textit{workflow} de \textbf{\acrshort{onnx}-TensorRT}, ya que es el que mayor rendimiento otorga, además de un uso eficiente de la memoria de la \acrshort{gpu}, superior incluso a utilizar \acrshort{onnx} Runtime \cite{trtworkflows}. La única desventaja frente a utilizar los \textit{frameworks} de Tensorflow yPyTorch sería la incompatibilidad de ciertas capas del modelo en la fase de exportación al formato \acrshort{onnx}.

\section{Procedimiento}

A continuación, se explican los pasos seguidos (tomando \cite{tutos} como referencia) para la ejecución de los modelos en el \textit{workflow} de \acrshort{onnx}-TensorRT.

\subsection{Conversión a ONNX}

Según el formato se han aplicado las siguientes herramientas en la exportación a \acrshort{onnx}:

\begin{itemize}
    \item \textbf{PyTorch}: la API de Python ya proporciona una herramienta (torch.onnx.export(), como se muestra en la figura \ref{fig:trtworkflows}).
    \item \textbf{TensorFlow}: se usa la librería de Python \textbf{tf2onnx}.
\end{itemize}

\subsection{Conversión a TensorRT engine}
Los archivos \textit{engine} de TensorRT (extensión .trt o .engine) representan los modelos finales optimizados por dicha herramienta. El proceso de conversión se puede realizar a través de la API de Python/C++ o mediante el ejecutable \textit{trtexec}, este último analiza el rendimiento del modelo ejecutando una serie de inferencias y devuelve los resultados de \gls{profiling} del mismo. Según la imagen de Docker que se utilize, el ejecutable \textit{trtexec} no se encuentra instalado (las imágenes de TensorRT pensadas para la Jetson suelen tenerlo), en ese caso es necesario utilizar la API para convertir los modelos.

\input{contenido/codes/trtshapes.tex}

En \ref{coud:trtshapes} se muestran las características de un modelo transformado en un \textit{engine} de TensorRT. La aparición de un -1 en la primera dimensión de los tensores indica que el modelo soporta \textit{\glspl{batch}} de tamaño dinámico.

\subsection{Flags de \textit{trtexec}}

A la hora de compilar algunos de los modelos a partir de la herramienta \textit{trtexec}, se han aplicado los siguientes flags:
\begin{itemize}
    \item --fp16: activa el uso de tácticas \acrshort{fp}32 y \acrshort{fp}16 en cada capa del modelo (se escoge la precisión que se ejecute más rápido). Se aplica en todos los dispositivos Jetson.
    \item --sparsity: activa la aceleración de operaciones de matrices dispersas, a partir de la arquitectura Ampere.
    \item --tacticSources=+CUBLAS,+CUDNN: emplea el uso de tácticas de las librerías cuBLAS y cuDNN. Se aplica en todos los dispositivos Jetson.
    \item --builderOptimizationLevel: por defecto utiliza el valor 3, este valor se mantiene en todas las Jetson salvo en un caso de la Jetson AGX Thor (ver sección \ref{sec:discuss}).
\end{itemize}

\section{Exportación de YOLO con ultralytics}
\label{sec:exports}

La \acrshort{api} de ultralytics permite la exportación de modelos YOLO con una gran variedad de ajustes y conversión a diferentes formatos como \acrshort{onnx} o incluso los utilizados por los \gls{rt} de OpenVINO, TensorRT, entre otros.

\input{contenido/codes/YOLOexport.tex}

El código \ref{coud:yolexport} se muestra un ejemplo de como exportar el modelo YOLO de ultralytics. Se descarga el archivo de pesos (.pt) y se exporta a \acrshort{onnx}, con un tamaño de batch de 1 y precisión de 16 bits de punto flotante (flag half).