\chapter{Procedimiento de optimización de redes neuronales}
\label{chap:procedure}

\section{Procedimiento}

A continuación, se explican los pasos seguidos que permitieron obtener los resultados de la sección \ref{sec:modelic}. Los pasos se siguen como en \cite{tutos}.

\subsection{Conversión a ONNX}

Los modelos del proyecto se encuentran en diferentes formatos, existen integraciones para utilizar los modelos directamente de TensorFlow y de PyTorch. Sin embargo, se ha optado por usar exclusivamente el formato \acrfull{onnx}.

La principal desventaja de \acrshort{onnx} frente a otros formatos es la incompatibilidad de ciertas capas de redes neuronales. En cambio, es la solución que otorga el mayor rendimiento. Según el formato se han aplicado las siguientes herramientas en la exportación a \acrshort{onnx}:

\begin{itemize}
    \item \textbf{PyTorch}: la API de Python ya proporciona una herramienta (torch.onnx.export).
    \item \textbf{TensorFlow}: se usa la librería de Python \textbf{tf2onnx}.
\end{itemize}

\subsection{Conversión a TensorRT engine}
Los archivos \textit{engine} de TensorRT (extensión .trt o .engine) representan los modelos finales optimizados por dicha herramienta. El proceso de conversión se puede realizar a través de la API de Python/C++ o mediante el ejecutable \textit{trtexec}, este último analiza el rendimiento del modelo ejecutando una serie de inferencias y devuelve los resultados de \gls{profiling} del mismo. Según la distribución del software (TODO: comentar sobre las imágenes de Docker), el ejecutable \textit{trtexec} no se encuentra instalado, por lo que una forma más cómoda y general es usar la API.

\input{contenido/codes/trtshapes.tex}

En \ref{coud:trtshapes} se muestran las características de un modelo transformado en un \textit{engine} de TensorRT. La aparición de un -1 en la primera dimensión de los tensores indica que el modelo soporta \textit{\glspl{batch}} de tamaño dinámico.

\subsection{Características de los modelos}
Hablar de los 4 modelos que usamos

\subsection{Compilación de OpenCV con soporte para CUDA}

En \cite{andrew} se utiliza una versión del modelo YuNet otorgada por la librería OpenCV, esta versión abstrae al programador de implementar las fases de pre y postprocesado de dicho modelo, que para este caso resultan muy complejas de implementar. Como ya se comentó en la sección \ref{sec:sw}, OpenCV dispone de soporte para CUDA, aunque es necesario compilar el programa desde el fuente. El proceso de compilación seguido es el mismo que en \cite{OpenCVCUDA}.

TODO: algo más.

\section{Exportación de YOLO con ultralytics}
\label{sec:exports}

La \acrshort{api} de ultralytics permite la exportación de modelos YOLO con una gran variedad de ajustes y conversión a diferentes formatos como \acrshort{onnx} o incluso los utilizados por los \textit{backends} de OpenVINO, TensorRT, entre otros.

\input{contenido/codes/YOLOexport.tex}

El código \ref{coud:yolexport} se muestra un ejemplo de como exportar el modelo YOLO de ultralytics. Se descarga el archivo de pesos (.pt) y se exporta a \acrshort{onnx}, con un tamaño de batch de 1 y precisión de 16 bits de punto flotante (flag half).