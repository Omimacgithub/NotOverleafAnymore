\chapter{Pequeñas optimizaciones al código del sistema}
\label{chap:optimus}

\lettrine{E}{n} este apéndice se exponen algunas optimizaciones realizadas al código del sistema en términos de reserva de memoria y rendimiento.

\section{Optimizaciones en la reserva de memoria}

Debido a las limitaciones de memoria de la Jetson Orin Nano durante la ejecución del sistema (sección \ref{sec:syscal}), se ha optado por modificar el código a modo de hacerlo más eficiente en términos de reserva de memoria. Los cambios realizados son los siguientes:
\begin{itemize}
    \item Sustituir \textit{imports} de librerías "pesadas" (ejemplo: OpenCV o ultralytics, en torno a cientos de \acrshort{mb}) por otras más ligeras (ejemplo: numpy o scipy, en torno a decenas de \acrshort{mb}) con funciones alternativas, sin comprometer en exceso la precisión y latencia de las inferencias (ejemplo: sustituir cv2.copyMakeBorder por np.pad, o cv2.imread por PIL.Image).
    \item Uso de \textit{slots}: la variable \textit{\textunderscore \textunderscore slots\_\_} evita la creación del diccionario dinámico de Python (\textit{\_\_dict\_\_}), que almacena las variables de una \textbf{clase}. Los \textit{\_\_slots\_\_} son más rápidos y eficientes que los diccionarios dinámicos de Python, aunque también más extrictos (no se pueden crear más atributos que los especificados en \textit{\_\_slots\_\_}). Un ejemplo de uso se muestra en el código \ref{coud:slots}.
\end{itemize}

\input{contenido/codes/slots.tex}

El modelo ArcFace utilizado pesa \textbf{79 \acrshort{mb}} en su versión optimizada para TensorRT (en el formato \acrshort{onnx}, pesa \textbf{152 \acrshort{mb}}), 10 veces más que cualquiera de los otros modelos empleados (el tamaño es en torno a 8 \acrshort{mb}). Utilizando un modelo ArcFace más pequeño podría reducir en torno a cientos de \acrshort{mb} la ejecución del sistema.

\section{Modificaciones en el pre y postprocesado de modelos}
\label{chap:prepost}

Los pipelines de pre y postprocesado de los modelos suponen uno de los principales retos de los dispositivos Jetson, debido a limitada capacidad de procesamiento de las \acrshort{cpu} ARM, en este capítulo se exponen las soluciones aplicadas para mitigar el impacto computacional de dichos procesos en los modelos YOLO y OSNet.

\subsection{Postprocesado YOLO}
\label{sec:postyolo}

\subsection{Preprocesado OSNet}
\label{sec:preosnet}

Gracias a que se disponía del código de PyTorch del modelo, se ha podido integrar el pipeline de preprocesado de OSNet. El preprocesado de OSNet lleva a cabo las siguientes operaciones aplicadas a la imagen de entrada:
\begin{itemize}
    \item Intercambiar el orden de los canales de color (\acrshort{bgr} -> \acrshort{rgb}), ya que OpenCV trabaja con imágenes en \acrshort{bgr} y TensorRT con imágenes en \acrshort{rgb}.
    \item Reescalar la imagen a la resolución de entrada del modelo.
    \item Normalizar la imagen (dividir sus valores por 255, restar la media y dividir el resultado por la desviación típica).
    \item Intercambiar el orden de las dimensiones de la imagen: HWC (Height, Width, Channels) -> CHW, que es el modo que recomienda TensorRT para presentar los datos.
    \item Expandir una dimensión: CHW -> NCHW (donde N es el tamaño del \gls{batch}).
\end{itemize}

\input{contenido/codes/preosnet.tex}

El código \ref{coud:preosnet} muestra un extracto de la función \textit{forward} del modelo en el que se muestran las operaciones del preprocesado. Se han tenido que intercambiar funciones de OpenCV por alternativas integradas de los arrays de NumPy (ejemplo: cvtColor por permute), el reescalado de la imagen se ha dejado en el código de la inferencia.

Finalmente, se crea la instancia del modelo en PyTorch y se exporta mediante la función \textbf{torch.onnx.export}.

Es \textbf{muy importante} nombrar de forma única los \glspl{tensor} que se declaren, ya que especialmente en las últimas versiones de \gls{onnx} esto puede causar errores de \textbf{grafos acíclicos} en los modelos exportados.