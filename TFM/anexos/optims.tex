\chapter{Pequeñas optimizaciones al código del sistema}
\label{chap:optimus}

\lettrine{E}{n} este apéndice se exponen algunas optimizaciones realizadas al código del sistema en términos de reserva de memoria y rendimiento.

\section{Optimizaciones en la reserva de memoria}

Debido a las limitaciones de memoria de la Jetson Orin Nano durante la ejecución del sistema, se ha optado por modificar el código a modo de hacerlo más eficiente en términos de reserva de memoria. Los cambios realizados son los siguientes:
\begin{itemize}
    \item Sustituir \textit{imports} de librerías "pesadas" (ejemplo: OpenCV o ultralytics, en torno a cientos de \acrshort{mb}) por otras más ligeras (ejemplo: numpy o scipy, en torno a decenas de \acrshort{mb}) con funciones alternativas, sin comprometer en exceso la precisión y latencia de las inferencias (ejemplo: sustituir cv2.copyMakeBorder por np.pad, o cv2.imread por PIL.Image).
    \item Uso de \textit{slots}: la variable \textit{\textunderscore \textunderscore slots\_\_} evita la creación del diccionario dinámico de Python (\textit{\_\_dict\_\_}), que almacena las variables de una \textbf{clase}. Los \textit{\_\_slots\_\_} son más rápidos y eficientes que los diccionarios dinámicos de Python, aunque también más extrictos (no se pueden crear más atributos que los especificados en \textit{\_\_slots\_\_}). Un ejemplo de uso se muestra en el código \ref{coud:slots}.
\end{itemize}

\input{contenido/codes/slots.tex}

El modelo ArcFace utilizado pesa \textbf{79 \acrshort{mb}} en su versión optimizada para TensorRT (en el formato \acrshort{onnx}, pesa \textbf{152 \acrshort{mb}}), 10 veces más que cualquiera de los otros modelos empleados (el tamaño es en torno a 8 \acrshort{mb}). Utilizando un modelo ArcFace más pequeño podría reducir en torno a cientos de \acrshort{mb} la ejecución del sistema.

\section{Modificaciones en el pre y postprocesado de modelos}
\label{chap:prepost}

Los pipelines de pre y postprocesado de los modelos suponen uno de los principales retos de los dispositivos Jetson, debido a limitada capacidad de procesamiento de las \acrshort{cpu} \acrshort{arm}, en este capítulo se exponen las soluciones aplicadas para mitigar el impacto computacional de dichos procesos en los modelos YOLO y OSNet.

\subsection{Postprocesado YOLO}
\label{sec:postyolo}

\subsection{Preprocesado OSNet}
\label{sec:preosnet}

Gracias a que se disponía del código de PyTorch del modelo, se ha podido integrar el pipeline de preprocesado de OSNet. El preprocesado de OSNet lleva a cabo las siguientes operaciones aplicadas a la imagen de entrada:
\begin{itemize}
    \item Intercambiar el orden de los canales de color (\acrshort{bgr} -> \acrshort{rgb}), ya que OpenCV trabaja con imágenes en \acrshort{bgr} y TensorRT con imágenes en \acrshort{rgb}.
    \item Reescalar la imagen a la resolución de entrada del modelo.
    \item Normalizar la imagen (dividir sus valores por 255, restar la media y dividir el resultado por la desviación típica).
    \item Intercambiar el orden de las dimensiones de la imagen: HWC (Height, Width, Channels) -> CHW, que es el modo que recomienda TensorRT para presentar los datos.
    \item Expandir una dimensión: CHW -> NCHW (donde N es el tamaño del \gls{batch}).
\end{itemize}

\input{contenido/codes/preosnet.tex}

El código \ref{coud:preosnet} muestra un extracto de la función \textit{forward} del modelo en el que se muestran las operaciones del preprocesado. Se han tenido que intercambiar funciones de OpenCV por alternativas integradas de los arrays de NumPy (ejemplo: cvtColor por permute), el reescalado de la imagen se ha dejado en el código de la inferencia.

Finalmente, se crea la instancia del modelo en PyTorch y se exporta mediante la función \textbf{torch.onnx.export}.

Es \textbf{muy importante} nombrar de forma única los \glspl{tensor} que se declaren, ya que especialmente en las últimas versiones de \gls{onnx} esto puede causar errores de \textbf{grafos acíclicos} en los modelos exportados.

\section{Memoria fijada mapeada (\textit{pinned mapped memory})}
\label{sec:pmm}

Los dispositivos con \acrshort{gpu}s integradas, como en el caso de las Jetson, disponen de la \textbf{memoria fijada mapeada} (o \textit{pinned mapped memory}) de CUDA. La memoria fijada es una región de memoria que no puede ser paginada de vuelta a disco, de forma que siempre se mantiene en \acrshort{dram}, lo que acelera las transferencias de datos. Si la \acrshort{gpu} es integrada, entonces el acceso a la memoria entre el \textit{host} (o \acrshort{cpu}) y el \textit{device} (o \acrshort{gpu}) es \textbf{directo} (se pueden mapear directamente las direcciones de memoria ente el \textit{host} y el \textit{device}), de este modo, se \textbf{evitan operaciones de copias de memoria} (cudaMemcpy), lo que permite recortar la latencia asociada a estas operaciones. Es importante mencionar que dicho tipo de memoria también se puede aplicar a \textbf{\acrshort{gpu}s discretas}, aunque su uso es ventajoso solo en ciertas situaciones \cite{leimao}.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.75\linewidth]{imagenes/memcpy.png}
    \caption{Arquitectura de interconexión de una \acrshort{gpu} discreta y una \acrshort{gpu} integrada. Figura extraída de \cite{zerocpy}}
    \label{fig:memcpy}
\end{figure}

La figura \ref{fig:memcpy} muestra un esquema de interconexión de una \acrshort{gpu} discreta (familia Intel) y una \acrshort{gpu} integrada (familia Jetson), en la que la primera requiere un cudaMemcpy para poder disponer de los datos en la \acrshort{gpu} y la segunda evita realizar transferencias debido a que se utiliza la misma \acrshort{dram} (\textbf{zero-copy}).

A pesar de los beneficios en la reducción de operaciones \textbf{cudaMemcpy} a procesar en los \glspl{stream} de CUDA, la mejora apenas ha supuesto una reducción de \textbf{1 milisegundo} en las pruebas del capítulo \ref{chap:cnn} y, debido a la falta de tiempo y a que ya se encontraban realizadas las pruebas de los modelos y del sistema, finalmente \textbf{no se ha aplicado} dicha mejora en los resultados de esta memoria.