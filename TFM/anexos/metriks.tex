\chapter{Métricas de evaluación de modelos de visión artificial}
\label{sec:modeleval}

\lettrine{E}{n} este apéndice se presentan los \textit{items} y métricas utilizadas para evaluar los modelos de detección y reconocimiento de este proyecto.

\section{Matriz de confusión}
\label{sec:metriks}

Para evaluar la clasificación de los modelos de visión artificial del proyecto, se empleó la famosa matriz de confusión, que es una tabla compuesta por los siguientes \textit{items}:
\begin{description}
    \item[True Positive (TP)] Predicción positiva correcta. Para los modelos de detección, sería detectar correctamente a una persona, mientras que para los modelos de reconocimiento sería asignar la etiqueta correcta a la persona detectada.
    \item[True Negative (TN)] Predicción negativa correcta. Para los modelos de detección, sería no detectar correctamente a una persona, lo que carece de sentido. Para los modelos de reconocimiento, sería predecir que la persona no pertenece a ninguna de las ya registradas, es decir, predecir un \textbf{desconocido}. Este componente \textbf{no se llega a utilizar} para las pruebas.
    \item[False Positive (FP)] Predicción positiva errónea. Para los modelos de detección, sería detectar una persona donde no la hay, mientras que para los modelos de reconocimiento sería otorgar una etiqueta incorrecta a la persona detectada.
    \item[False Negative (FN)] Predicción negativa errónea. Para los modelos de detección, sería no detectar a la persona presente, mientras que para los modelos de reconocimiento sería asignar la entidad de desconocido a una persona ya registrada en el sistema.
\end{description}

A continuación, se exponen las \textbf{métricas de evaluación} utilizadas para los modelos de detección y reconocimiento.

\section{Modelos de detección}
\label{subsec:det}

\begin{description}
    \item[Precision] \textbf{Cuando el modelo detecta un objeto}, cuantas veces la detección corresponde con una persona presente.
        \begin{equation}
            Precision = \frac{TP}{TP + FP}
        \end{equation}
    \item[Recall] \textbf{Cuando realmente hay una persona}, cuantas veces el modelo detecta a esa persona.
        \begin{equation}
            Recall = \frac{TP}{TP + FN}
        \end{equation}
    \item[F1\_score] Se calcula como una media armónica entre el \textit{recall} (proporción de personas detectadas) y el \textit{precision} (proporción de detecciones correctas) \cite{andrew}.
        \begin{equation}
            F1\_score = 2* \frac{Precision * Recall}{Precision + Recall}
        \end{equation}
\end{description}

\section{Modelos de reconocimiento}
\label{subsec:recon}

Las métricas de evaluación se calculan igual que en los modelos de detección. A continuación se exponen las diferencias en el significado:
\begin{description}
    \item[Precision] \textbf{Cuando se reconoce a una persona como conocida}, cuantas veces se ha reconocido correctamente su entidad.
    \item[Recall] \textbf{Cuando se reconoce a una persona}, cuantas veces se ha reconocido correctamente su entidad.
    \item[F1\_score] Lo mismo que en los modelos de detección.
\end{description}