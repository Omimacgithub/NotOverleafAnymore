\chapter{Análisis de rendimiento}
\label{chap:cnn}

\lettrine{E}{n} este capítulo se detalla brevemente la elección de la NVIDIA Jetson para este proyecto, las pruebas realizadas y resultados de rendimiento de las inferencias de las \acrshort{cnn} utilizadas.

\section{Introducción}

A la hora de diseñar aceleradores hardware para aplicaciones de redes neuronales es necesario mantener el balance entre precisión, rendimiento y bajo consumo. La NVIDIA Jetson es una familia de sistemas embebidos que persigue maximizar dicho balance \cite{MITTAL2019428}. Un claro ejemplo es el uso de la Jetson TX2 en aplicaciones como el reconocimiento facial para el procesamiento de videos en resolución Full HD (1080p) \cite{qi2018iot} o la detección de personas con cámaras \acrshort{rgb} en resolución HD (720p) \cite{rahmaniar2021real}, ambos casos de uso han reportado un rendimiento satisfactorio en tiempo real.

Entre las múltiples técnicas y herramientas que existen \cite{MITTAL2019428}, se ha decantado principalmente por usar TensorRT. Este ecosistema de herramientas permite la optimización de redes neuronales \cite{rahmaniar2021real}, logrando así la ejecución de múltiples aplicaciones en tiempo real. NVIDIA dispone dicho software para una gran variedad de sus \acrshort{gpu} y dispositivos Jetson, además de tutoriales para múltiples aplicaciones de \acrshort{ia} \cite{x36}.

\section{Realización de pruebas}

Las pruebas del sistema se han focalizado en evaluar las redes neuronales utilizadas en los nodos cámara y el rendimiento del sistema completo. Para este fin, se cuentan con \textbf{\textit{\glspl{dataset}}} generados a partir de los datos grabados de una prueba de 2 minutos dentro del laboratorio de robótica del \acrshort{citic} con 6 personas.

Para las pruebas de los modelos de detección, se reproduce el video para que el modelo realice las detecciones en cada frame. Las predicciones obtenidas por el modelo se comparan contra el \textit{\gls{dataset}} para evaluar si son correctas. En la tabla (TODO) se muestra para cada modelo de detección (YOLOv8n y YuNet) la métrica F1 (definida en \ref{sec:metriks}).

Para las pruebas de los modelos de reconocimiento, se guardan como imágenes los recortes de los cuerpos y de las caras de cada individuo en cada frame del video grabado por la cámara. Para cada recorte, se ejecuta el modelo y se compara su predicción con la recogida en el \textit{\gls{dataset}} \cite{andrew}. En la tabla (TODO).

\section{Resultados}
\label{sec:modelic}

Todas las \acrshort{cnn} de este proyecto utilizan OpenVINO (ver sección \ref{sec:sw}) salvo YuNet, que utiliza el motor de OpenCV (ver sección \ref{sec:sw}) en \acrshort{cpu}.

Los primeros resultados arrojados en la tabla \ref{tab:equips} muestran que las placas Jetson \textbf{no son capaces de cumplir el objetivo de 10 Hz} usando OpenVINO. Sólo la ejecución de YOLOv8n consume los 100 ms en la Jetson Xavier NX. En cuanto a la Jetson Orin Nano, la suma de la ejecución de las 4 redes supera también los 100 ms.

TensorRT es un framework para inferencia de alto rendimiento creado por NVIDIA, pensado para la ejecución de redes neuronales en sistemas embebidos de NVIDIA como es la Jetson \cite{MITTAL2019428, rahmaniar2021real}.

TensorRT se ha empleado para optimizar modelos ONNX al formato propio de la herramienta (denominado engine) y así poder realizar inferencias a partir de su API. Un ejemplo de código de inferencia en TensorRT es el que se muestra en TODO.

Todas las redes neuronales fueron exportadas a un engine de TensorRT salvo YuNet, que utiliza \textbf{OpenCV con CUDA}. Para disponer de soporte CUDA, es necesario compilar OpenCV \textbf{desde el fuente} \cite{OpenCVCUDA}.

NVIDIA consta que con TensorRT, el Speed up de las inferencias aumenta \textbf{36 veces}. En este caso se ha logrado reducir hasta \textbf{5 veces} el tiempo de inferencia en el caso de YOLOv8n en la Jetson Xavier NX. Si sumamos todos los tiempos de inferencia usando el motor de TensorRT, da un total de \textbf{63.6 milisegundos} para la Jetson Xavier NX y \textbf{49.1 milisegundos} para la Jetson Orin Nano.

\input{contenido/tables/tests/tabequips}

Afortunadamente, todas las librerías del proyecto cuenta con soporte para ambas arquitecturas x86 y ARM64, por lo que fué prácticamente inmediato el despliegue entre dispositivos mediante Docker (exceptuando pequeños cambios en el código por el cambio de versión de TensorRT y omitir ciertas optimizaciones (ejemplo: flag msse de las CPUs x86)).

Una ventaja de TensorRT es la \textbf{fácil portabilidad del código}, se puede ejecutar un mismo script de inferencia en cualquier equipo que posea una GPU de NVIDIA, siempre que sea posible instalar la misma versión de TensorRT (en caso contrario, es necesario realizar un cambio en el código, aunque este es leve).

En la tabla (TODO) se muestran los resultados obtenidos en 2 equipos x86, comparándolos con la Jetson Orin Nano, el sistema embebido con el que se ha obtenido el mejor rendimiento. En equipos x86, la latencia experimentada  es hasta \textbf{5.3 veces menor} (YOLOv8n) para los modelos ejecutados en CPU y hasta \textbf{4.9 veces menor} (YuNet) para los modelos ejecutados en GPU que la obtenida por la Jetson Orin Nano. Debido a que OpenVINO está pensado para hardware de Intel, las optimizaciones en la Jetson no suponen ninguna mejora, al contrario que en los equipos x86 con una CPU \textbf{Intel i7}. Por otro lado, es sorprendente que también existan diferencias significativas respecto a la Jetson \textbf{cuando se realizan las inferencias en GPU}. La diferencia más notoria es con YuNet en el PC del laboratorio, que poseé una \textbf{GeForce GTX 1650}, que está \textbf{una generación por detrás} de la GPU de la Jetson Orin (las arquitecturas son Turing y Ampere respectivamente) \cite{archs}. Esto implica que, para la carga de trabajo de este proyecto, \textbf{la GPU no es el factor predominante} (TODO: con estos datos puedo sacar esta conclusión?).

TODO: El rendimiento esperado en tiempo real para el sistema son los \textbf{10 Hz} (o 10 FPS), por lo tanto, todos los nodos deben de funcionar a 10 Hz. Las NVIDIA Jetson solo tienen que ocuparse de ejecutar los \textbf{nodos cámara a 10 Hz} y que otro equipo ejecute el nodo LiDAR e integrador del sistema. Que los nodos cámara trabajen a 10 Hz implica que a cada \textbf{100 ms se recibe un nuevo frame} (10 * 100 = 1000 ms = 1 segundo), la figura \ref{fig:finalsys} muestra el procesado realizado por dicho nodo en cada frame, que debe de finalizar \textbf{antes de que transcurran 100 ms}. En cada iteración del procesado, se ejecutan \textbf{4 redes neuronales (\acrshort{cnn})}. 2 de ellas, YOLOv8n y YuNet, para la detección de cuerpos y de caras respectivamente. Las otras 2, OSNet\_x1 y ArcFace, para el reconocimiento de cuerpos y caras respectivamente. A pesar de lo que la figura \ref{fig:finalsys} muestra, las redes de detección YOLOv8n y YuNet \textbf{no se ejecutan en paralelo}, lo mismo sucede con las redes ArcFace y OSNet\_x1. Se ha considerado ejecutar en paralelo estas redes. Sin embargo, las limitaciones de memoria principal de la Jetson restringen tomar esta aproximación.