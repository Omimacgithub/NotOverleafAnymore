\chapter{Análisis de rendimiento}
\label{chap:cnn}

\lettrine{E}{n} este capítulo se expone una discusión detallada acerca de las pruebas y de los resultados de rendimiento de los modelos de \acrshort{cnn} utilizados en los diferentes dispositivos del proyecto.

\section{Modelos de redes neuronales utilizadas}
\label{sec:models}

A continuación se exponen las \acrshort{cnn} utilizadas en el sistema, que también se emplearon en \cite{andrew}.

\subsection{YuNet}

Es un modelo de detección de rostros diseñado para sistemas con recursos muy limitados \cite{wu2023yunet}. Debido a que el código de pre y postprocesado es demasiado complejo de implementar, se ha optado por utilizar la versión de OpenCV más simplificada.

\subsection{ArcFace}

ArcFace \cite{deng2019arcface} ha demostrado ser de las mejores redes en el \acrshort{sota} del reconocimiento facial.

\subsection{YOLO}

\textit{You Only Look Once} (YOLO) es una red de detección de objetos que otorga una velocidad de inferencia asombrosa manteniendo una buena precisión \cite{redmon2016you}. Existen múltiples versiones de YOLO, en \cite{andrew} se probaron las versiones 3, 5 y 8 en el tamaño nano, más ligero y veloz, pero al mismo tiempo más impreciso. En este proyecto a mayores se ha probado la versión 11, la última existente a día de hoy, en su tamaño nano, en los \textit{backends} OpenVINO y TensorRT. Se ha optimizado el postprocesado explotando la librería de NumPy y los resultados son muy notorios.

\subsection{OSNet}

\textit{Omni-Scale Network} es un modelo de reidentificación de personas que emplea características de diferentes tamaños (\textit{omni-scale features}) para la clasificación. Destaca por ser extremadamente ligero y por su elevada precisión \cite{zhou2019omni}. Se emplea la implementación de \cite{andrew} en el framework Pytorch. Se incorporó preprocesado en la GPU.

TODO: Que los nodos cámara trabajen a 10 Hz implica que a cada \textbf{100 ms se recibe un nuevo frame} (10 * 100 = 1000 ms = 1 segundo), la figura \ref{fig:finalsys} muestra el procesado realizado por dicho nodo en cada frame, que debe de finalizar \textbf{antes de que transcurran 100 ms}. En cada iteración del procesado, se ejecutan \textbf{4 redes neuronales (\acrshort{cnn})}. 2 de ellas, YOLOv8n y YuNet, para la detección de cuerpos y de caras respectivamente. Las otras 2, OSNet\_x1 y ArcFace, para el reconocimiento de cuerpos y caras respectivamente. A pesar de lo que la figura \ref{fig:finalsys} muestra, las redes de detección YOLOv8n y YuNet \textbf{no se ejecutan en paralelo}, lo mismo sucede con las redes ArcFace y OSNet\_x1. Se ha considerado ejecutar en paralelo estas redes. Sin embargo, las limitaciones de memoria principal de la Jetson restringen tomar esta aproximación.

\section{Realización de pruebas}

Para la evaluación del rendimiento, se parten de unos tests que reproducen los videos de las dos cámaras (ver sección \ref{sec:hw}) y alimenta a los modelos de redes neuronales con los respectivos frames, de forma que al momento de terminar el procesamiento de un frame, el modelo puede continuar inmediatamente con el siguiente. Las salidas devueltas por las \acrshort{cnn} se contrastan con la información del \textit{\gls{dataset}} correspondiente (ver sección \ref{sec:dataset}), que devuelve un resultado final de precisión.

Se realiza una medición de la latencia de la inferencia, en el caso de los modelos de detección, y de la inferencia mas el reconocimiento en el caso de los modelos de reconocimiento. En ambos casos se emplea la función \textbf{time} del módulo \textbf{time} de Python para obtener los tiempos en cada frame. Finalmente, todas las latencias se fusionan a partir de la media.

En las pruebas de los modelos de detección, se obtienen los frames directamente de los videos, que son procesados por los modelos para devolver las \glspl{bbox}. El test determina una detección como coincidente si los porcentajes de intersección entre la \glspl{bbox} obtenida respecto a la de referencia y vicerversa superan o igualan a un umbral, en este caso fijado en el 50\% \cite{andrew}.

En el caso de los modelos de reconocimiento, se recupera para cada frame una lista de recortes \textbf{generados previamente a la ejecución de la prueba}. El test determina un reconocimiento como correcto si la predicción obtenida es la misma que la etiqueta (\textit{\gls{gt}}) recogida en el \gls{dataset}. Por sencillez, siempre se escoge la primera predicción de la lista como la predicción final (\textit{top\_k=1}, ver sección \ref{sec:basearch}).

Afortunadamente, todas las librerías del proyecto cuenta con soporte completo e incluso optimizado para las arquitecturas x86 y ARM64 TODO: seguro que hay diferencias.

\section{Resultados}
\label{sec:modelic}

TODO: los backends de OpenVINO y TensorRT soportan precisión mixta, es decir, se ejecutan las capas de un modelo en la precisión que menor latencia reporte dentro de una lista de precisiones (ejemplo: FP16 o FP32) \cite{mixed}. Esta funcionalidad puede emplearse siempre que la arquitectura de los dispositivos la implemente. Los modelos del proyecto se han compilado para utilizar la precisión mixta en FP16 o FP32 (TODO: y YuNet?), salvo OSNet\_x1, que se encuentra enteramente en FP16 en la versión de TensorRT.

En la figura \ref{tab:equips} se muestran los resultados de latencia (en milisegundos) y la métrica \textit{F1\_score} para los modelos de detección y \textit{precision} para los modelos de reconocimiento (ver métricas en la sección \ref{sec:modeleval}). Para la ejecución de inferencias, se han empleado los backends de OpenVINO, TensorRT y OpenCV, este último para el caso concreto de la red YuNet. En el caso del modelo YOLO, los dos dispositivos menos potentes (Jetson Xavier NX y Jetson Orin Nano) no alcanzan o apenas superan los 10 Hz en la versión de OpenVINO, que no utiliza la \acrshort{gpu} para la inferencia. Al trasladar la carga de la inferencia a la \acrshort{gpu} cuando se utiliza TensorRT, el tiempo de inferencia es hasta \textbf{7 veces} menor en el caso de la Jetson AGX Thor y \textbf{5 veces} menor en el caso de la Jetson Xavier NX. En la versión de TensorRT de YOLO, las Jetson NX y Orin Nano logran alcanzar los 35 y 45 Hz respectivamente, lo que hace posible la ejecución en tiempo real (fijada a 10 Hz) en estos dispositivos.

TODO: Si sumamos todos los tiempos de inferencia usando el motor de TensorRT, da un total de \textbf{63.6 milisegundos} para la Jetson Xavier NX y \textbf{49.1 milisegundos} para la Jetson Orin Nano.

TODO: demostrar por que la GPU del lab pc hace que esté ligeramente detrás de la GPU de la laptop en TensorRT

TODO: Siendo la AGX Thor ligeramente peor en OpenVINO, consigue el mejor resultado en TensorRT para YOLO, estudiarlo.

\input{contenido/tables/tests/tabequips}

TODO: En la tabla \ref{tab:equips} se muestran los resultados obtenidos en 2 equipos x86, comparándolos con la Jetson Orin Nano, el sistema embebido con el que se ha obtenido el mejor rendimiento. En equipos x86, la latencia experimentada  es hasta \textbf{5.3 veces menor} (YOLOv8n) para los modelos ejecutados en CPU y hasta \textbf{4.9 veces menor} (YuNet) para los modelos ejecutados en GPU que la obtenida por la Jetson Orin Nano. Debido a que OpenVINO está pensado para hardware de Intel, las optimizaciones en la Jetson no suponen ninguna mejora, al contrario que en los equipos x86 con una CPU \textbf{Intel i7}. Por otro lado, es sorprendente que también existan diferencias significativas respecto a la Jetson \textbf{cuando se realizan las inferencias en GPU}. La diferencia más notoria es con YuNet en el PC del laboratorio, que poseé una \textbf{GeForce GTX 1650}, que está \textbf{una generación por detrás} de la GPU de la Jetson Orin (las arquitecturas son Turing y Ampere respectivamente) \cite{archs}. Esto implica que, para la carga de trabajo de este proyecto, \textbf{la GPU no es el factor predominante} (TODO: no lo creo, no tiene mucho sentido si digo que se reduce la latencia hasta 7 veces usando la GPU, y que la AGX Thor es la que mejor ejecuta YOLO cuando se utiliza la GPU).

Hardware Abstraction Layer, KleidiCV para acelerar operaciones de OpenCV (a partir de 4.10.0) en arquitecturas Armv8 y v9. Putada: es una API sólo en C (usar ctypes en Python?).