\chapter{Análisis de rendimiento}
\label{chap:cnn}

\lettrine{E}{n} este capítulo, se comentan los modelos de redes neuronales utilizadas y se expone una discusión detallada acerca de las pruebas y de los resultados de rendimiento de los modelos en los diferentes dispositivos del proyecto.

\section{Modelos de redes neuronales utilizadas}
\label{sec:models}

A continuación se exponen las \acrshort{cnn} utilizadas en los nodos cámara, que también se emplearon en \cite{andrew}.

\subsection{YuNet}

Es un modelo de detección facial diseñado para sistemas con recursos muy limitados, con tan solo \textbf{75856 parámetros} \cite{wu2023yunet}. Debido a los múltiples \textit{outputs} del modelo, complejos de procesar, se ha optado por utilizar la versión de OpenCV, que simplifica todo el proceso de pre y postprocesado. El modelo en formato \acrshort{onnx} se encuentra en este enlace \footnote{https://github.com/opencv/opencv\_zoo/blob/main/models/face\_detection\_yunet/face\_detection\_yunet\_2023mar.onnx}.

\subsection{ArcFace}

\textit{Additive Angular Margin Loss} o ArcFace es una función de pérdida (\gls{lossfunc}) que mejora el poder discriminativo de los modelos de reconocimiento facial, es decir, maximiza el margen que separa a las clases y minimiza el margen de los datos que pertenecen a una misma clase \cite{deng2019arcface}. Los \glspl{embedding} con los que opera ArcFace pertenecen al modelo ResNet-100. Se emplea el archivo de pesos de una implementación en TensorFlow Lite, accesible mediante el siguiente enlace \footnote{https://www.digidow.eu/f/datasets/arcface-tensorflowlite/model.tflite}.

%ha demostrado ser de las mejores redes en el \acrshort{sota} del reconocimiento facial.

\subsection{YOLO}

\textit{You Only Look Once} (YOLO) es la red más rápida y simple de detección de objetos en tiempo real, que logra mantener una buena precisión \cite{redmon2016you, mdpi}. Existen múltiples versiones de YOLO, en \cite{andrew} se probaron las versiones 3, 5 y 8 en el tamaño nano, más ligero y veloz, pero al mismo tiempo más impreciso. En este proyecto a mayores se ha probado la versión 11, la última existente a día de hoy, en su tamaño nano. El modelo se puede obtener mediante la \acrshort{api} de Python de ultralytics, que permite realizar exportaciones con diferentes ajustes (ver sección \ref{sec:exports}).

Se ha optimizado el postprocesado explotando la librería de NumPy y los resultados son muy notorios.

\subsection{OSNet}

\textit{Omni-Scale Network} (OSNet) es un modelo de reidentificación de personas que emplea características de diferentes tamaños (\textit{omni-scale features}) para la clasificación. Destaca por ser extremadamente ligero y por su elevada precisión \cite{zhou2019omni}. Se realizó una exportación a partir del modelo implementado en Pytorch (\cite{andrew}).

\subsection{Características de los modelos}

\input{contenido/tables/modelspecs.tex}

En la tabla \ref{tab:modelspecs} se muestra el tamaño de la entrada, el número de parámetros y la latencia de cada modelo.

Cada valor de la columna \textit{input size} se compone de la resolución de la imagen de entrada (ejemplo: 112x112) multiplicado por el número de canales de entrada (en este caso 3, que se corresponde con los canales \textit{Red, Green} y \textit{Blue} de una imagen \acrshort{rgb}). Los modelos OSNet y ArcFace utilizan una resolución fija de 256x128 y 112x112 respectivamente de los frames de entrada, mientras que YOLO y YuNet permiten un ajuste dinámico de dicha resolución en tiempo de ejecución. Para obtener el mejor balance entre precisión y latencia, se ha optado por establecer una resolución fija de 640x640 y 480x480 respectivamente.

\section{Dataset de las pruebas}
\label{sec:modset}

A modo de evaluar el rendimiento en términos de precisión de los modelos de visión artificial, se parte de un \gls{dataset} compuesto de la información extraída de un fichero Rosbag, que contiene los datos generados por dos cámaras Kinect \acrshort{rgbd} y un sensor \acrshort{lidar} instalados en el Summit\_XL durante la realización de una prueba.

La prueba del Rosbag se ha grabado en el laboratorio de robótica del \acrshort{citic}, en el que 6 personas caminan alrededor del Summit\_XL y se entrecruzan durante 105 segundos (aproximadamente). El robot también se desplaza, generando así imágenes borrosas y cambios impredecibles de la posición de las personas respecto del robot.

Existe un fichero \acrshort{json} con el \gls{dataset} específico para cada cámara. Este se compone de los siguientes objetos \cite{andrew}:
\begin{itemize}
    \item \textit{\textbf{frames}}: lista de todos los frames capturados por la cámara. Cada entrada en la lista sigue la siguiente estructura:
          \begin{itemize}
              \item \textit{\textbf{id}}: identificador del frame.
              \item \textit{\textbf{entities}}: lista de las personas presentes, que contiene los siguientes campos:
                    \begin{itemize}
                        \item \textbf{bbox}: \gls{bbox} corporal del sujeto, compuesta por los siguientes elementos:
                              \begin{itemize}
                                  \item \textit{\textbf{x0, x1, y0 e y1}}: Coordenadas de la \gls{bbox}.
                                  \item \textit{\textbf{conf}}: confianza de la detección.
                                  \item \textit{\textbf{height}}: altura de la \gls{bbox}.
                                  \item \textit{\textbf{width}}: anchura de la \gls{bbox}.
                                  \item \textit{\textbf{difficulty}}: valoración de la dificultad de detección (easy, medium o hard), las dificultades se asignan de acuerdo con un sistema de puntos desarrollado en \cite{andrew} que tiene en cuenta diversos factores (ejemplo: porcentaje de oclusión o borrosidad), de esta forma, se organizan los resultados a partir de este criterio. Por sencillez, los resultados reflejados son siempre globales (es decir, la media de todas las dificultades).
                              \end{itemize}
                        \item \textit{\textbf{face\_bbox}}: \gls{bbox} facial del sujeto, compuesta por los mismos elementos que el campo \textbf{bbox}, agregando dos campos más, uno con las coordenadas de los puntos faciales (\textit{landmarks}) y el otro con la orientación de la cara (si está frontal, girada a la izquierda o a la derecha), calculada a partir de los \textit{landmarks}.
                        \item \textit{\textbf{filename}}: nombre de los archivos con los recortes facial y corporal del individuo.
                        \item \textit{\textbf{person\_id}}: etiqueta asignada al individuo.
                    \end{itemize}
          \end{itemize}
    \item \textit{\textbf{res}}: resolución de la cámara.
    \item \textit{\textbf{camera}}: identificador de la cámara.
    \item \textit{\textbf{face\_padding}}: valor de relleno para aumentar el tamaño de la \gls{bbox} facial.
    \item \textit{\textbf{input}}: ruta al video generado por la cámara.
\end{itemize}

\section{Realización de las pruebas}
\label{sec:probes}

Para la evaluación del rendimiento, se parten de unos tests que reproducen los videos de las dos cámaras (ver sección \ref{sec:hw}) y alimenta a los modelos de redes neuronales con los respectivos frames, de forma que al momento de terminar el procesamiento de un frame, el modelo puede continuar inmediatamente con el siguiente. Las salidas devueltas por las \acrshort{cnn} se contrastan con la información del \textit{\gls{dataset}} de la respectiva cámara (visto en la anterior sección), que devuelve un resultado final de precisión.

Se realiza una medición de la latencia de la inferencia, en el caso de los modelos de detección, y de la inferencia más el reconocimiento en el caso de los modelos de reconocimiento. En ambos casos, se emplea la función \textbf{time} del módulo \textbf{time} de Python para obtener los tiempos en cada frame. Finalmente, todas las latencias se fusionan a partir de la media.

En las pruebas de los modelos de detección, se obtienen los frames directamente de los videos. El test determina una detección como coincidente si los porcentajes de intersección entre la \gls{bbox} obtenida respecto a la de referencia y viceversa superan o igualan un umbral. En este caso, dicho umbral se ha fijado en el 50\% \cite{andrew}.

En el caso de los modelos de reconocimiento, se recupera para cada frame una lista de recortes generados por los modelos de detección \textbf{previamente a la ejecución de la prueba}. El test determina un reconocimiento como correcto si la predicción obtenida es la misma que la etiqueta (\textit{\gls{gt}}) recogida en el \gls{dataset}. Por sencillez, siempre se escoge la primera predicción de la lista como la predicción final (\textit{top\_k=1}, ver sección \ref{sec:basearch}).

Para la ejecución de inferencias, se han empleado los \glspl{rt} de OpenVINO, TensorRT y OpenCV, este último para el caso concreto de la red YuNet. OpenVINO ejecuta las inferencias \textbf{puramente en \acrshort{cpu}}, mientras que TensorRT realiza las inferencias en \textbf{\acrshort{gpu}}. OpenCV permite la ejecución en el propio \gls{rt} que ofrece, que opera \textbf{puramente en \acrshort{cpu}} o permite realizar la inferencia por medio de CUDA, que aplica la \acrshort{gpu}.

Es importante destacar que los \glspl{rt} de OpenVINO y TensorRT soportan precisión mixta, es decir, se ejecutan las capas de un modelo en la precisión que menor latencia reporte dentro de una lista de precisiones (ejemplo: \acrshort{fp}16 o \acrshort{fp}32) \cite{mixed}. Esta funcionalidad puede emplearse siempre que la arquitectura de los dispositivos la implemente. Las últimas versiones de OpenCV también implementan soporte para ambas precisiones \acrshort{fp}32 y \acrshort{fp}16. Los modelos del proyecto se han compilado para utilizar la precisión mixta en \acrshort{fp}16 o \acrshort{fp}32.

\section{Resultados}
\label{sec:modelic}

\input{contenido/tables/tests/tabequips}

En la tabla \ref{tab:equips} se muestran los resultados de latencia (en milisegundos) y de la métrica \textit{F1\_score} para los modelos de detección y \textit{precision} para los modelos de reconocimiento (ver métricas en el apéndice \ref{sec:modeleval}). Cada resultado refleja la media y la desviación típica de \textbf{5 ejecuciones}, se ejecuta un test previo a modo de calentamiento para cada modelo.

En el caso del modelo YOLO, los 2 equipos de sobremesa y la Jetson AGX Thor se quedan por debajo de 30 ms en la versión de OpenVINO, mientras que la latencia en el resto de dispositivos es significativamente mayor. Es interesante como entre el portátil y el PC del laboratorio hay \textbf{8 milisegundos de diferencia} a favor del último, siendo ambas \acrshort{cpu}s de la misma generación (Intel Core i7).

Al trasladar la carga de la inferencia de YOLO a la \acrshort{gpu} cuando se utiliza TensorRT, el tiempo de inferencia es hasta \textbf{7 veces} menor en el caso de la Jetson AGX Thor y \textbf{5 veces} menor en el caso de la Jetson Xavier NX. Las Jetson Xavier NX y Orin Nano logran alcanzar los \textbf{35 y 45 \acrshort{fps}} respectivamente, lo que hace posible la ejecución en tiempo real (fijada a 10 \acrshort{fps}) en estos dispositivos.

Un punto a favor para la AGX Thor es haber obtenido la menor latencia al ejecutar YOLO11n en \acrshort{gpu}, estando notablemente por debajo de los equipos de sobremesa en la versión de OpenVINO. Gran parte de la diferencia en la latencia viene de la implementación del pre y postprocesado de YOLO, que resulta muy costoso y se realiza en la \acrshort{cpu}, que es donde los equipos de sobremesa destacan. Respecto al postprocesado, se ha optimizado para aprovechar los arrays y operaciones de NumPy (ver apéndice \ref{sec:postyolo}), de esta forma ha sido posible recortar varios milisegundos en todos los equipos.

%TODO: En la tabla \ref{tab:equips} se muestran los resultados obtenidos en 2 equipos x86, comparándolos con la Jetson Orin Nano, el sistema embebido con el que se ha obtenido el mejor rendimiento. En equipos x86, la latencia experimentada  es hasta \textbf{5.3 veces menor} (YOLOv8n) para los modelos ejecutados en CPU y hasta \textbf{4.9 veces menor} (YuNet) para los modelos ejecutados en GPU que la obtenida por la Jetson Orin Nano. Debido a que OpenVINO está pensado para hardware de Intel, las optimizaciones en la Jetson no suponen ninguna mejora, al contrario que en los equipos x86 con una CPU \textbf{Intel i7}. Por otro lado, es sorprendente que también existan diferencias significativas respecto a la Jetson \textbf{cuando se realizan las inferencias en GPU}. La diferencia más notoria es con YuNet en el PC del laboratorio, que posee una \textbf{GeForce GTX 1650}, que está \textbf{una generación por detrás} de la GPU de la Jetson Orin (las arquitecturas son Turing y Ampere respectivamente) \cite{archs}. Esto implica que, para la carga de trabajo de este proyecto, \textbf{la GPU no es el factor predominante} (TODO: no lo creo, no tiene mucho sentido si digo que se reduce la latencia hasta 7 veces usando la GPU, y que la AGX Thor es la que mejor ejecuta YOLO cuando se utiliza la GPU).

En el caso de YuNet, el uso de CUDA como \gls{rt} supone un salto menor respecto a TensorRT, ya que OpenCV integrado con CUDA no aprovecha todos los componentes de la arquitectura de la \acrshort{gpu}, como los tensor cores. Excepto en las Jetson Xavier NX y AGX Thor, el resto de dispositivos no llegan a recortar ni 1 milisegundo o, en el caso de la AGX Orin, incluso llegan a aumentar la latencia. También es importante considerar que YuNet está compuesto por una arquitectura extremadamente ligera (ver tabla \ref{tab:modelspecs}), por lo que no hay mucho margen de mejora en la latencia de inferencia, en cambio, el postprocesado entraña operaciones costosas que se ejecutan en la \acrshort{cpu}, como el algoritmo \gls{nms} (al igual que YOLO). Esta podría ser la principal causa del excelente rendimiento en los equipos de sobremesa.

En el caso del modelo ArcFace, el uso de TensorRT reduce a la mitad o 3 veces la latencia, llegando a reducirse \textbf{5 veces} en el caso del portátil.

OSNet\_x1, en comparación con ArcFace, no representa muchas variaciones en la latencia para los equipos de sobremesa. En cambio, en los dispositivos Jetson el salto es más notorio. A diferencia de YuNet, ni el pre ni el postprocesado resultan un problema, de hecho, aprovechando la disponibilidad del código de PyTorch, se ha implementado el preprocesado internamente en el modelo, de forma que se ejecuta en la \acrshort{gpu} junto con los \glspl{kernel} de la inferencia (ver apéndice \ref{sec:preosnet} para más detalles). La ejecución del postprocesado en \acrshort{gpu} ha supuesto una mejora de aproximadamente 1 milisegundo en la ejecución.

\section{Discusión}
\label{sec:discuss}

Las \acrshort{gpu}s de los equipos de sobremesa rinden de una forma excelente, consiguiendo un salto de mejora equiparable al de los dispositivos Jetson, cuyas \acrshort{gpu}s poseen los tensor cores, más potentes que los núcleos de CUDA y la memoria unificada, que evita el uso de operaciones cudaMemcpy.

Los resultados tan bajos de latencia en los equipos de sobremesa son especialmente sorprendentes, teniendo en cuenta de que las operaciones de copia de memoria y sincronización deberían de añadir una sobrecarga notoria en las inferencias.

El uso de memoria fijada mapeada en los dispositivos Jetson supone una ligera reducción en la latencia (en torno a 1 milisegundo según las pruebas), que no compensa la latencia de procesamiento. De todos modos, se consiguen ahorrar las llamadas a cudaMemcpy en los sistemas con \acrshort{gpu}s integradas, lo que ayuda a reducir la congestión de la cola de operaciones de un \gls{stream} en la \acrshort{gpu}.

La Jetson AGX Orin, a pesar de doblar los recursos computacionales de la Orin Nano, apenas consigue una mejora en cuanto a reducción de latencia, al mismo tiempo que se queda atrás de los resultados logrados en la AGX Thor. Un ejemplo es la ejecución de YOLO11n en TensorRT, cuya diferencia entre ambas AGX es de \textbf{15 milisegundos} a favor de la AGX Thor, que es una diferencia enormemente significativa (TODO: porqué?).

Las arquitecturas anteriores a Blackwell demuestran ser más conservadoras a la hora de aplicar optimizaciones, mientras que esta última arquitectura opta por las tácticas más rápidas, aunque impliquen una pérdida en la precisión. Un claro ejemplo es el modelo OSNet\_x1, la tabla \ref{tab:equips} muestra los resultados del modelo compilado con un nivel de optimización de 0 en \textit{trtexec} (builderOptimizationLevel igual a 0) que no aplica la generación de \glspl{kernel} dinámicos (que equivale a no aplicar ninguna optimización de la arquitectura y escoger la primera táctica que funcione correctamente). Al aplicar la generación de \glspl{kernel} dinámicos (nivel de optimización > 1), la Jetson AGX Thor consigue unos excelentes \textbf{1.7 milisegundos}, llegando a ser la triunfadora, a costa de una precisión en torno al \textbf{40\%}.

TensorRT exprime todas las bondades que ofrece cada arquitectura de \acrshort{gpu}s de NVIDIA, demostrando resultados que superan en rendimiento a equipos con \acrshort{cpu}s de Intel respecto a \acrshort{arm}, como es el caso de la Jetson AGX Thor, aunque en el caso de OSNet, dichas optimizaciones pueden afectar significativamente en la precisión al ser demasiado agresivas.