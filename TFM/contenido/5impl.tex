\chapter{Implementación del sistema}
\label{chap:impl}

\lettrine{E}{n} este capítulo se ahonda en los detalles de implementación de los componentes del sistema extendido. También se comentan los cambios relevantes realizados al sistema base y a los modelos para su ejecución en \acrshort{gpu}.

\section{Códigos de inferencia para el \gls{rt} de TensorRT}
En la figura \ref{fig:finalsys} se muestran los 4 modelos utilizados en el nodo cámara y que se detallarán en la sección \ref{sec:models}, todos cuentan con su implementación optimizada para OpenVINO creada en \cite{andrew}, excepto para el modelo de detección de caras (YuNet), que se ejecuta en el \gls{rt} de OpenCV. En este proyecto se han implementado nuevas versiones de las inferencias adaptadas para su uso en \acrshort{gpu} por medio de TensorRT y PyCUDA. PyCUDA ofrece una API que facilita gran parte de la interacción con CUDA, aun así, se necesita gestionar en el código temas como la creación de contextos y reserva de la memoria.

TensorRT trabaja con su propio formato de los modelos, por lo que es necesario realizar una conversión de los mismos a dicho formato mediante las herramientas expuestas en el apéndice \ref{chap:procedure}. En el proceso de conversión se aplica un proceso de selección de optimizaciones según la arquitectura de la \acrshort{gpu}, que prueba distintas tácticas para la distribución del trabajo y diferentes precisiones para quedarse con la combinación más rápida, sin comprometer la precisión del modelo.

El código \ref{coud:trtskel} muestra el esqueleto de dichas inferencias, cada vez que se crea una instancia del modelo en Python, se reservan los siguientes recursos:
\begin{itemize}
    \item \textbf{Contexto de CUDA}: en este caso se recibe como parámetro, ya que es necesario crear dicho contexto en el mismo código donde se ejecuta el bucle infinito de \acrshort{ros} (ejemplo: nodo cámara), de modo que pueda manejarse correctamente.
    \item \textbf{Contexto del modelo}: dicho contexto se crea a partir del modelo deserializado de TensorRT.
    \item \textbf{CUDA \gls{stream}}: cola que utilizará la \acrshort{gpu} para ejecutar las inferencias.
    \item \textbf{Device input}: memoria del \gls{tensor} de entrada reservado en la \acrshort{gpu}. Se especifica el tamaño en bytes del \gls{tensor} que representa los datos de entrada.
    \item \textbf{Device output}: memoria del \gls{tensor} de salida reservado en la \acrshort{gpu}. Se especifica el tamaño en bytes del \gls{tensor} que representa los datos de salida.
    \item \textbf{Bindings}: lista de direcciones de memoria de los \glspl{tensor} a utilizar durante la inferencia.
\end{itemize}

Dichos recursos se reservan \textbf{una sola vez}, que corresponde con el momento de creación de la instancia de la clase Python que representa al modelo.

La función \textit{infer} del código \ref{coud:trtskel} se ejecuta por cada vez que se recibe una entrada a procesar, la entrada puede ser un solo frame o una secuencia de estos.

Se ejecuta la función PREPROCESING con la entrada, que representa el pipeline de preprocesado específico para cada modelo. Los datos preprocesados se copian a la memoria de la \acrshort{gpu} (o \textit{device}) a través de un \textit{memcpy} y se realiza la inferencia en el \textit{\gls{stream}} de CUDA reservado. Antes de realizar la inferencia, se especifica el tamaño concreto de los datos de entrada (función \textit{set\_input\_shape}), ya que en un procesamiento en modo \gls{batch}, el conjunto de imágenes puede diferir. Finalmente, los resultados se copian de vuelta a la memoria de la \acrshort{cpu} (o \textit{host}) y se ejecuta el pipeline de postprocesado del resultado definido en la función POSTPROCESING.

Las operaciones de copia de datos y la inferencia se ejecutan de forma \textbf{asíncrona}, por lo que es necesario introducir una barrera de sincronización una vez todas las operaciones se emitan, de forma que nuevos \glspl{thread} no sobrescriben la memoria de la \acrshort{gpu} mientras esta realiza inferencias.

\section{Procesamiento de video}
La implementación sigue el flujo expuesto en la sección \ref{sec:tracker}. El cálculo del \acrshort{iou} es sencillo y no entraña apenas complejidad computacional. Previo al \acrshort{iou}, se obtiene el área a partir de las coordenadas de las \glspl{bbox} y se calcula la intersección y la unión. La distancia euclidiana se calcula como se expone en la sección \ref{subsec:reiden} y tampoco entraña mayor dificultad.

Para realizar una asignación óptima del método húngaro, se utilizó la función \textit{linear\_sum\_assignment} otorgada por el módulo \textit{optimize} de la librería SciPy \cite{lsa}. Dicha librería implementa el algoritmo Jonker-Volgenan, que es una variante del método húngaro con complejidad computacional $O(n^{3})$ \cite{lsa, o3}. Atendiendo a la elevada complejidad computacional, este algoritmo se volvería impracticable con matrices de cientos de filas y columnas (ejemplo: detección de 100 personas de una cámara en un frame y su anterior) \cite{325}. Sin embargo, como en los videos de prueba una cámara llega a detectar como máximo 4 personas a la vez, la ejecución del algoritmo no supone apenas algún coste.

\section{Personas registradas}
\label{sec:init}

En esta sección se comentan las implementaciones de los componentes encargados de crear, mantener y compartir la base de datos de las personas registradas del sistema.

\subsection{Creación y entrenamiento de las \acrshort{svm}}

El código \ref{coud:training} corresponde al proceso seguido a la hora de crear y entrenar nuevas \acrshort{svm}. Si el individuo es un desconocido, entonces todas las muestras de la base de datos sirven como datos \textbf{negativos}. En el caso de determinar una entidad conocida, es necesario \textbf{excluir las muestras del propio sujeto} del conjunto negativo, de lo contrario se estaría perdiendo el poder discriminativo de las \acrshort{svm}.

A la hora de entrenar las \acrshort{svm}, se asigna la etiqueta 1 (variable \textit{plabels}) para las muestras positivas (variable \textit{d1}) y -1 (variable \textit{labels}) para las negativas (variable \textit{samplesNegative}). OpenCV asigna los signos a las clases según el orden de los signos y las etiquetas que se presentan en el entrenamiento. En el caso de presentar primero las muestras negativas con -1 y las positivas con +1, entonces la \acrshort{svm} devuelve como máximo -1 cuando una muestra pertenece a la clase positiva, siendo -1 la distancia al hiperplano desde la clase positiva, en caso contrario se devolverá como máximo un +1 para las clases negativas. Se ha aplicado este comportamiento para que la fusión de puntuaciones mediante la mediana \textbf{funcione} (ya que ordena los valores de menor a mayor), pero la \acrshort{svm} podría entrenarse para causar el efecto contrario, solo es necesario cambiar el orden de aparición de las etiquetas y los signos en el entrenamiento.

\subsection{TODO: Método de inicialización no supervisado}

Como ya se explicó en la sección \ref{sec:initarch}, se parte únicamente de la información de las cámaras en el momento de ejecución del sistema, en el caso de las pruebas realizadas se utilizan los frames de los videos grabados. Para la implementación del algoritmo de la figura \ref{fig:init} se ha podido aprovechar el módulo de procesamiento de video para la fase de recolección de \glspl{bbox}. El resto de detalles de implementación no entrañan nada destacable.

\section{Escalabilidad y tolerancia a fallos de las cámaras}

Cuando se fusionan los resultados procesados por los distintos sensores (nodo integración de sensores en la figura \ref{fig:finalsys}), se requiere que todos ellos se encuentren \textbf{sincronizados} dentro de un intervalo temporal (ejemplo: 100 milisegundos), de forma que las predicciones no se realizan a partir de información desactualizada.

En \cite{andrew} se utiliza una clase del paquete \textit{message\_filters} de \acrshort{ros} llamado \textit{ApproximateTimeSynchronizer}, que utiliza un algoritmo adaptativo para emparejar mensajes a partir de su \gls{timestamp} \cite{ApproximateTime}. El problema de \textit{ApproximateTimeSynchronizer} es que espera recibir datos \textbf{de todas las fuentes en todo momento}, de modo que si uno de los sensores falla, el sistema \textbf{se congela} debido a que se dejan de recibir mensajes de dicha fuente.

Se ha decidido sustituir dicha clase por \textit{MessageFiltersCache} de la misma librería \cite{MFC}. En esta nueva implementación, cada nodo posee una caché en la que se almacenan sus mensajes, que pueden recuperarse especificando un timestamp.

El código \ref{coud:cash} implementa una función (\textit{process}) que recoge los datos de las cachés a una frecuencia fija (ejemplo: 10 Hz), de modo que se recuperan los últimos mensajes de los sensores que emitieron dentro del intervalo, sin esperar por sensores que hayan sufrido latencias o fallos.

Se otorga una ventana de 500 ms para mensajes que llegan con cierto retardo antes de su descarte. Si un nodo no procesa y envía su mensaje antes de acabar el intervalo (ejemplo: 100ms), \textbf{se sigue aplicando el último mensaje recibido de este}, siempre y cuando no se exceda la ventana de tiempo ya comentada.

\section{Migración a ROS 2}
\label{subsec:ROS2}

Con el motivo del fin de soporte de ROS 1 \cite{ROSEOL}, se ha optado por migrar el sistema para ser ejecutado en ROS 2 con el fin de mantener su continuidad.

El proceso de migración se ha llevado a cabo por medio de la guía oficial de \acrshort{ros} \cite{ros2tuto}. Muchos de los problemas encontrados en este proceso están relacionados con los nuevos archivos de configuración.

Se han migrado los nodos cámara e integrador. El proceso incluye cambiar las firmas de las funciones, sus parámetros y cambiar de paquetes. En esencia, la mayoría de paquetes de \acrshort{ros} 2 preservan las mismas funcionalidades e incluso mantienen las mismas interfaces de las funciones de \acrshort{ros} 1, lo que ha facilitado la correcta migración.

La migración del nodo \acrshort{lidar} no ha sido posible, debido a que el código depende de versiones de librerías no soportadas a partir de la versión 22.04 de Ubuntu y de muchos componentes de \acrshort{ros} 1, eliminados en \acrshort{ros} 2. Sería necesario rediseñar todo el código o optar por un paquete de \acrshort{ros} 2 con las mismas funcionalidades que el paquete \textit{hdl\_people\_tracking}, utilizado en \cite{andrew} para detectar personas en nubes de puntos \acrshort{3d}.

En ROS 2 se ha introducido la posibilidad de escribir scripts de lanzamiento en Python. El archivo de lanzamiento del sistema se ha convertido del formato \acrshort{xml} a un script de Python para aprovechar la flexibilidad que el propio lenguaje ofrece y utilizar las nuevas funcionalidades. %Ref a esto: https://docs.ros.org/en/foxy/How-To-Guides/Launch-file-different-formats.html#launch-file-examples