\chapter{Implementación del sistema}
\label{chap:impl}

\lettrine{E}{n} este capítulo se ahonda en los detalles de implementación de los nuevos componentes, así como los cambios realizados en el propio sistema base y los detalles de la migración a \acrshort{ros} 2. También se exponen los cambios en las inferencias de los modelos para su ejecución en TensorRT.

Los nuevos componentes \textbf{no se han integrado en el sistema base}, sino que se han desarrollado y probado de forma \textbf{independiente} (en el capítulo \ref{chap:systesting} se prueban dichos componentes), por lo tanto no se ha implementado la \textbf{arquitectura extendida} (sección \ref{sec:finalsys}) ni ninguno de los métodos de compartición vistos en la sección \ref{sec:shared}. Se ha omitido comentar el módulo de personas registradas, puesto a que su lógica no entraña nada destacable. Adicionalmente, la implementación del reconocimiento adaptativo se encuentra en el apéndice \ref{sec:adaptimpel}.

\section{Procesamiento de vídeo}
La implementación sigue el flujo expuesto en la figura \ref{fig:reident} (sección \ref{sec:vídeo}). El cálculo del \acrshort{iou} es sencillo y no entraña apenas complejidad computacional, se calcula la intersección de las 2 \glspl{bbox} a partir de sus coordenadas, mientras que la unión se obtiene a partir de la suma de las áreas que encierran las 2 \glspl{bbox} \textbf{menos} la intersección. Finalmente, se calcula el \acrshort{iou} según la ecuación \ref{eq:iou} (sección \ref{sec:vídeo}). La distancia euclidiana se calcula como se expone en la sección \ref{sec:vídeo} y tampoco entraña mayor dificultad.

Se utilizó la función \textit{linear\_sum\_assignment} de la librería SciPy \cite{lsa} como implementación del \textbf{método húngaro}. Dicha librería implementa el algoritmo Jonker-Volgenan, que es una variante del método húngaro con complejidad computacional \textbf{$O(n^{3})$} \cite{lsa, o3}, por lo que se vuelve impracticable para matrices en el orden de miles de elementos (ejemplo: detección de 100 personas de una cámara en un frame y su anterior) \cite{325}. Sin embargo, como en los vídeos de prueba una cámara llega a detectar como máximo 4 personas a la vez, la ejecución del algoritmo no supone apenas algún coste.

%\section{Personas registradas}
%\label{sec:init}

%En esta sección se comentan las implementaciones de los componentes encargados de crear la base de datos de las personas registradas del sistema.

\section{Creación y entrenamiento de las SVM}
\label{sec:training}

Para este proyecto, se ha utilizado la implementación de las \acrshort{svm} ofrecida por la librería de OpenCV (cv2). El código \ref{coud:training} corresponde al proceso seguido a la hora de crear y entrenar nuevas \acrshort{svm}. Si el individuo es un desconocido, entonces todas las muestras de la base de datos sirven como datos \textbf{negativos}, en caso contrario, es necesario \textbf{excluir las muestras del propio sujeto} del conjunto de negativos, de lo contrario se estaría perdiendo el poder discriminativo de las \acrshort{svm}. En el entrenamiento, se asigna la \textbf{puntuación} 1 (variable \textit{plabels}) para las muestras positivas (variable \textit{d1}) y -1 (variable \textit{nlabels}) para las negativas (variable \textit{neg\_samples}). OpenCV asigna los signos a las clases según el orden de estos junto a las etiquetas que se presentan en el entrenamiento. En el caso de presentar primero las muestras negativas con -1 y las positivas con +1, entonces la \acrshort{svm} devuelve como máximo -1 cuando una muestra pertenece a la clase positiva y como máximo un +1 para las clases negativas. Se ha aplicado este comportamiento debido a que se utiliza la \textbf{mediana} (que ordena los valores de menor a mayor) para obtener el mínimo de las puntuaciones. En cambio, las \acrshort{svm} podrían entrenarse para causar el efecto contrario, solo se requiere de cambiar el orden de aparición de las etiquetas y los signos en el entrenamiento.

%\section{Método de inicialización no supervisado}

%Como ya se explicó en la sección \ref{sec:initarch}, en el método de inicialización no supervisado se crean los comités de las nuevas entidades únicamente de la información disponible en el momento de ejecución del sistema. Para la implementación del algoritmo de la figura \ref{fig:init} se ha podido aprovechar el módulo de procesamiento de vídeo para la fase de recolección de \glspl{bbox}. El resto de detalles de implementación no entrañan nada destacable.

\section{Migración a ROS 2}
\label{subsec:ROS2}

Con el motivo del fin de soporte de \acrshort{ros} 1 \cite{ROSEOL}, se ha optado por migrar el sistema para ser ejecutado en \acrshort{ros} 2 con el fin de mantener su continuidad. El proceso de migración se ha llevado a cabo por medio de la guía oficial de \acrshort{ros} \cite{ros2tuto}. Muchos de los problemas encontrados en este proceso están relacionados con los nuevos archivos de configuración y de lanzamiento del sistema (cambio del formato \acrshort{xml} a Python de este último \cite{lanch}).

Se han migrado los nodos \textbf{cámara e integrador}. El proceso incluye cambiar las firmas de las funciones, sus parámetros y cambiar de paquetes. En esencia, la mayoría de paquetes de \acrshort{ros} 2 preservan las mismas funcionalidades e incluso mantienen las mismas interfaces de las funciones de \acrshort{ros} 1, lo que ha facilitado dicho proceso. La migración del nodo \acrshort{lidar} \textbf{no ha sido posible}, debido a que el código depende de versiones de librerías no soportadas a partir de la versión 22.04 de Ubuntu y de varios componentes de \acrshort{ros} 1 eliminados en \acrshort{ros} 2. Sería necesario rediseñar todo el código o optar por un paquete de \acrshort{ros} 2 con las mismas funcionalidades que el paquete \textit{hdl\_people\_tracking}, utilizado en \cite{andrew} para detectar personas en nubes de puntos \acrshort{3d}.

\section{Escalabilidad y tolerancia a fallos de las cámaras}
\label{sec:scal}

Todos los nodos del sistema requieren estar \textbf{sincronizados} dentro de un intervalo temporal (ejemplo: 100 milisegundos), de modo que el nodo integración de sensores no fusione información \textbf{desactualizada}. En \cite{andrew} se utiliza \textit{ApproximateTimeSynchronizer} del paquete \textit{message\_filters} de \acrshort{ros}, que implementa un algoritmo adaptativo para emparejar mensajes a partir de su \gls{timestamp} \cite{ApproximateTime}. El problema de \textit{ApproximateTimeSynchronizer} es que espera recibir datos \textbf{de todas las fuentes en todo momento}, de modo que si uno de los sensores falla, el sistema \textbf{se congela}. Como alternativa a la anterior solución, se ha propuesto que el nodo integrador \textbf{mantenga una caché} por cada nodo del que reciba mensajes, que pueden recuperarse especificando un \gls{timestamp}, de forma que si un nodo no publica a tiempo su mensaje, el integrador sigue recuperando su último mensaje en la caché dentro de un límite de \textbf{200 milisegundos} (tras superar dicho límite, el mensaje se descarta). Las cachés se implementan por medio de la clase \textit{MessageFiltersCache} del mismo paquete de \acrshort{ros} \cite{MFC}, que actúan de suscriptores de los tópicos de cada nodo.

El código \ref{coud:cash} implementa una función (\textit{process}) que recoge los datos de las cachés a una frecuencia fija (ejemplo: 10 Hz), de modo que se recuperan los últimos mensajes de los nodos que emitieron dentro del intervalo, sin esperar por nodos que hayan sufrido latencias o fallos. Todos los mensajes se guardan en una lista (variable \textit{message\_list}) con la que se llama a la fusión de los resultados (función \textit{on\_frame}) para devolver las detecciones finales.

Por otro lado, el sistema original no había sido diseñado para gestionar las detecciones provenientes de cámaras con rangos de visión \textbf{parcialmente solapados} (en las pruebas del capítulo \ref{chap:syscal} se trata este caso, puesto que se duplican literalmente las cámaras), de forma que si más de 2 cámaras detectan a la misma persona, dicha detección \textbf{se descarta en ambas cámaras}, lo que es una decisión coherente cuando se cuenta con una configuración con cámaras \textbf{sin solape}. Se ha modificado el código para evitar que en dicha situación se eliminen las detecciones y considerar solo una de ellas en el nodo integración de sensores.

\section{Códigos de inferencia para el runtime de TensorRT}
\label{sec:trtinfer}

En este proyecto se han implementado nuevas versiones de las inferencias adaptadas para su uso en \acrshort{gpu} por medio de TensorRT y PyCUDA. PyCUDA ofrece una \acrshort{api} que facilita gran parte de la interacción con \gls{CUDA}, aun así, el programador necesita gestionar temas como la creación de contextos y reserva de la memoria.

%TensorRT trabaja con su propio formato de los modelos, por lo que es necesario realizar una conversión de los mismos a dicho formato mediante las herramientas expuestas en el apéndice \ref{chap:procedure}. En el proceso de conversión se aplica un proceso de selección de optimizaciones según la arquitectura de la \acrshort{gpu}, que prueba distintas tácticas para la distribución del trabajo y diferentes precisiones para quedarse con la combinación más rápida, sin comprometer la precisión del modelo.

El código \ref{coud:trtskel} muestra el esqueleto de dichas inferencias, cada vez que se crea una instancia del modelo en Python (invocación a \textit{\_\_init\_\_}), se reservan los siguientes recursos:
\begin{description}
    \item[\gls{CUDA} context] Entorno en el que se ejecutan los \glspl{stream} de \gls{CUDA}. A nivel del sistema, se reserva un contexto por nodo cámara, que es compartido a su vez por todos sus modelos.
    \item[\gls{CUDA} \gls{stream}] Cola de operaciones que utilizará la \acrshort{gpu} para realizar las transferencias de memoria y ejecutar los \glspl{kernel} de las inferencias. A nivel de sistema, se reserva un \gls{stream} para cada modelo.
    \item[Execution context] Contexto para ejecutar las inferencias a partir de un \textbf{CudaEngine} (modelo deserializado). Se pueden crear múltiples contextos de ejecución para un mismo \textbf{CudaEngine}.
    \item[Device input] Memoria del \gls{tensor} de entrada reservado en la \acrshort{gpu} mediante un \textbf{mem\_alloc}. Se especifica el tamaño en bytes del \gls{tensor} que representa los datos de entrada.
    \item[Device output] Memoria del \gls{tensor} de salida reservado en la \acrshort{gpu} mediante un \textbf{mem\_alloc}. Se especifica el tamaño en bytes del \gls{tensor} que representa los datos de salida.
    \item [Bindings] Lista de direcciones de memoria de los inputs/outputs de la inferencia. Aplica a uno o a varios \textbf{execution contexts}.
\end{description}

La función \textit{infer} del código \ref{coud:trtskel} se invoca por cada vez que se recibe una \textbf{imagen} (o un \gls{batch} de imágenes) a procesar. Se ejecuta la función PREPROCESSING con la entrada, que representa el pipeline (conjunto de operaciones) de preprocesado específico de la imagen para cada modelo. Los datos preprocesados se copian a la memoria de la \acrshort{gpu} (o \textit{device}) a través de un \textit{memcpy} y se realiza la inferencia en el \textbf{execution context} por medio del \textit{\gls{stream}} de \gls{CUDA} reservado. Antes de la inferencia, se especifica el tamaño concreto de los datos de entrada (función \textit{set\_input\_shape}), ya que en un procesamiento en modo \gls{batch}, el conjunto de imágenes puede diferir. Finalmente, los resultados se copian de vuelta a la memoria de la \acrshort{cpu} (o \textit{host}) y se ejecuta el pipeline de postprocesado del resultado definido en la función POSTPROCESSING. Las operaciones de copia de datos y la inferencia se ejecutan de forma \textbf{asíncrona}, por lo que es necesario introducir una barrera de sincronización \textbf{a nivel de \gls{stream}} una vez todas las operaciones se emitan, de forma que la próxima llamada a \textit{infer} no sobrescriba la memoria de la \acrshort{gpu} mientras esta sigue realizando la inferencia anterior.