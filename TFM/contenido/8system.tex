\chapter{Pruebas de escalabilidad del sistema}
\label{chap:syscal}

\lettrine{E}{s}te capítulo pretende evaluar el techo del sistema al elevar el número de cámaras implicadas.

\section{Dataset de las pruebas}

En el caso que ocupa al sistema (pruebas de la sección \ref{sec:syscal}), se dispone de un \gls{dataset} que representa el \gls{gt} del nodo integración de sensores (ver sección \ref{sec:basearch}), ya que cada entrada del fichero es idéntica a la salida de dicho nodo. La prueba de la que se ha extraído el \gls{dataset} corresponde con la misma del \gls{dataset} de las cámaras. Cada entrada del fichero \acrshort{json} está compuesta de los siguientes objetos:
\begin{itemize}
    \item \textbf{\textit{secs}}: \gls{timestamp} del tiempo de UNIX en segudos
    \item \textbf{\textit{nsecs}}: \gls{timestamp} del tiempo en nanosegundos del segundo correspondiente al campo secs.
    \item \textbf{\textit{people}}: lista de objetos, cada uno compuesto por los siguientes elementos:
          \begin{itemize}
              \item \textbf{\textit{person\_id}}: etiqueta que identifica al usuario detectado (ejemplo: carlos)
              \item \textbf{\textit{position}}: coordenadas del espacio \acrshort{3d} compartido por las cámaras y el \acrshort{lidar} en el que se sitúa dicho usuario.
          \end{itemize}
\end{itemize}

\section{Realización de las pruebas}

Es importante remarcar que todas las pruebas de esta sección se realizan en \textbf{tiempo real}. Según el proyecto, la frecuencia del tiempo real puede variar, en este caso, se espera que el sistema trabaje a \textbf{10 Hz}, que es la frecuencia a la que ambos sensores \acrshort{lidar} y Kinect devuelven datos.

Se inicializa el sistema con el nodo integración de sensores, el nodo \acrshort{lidar} y el número de nodos cámara deseado (ver sección \ref{sec:basearch}). Se reproduce el fichero rosbag una vez inicializado el sistema, que lo nutrirá con la información generada por los sensores en \textbf{tiempo real}. Cada predicción generada por el nodo integrador se almacena en memoria y se vuelca en un fichero \acrshort{json} una vez el sistema se detiene (por ejemplo, con un Ctrl-C). Los datos guardados en el fichero \acrshort{json} se procesan contra el \textit{\gls{dataset}} y se devuelven los resultados finales en forma de las siguientes métricas:

%En cada iteración del procesado, se ejecutan \textbf{4 redes neuronales (\acrshort{cnn})}. 2 de ellas, YOLOv8n y YuNet, para la detección de cuerpos y de caras respectivamente. Las otras 2, OSNet\_x1 y ArcFace, para el reconocimiento de cuerpos y caras respectivamente. A pesar de lo que la figura \ref{fig:finalsys} muestra, las redes de detección YOLOv8n y YuNet \textbf{no se ejecutan en paralelo}, lo mismo sucede con las redes ArcFace y OSNet\_x1. Se ha considerado ejecutar en paralelo estas redes. Sin embargo, las limitaciones de memoria principal de la Jetson restringen tomar esta aproximación.

\begin{itemize}
    \item \textit{\textbf{Det precision}}: es la proporción de personas detectadas en la posición correcta (campo \textit{position} del \gls{dataset}) respecto al total de personas detectadas.
    \item \textit{\textbf{Det recall}}: representa la proporción de personas detectadas en la posición correcta respecto al total de personas en el \gls{dataset}.
    \item \textit{\textbf{Ident F1 score}}: corresponde con el \textit{F1\_score} global de los modelos de reconocimiento.
    \item \textit{\textbf{Ident precision}}: corresponde con la precisión global de los modelos de reconocimiento.
\end{itemize}

Los modelos de todas las pruebas realizadas en esta sección se han ejecutado en su versión de \textbf{TensorRT} (salvo YuNet, que utiliza CUDA por medio de OpenCV).

\section{Entorno de las pruebas}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imagenes/SYSTESTING.jpg}
    \caption{Reproducción y transmisión del rosbag por la red}
    \label{fig:systesting}
\end{figure}

La figura \ref{fig:systesting} ilustra el proceso comentado al inicio de la sección. Debido a que el rosbag utilizado es un archivo pesado (41 \acrshort{gb}), en ciertas situaciones ha sido necesario reproducir dicho fichero desde una \textbf{fuente externa} al no disponer de almacenamiento local suficiente.

La comunicación con la fuente externa se realiza a través de una red \acrshort{lan} \textbf{Gigabit Ethernet}, que posee un ancho de banda limitado para la transmisión de imágenes \acrshort{rgbd}. La fuente externa debe de enviar $(|imagen\_RGB| + |imagen\_profundidad)*nºcams + |nube\_de\_puntos\_LiDAR|$ megabytes de datos cada 100 ms (que es la frecuencia a la que trabaja el sistema), siendo $|imagen\_RGB| = 40 MB/s$, $|imagen\_profundidad| = 20 MB/s$ y $|nube de puntos LiDAR| = 5 MB/s$, lo que supera el ancho de banda máximo (125 \acrshort{mb}/s) utilizando únicamente dos cámaras.

Para reducir el ancho de banda necesario para la transmisión, se ha optado por desplegar nodos encargados de comprimir la imagen \acrshort{rgb} desde el equipo en el que se reproduce el rosbag (\textit{remote side} en la figura \ref{fig:systesting}). La compresión se realiza por medio del paquete \textit{image\_transport} de \acrshort{ros}, que logra reducir hasta 10 veces el tamaño de la imagen (resultando en 4 \acrshort{mb}/s). Los nodos cámara (\textit{host side} en la figura \ref{fig:systesting}) se suscriben al tópico con la imagen comprimida y se descomprime en el destino.

En el caso de la imagen de distancia, en sí no es un objeto pesado, por lo que no necesita compresión. Sin embargo, la frecuencia elevada de transmisión (30 Hz), hace que se requiera de un mayor ancho de banda. Como el resto de los sensores funcionan a 10 Hz, la tasa de la imagen de profundidad puede reducirse a dicha frecuencia, de forma que el sistema no percibe el cambio y se logra reducir el ancho de banda. Con reducir la tasa de envío del nodo de \acrshort{ros} encargado de publicar la imagen sería suficientemente, pero debido a que el rosbag se grabó dicho nodo a 30 Hz, se ha optado por ejecutar el paquete \textit{topic\_tools} de \acrshort{ros}, que permite publicar una réplica de un tópico a una menor frecuencia.

Debido a que el rosbag sólo contiene los datos de 2 cámaras, para realizar la prueba de escalabilidad con un mayor número de cámaras, ha sido necesario \textbf{replicar} los tópicos de las imgágenes mediante el paquete de \acrshort{ros} \textit{topic\_tools} (figura \ref{fig:systesting}), de forma que los nuevos nodos se pueden suscribir a dichos tópicos.

\section{Resultados}

\input{contenido/tables/tests/scaleperf.tex}

La tabla \ref{tab:scaleperf} muestra los resultados de los dispositivos en función del número de cámaras. La columna \textit{Reference} muestra los resultados del sistema obtenidos en \cite{andrew}, en el que solo se registraron datos del sistema funcionando con 2 cámaras.

Atendiendo a los datos de precisión en detección y reconocimiento, se muestra una ligera caída en contra de los dispositivos del proyecto, que puede deberse a su vez por la subida en el \textit{recall} (TODO: puede ser), ya que se han generado más detecciones. La subida en el \textit{recall} demuestra que el sistema tiene una mayor capacidad de procesamiento, en cierta medida influida por la potencia de los procesadores Intel Core i7 respecto a Core i5.

La precisión en la detección empeora al aumentar el número de cámaras. Ya que el nodo integrador debe de fusionar los datos procedentes de todos los sensores, al aumentar su número también se aumenta la cantidad de procesamiento. Por lo tanto, más tiempo se demorará en devolver una predicción, que se emparejará con el frame del \gls{dataset} con el mismo \gls{timestamp}, que corresponde a un instante real \textbf{anterior}.

El recall también empeora al aumentar la cantidad de cámaras, debido a la alta demanda de cómputo del sistema (para 10 cámaras, se deben de procesar un total de 40 redes neuronales en 100 milisegundos) lo que hace que se pierdan los frames que no tengan espacio en la cola. Sin embargo, la caída de rendimiento en ambos dispositivos \textbf{no es crítica} y permite atender a la mayoría de las peticiones, aunque los resultados reflejan una clara congestión en los nodos.

La precisión de los reconocimientos no baja del \textbf{80\%} en todos los casos, lo que demuestra que los modelos de reconocimiento funcionan de manera eficaz ante un enorme estrés del sistema. El \textit{F1\_score} del reconocimiento depende del \textit{recall} de detección, por lo que es de esperar un valor bajo, compensado en cierta medida por la precisión en el reconocimiento.

\input{contenido/tables/tests/scaleresources.tex}

La tabla \ref{tab:scaleresources} muestra el uso de recursos durante la prueba para los equipos de sobremesa. Se puede apreciar que, hasta las 10 cámaras, el sistema se desempeña de forma fluida, con una media de poco más de la mitad de recursos utilizados.

Los recursos de la \acrshort{gpu} se llevan al límite con 10 cámaras. La gráfica del portátil (Ampere) frente a la del PC del laboratorio (Turing) es capaz de gestionar mejor la memoria (debido a \dots). Por otro lado, la \acrshort{gpu} Ampere es capaz de lidiar mejor con la carga, debido a TODO: el número de SM seguramente.

El uso elevado de recursos explica, junto a otros factores, la bajada en el recall cuando se ejecutan 10 nodos cámara simultáneamente, más el nodo \acrshort{lidar} y el integrador.

\input{contenido/tables/tests/scaleros2.tex}

En la tabla \ref{tab:scaleros2} se muestra el rendimiento del sistema en \textbf{\acrshort{ros} 2} para el resto de equipos Jetson. Los resultados marcados con guion indican ausencia del dato cuando se realizó la prueba o imposibilidad de realizar dicha prueba (Jetson Orin Nano con 10 cámaras).

En estos dispositivos no es posible instalar \acrshort{ros} Noetic, al menos para poder contar con las últimas versiones del software de los mismos. Por los motivos comentados en la sección \ref{subsec:ROS2}, el sistema en \acrshort{ros} 2 no puede ejecutar el nodo \acrshort{lidar}, por lo que los resultados mostrados sólo tienen en cuenta los nodos cámara.

El recall de las detecciones se muestra claramente inferior respecto a los anteriores resultados, ya que el \acrshort{lidar} otorgaba cobertura completa a la reducida visión de las cámaras de aproximadamente 117º. Las 3 Jetson logran superar el valor de referencia en esta métrica, lo que demuestra el excelente rendimiento de las \acrshort{gpu}, capaces de compensar las limitaciones de una \acrshort{cpu} de \acrshort{arm} respecto a un Intel Core i5 (procesador del equipo de referencia).

La precisión en la detección empeora en los dispositivos Jetson respecto a los de sobremesa cuando se ejecutan 5 cámaras simultáneamente. Debido a la menor capacidad de procesamiento de la Jetson Orin Nano, es de esperar que muchos de los mensajes procesados por los nodos cámara llegan tarde al nodo integrador, que devuelve una posición desactualizada respecto al frame de referencia, por lo tanto, se asume la detección como \textbf{errónea}. TODO: y en la Jetson AGX Thor?, a pesar de la notable caída en la precisión, que es de la misma magnitud que en los dispositivos de sobremesa, el recall se mantiene constante e incluso mejora con 5 cámaras (TODO: querrá decir que va sobrada).

TODO: se ha conseguido optimizar YOLO en GPU, que suponía el cuello de botella del sistema para funcionar en tiempo real.

\input{contenido/tables/tests/scaleresourcesros2.tex}

En la tabla \ref{tab:scaleresourcesros2} se muestra la proporción de recursos utilizados en las pruebas del sistema en \acrshort{ros} 2. TODO: Como en la tabla \ref{tab:scaleresources}, se expone la mediana de todas las lecturas de recursos en una ejecución completa del sistema, para ignorar los picos de uso de los recursos.

Ambos dispositivos pueden ejecutar holgadamente el sistema usando 2 cámaras, en el caso de las 5 cámaras es cuando se empiezan a dislumbrar las limitaciones de la Jetson Orin Nano, principalmente en términos de memoria, lo que explica la caída en la precisión de las detecciones, que no permite a los modelos reservar memoria para varias de las inferencias.

Los 8 GB de memoria rápidamente se quedan escasos, debido a todos los recursos necesarios a reservar, como contextos de CUDA, el modelo deserializado, streams de CUDA, entre otros, que pesan en torno a cientos de \acrshort{mb}, que se multiplican por 4 modelos y a su vez por 5 cámaras. Debido al excesivo uso de memoria, se han aplicado ciertas optimizaciones de los códigos de Python, que han permitido ahorrar hasta \textbf{400 \acrshort{mb}} en la inicialización del sistema, y que se citan en el apéndice \ref{chap:optimus}.

\section{Discusión}

En varios equipos, se ha logrado escalar el sistema hasta un número de cámaras elevado (10), al menos para ser procesado por un solo dispositivo. La Jetson Orin Nano ha conseguido mantener resultados aceptables hasta las 5 cámaras, lo que demuestra la potencia de este pequeño dispositivo.

Es importante destacar que las pruebas se realizaron con \textbf{6 personas registradas en la base de datos}. Aumentar este número tendría consecuencias en los modelos de reconocimiento, ya que iteran toda la base de datos para calcular las distancias del vector con cada individuo, para devolver la mejor coincidencia. Según el dispositivo, esta operación tarda menos o alrededor de 1 milisegundo, lo que podría aumentar en varios milisegundos si se añaden 100 personas.