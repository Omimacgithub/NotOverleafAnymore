\chapter{Pruebas de escalabilidad del sistema}
\label{chap:syscal}

\lettrine{E}{n} este capítulo se realiza una evaluación del rendimiento del sistema al elevar el número de cámaras implicadas.

\section{Realización de las pruebas}

Para obtener resultados de precisión del sistema, se dispone de un \gls{dataset} que representa el \gls{gt} del nodo integración de sensores (ver sección \ref{sec:basearch}). La prueba de la que se ha extraído el \gls{dataset} corresponde con la misma del \gls{dataset} de las cámaras. En el apéndice \ref{sec:datsys} se comenta su composición.

Es importante remarcar que todas las pruebas de esta sección se realizan en \textbf{tiempo real}. Según el proyecto, la frecuencia del tiempo real puede variar, en este caso, se espera que el sistema trabaje a \textbf{10 Hz} (o 10 \acrshort{fps}), que es la frecuencia a la que ambos sensores \acrshort{lidar} y Kinect devuelven datos.

Se inicializa el sistema con el nodo integración de sensores, el nodo \acrshort{lidar} y el número de nodos cámara deseado (ver sección \ref{sec:basearch}). Se reproduce el fichero \gls{rosbag} una vez inicializado el sistema, que lo nutrirá con la información generada por los sensores en \textbf{tiempo real}. Cada predicción generada por el nodo integrador se almacena en memoria y se vuelca en un fichero \acrshort{json} una vez el sistema se detiene (por ejemplo, con un Ctrl+C). Los datos guardados en el fichero \acrshort{json} se procesan contra el \textit{\gls{dataset}} y se devuelven los resultados finales en forma de las siguientes métricas:

%En cada iteración del procesado, se ejecutan \textbf{4 redes neuronales (\acrshort{cnn})}. 2 de ellas, YOLOv8n y YuNet, para la detección de cuerpos y de caras respectivamente. Las otras 2, OSNet\_x1 y ArcFace, para el reconocimiento de cuerpos y caras respectivamente. A pesar de lo que la figura \ref{fig:finalsys} muestra, las redes de detección YOLOv8n y YuNet \textbf{no se ejecutan en paralelo}, lo mismo sucede con las redes ArcFace y OSNet\_x1. Se ha considerado ejecutar en paralelo estas redes. Sin embargo, las limitaciones de memoria principal de la Jetson restringen tomar esta aproximación.

\begin{itemize}
    \item \textit{\textbf{Det precision} (det\_p)}: es la proporción de personas detectadas en la posición correcta (campo \textit{position} del \gls{dataset}) respecto al total de personas detectadas.
    \item \textit{\textbf{Det recall} (det\_r)}: representa la proporción de personas detectadas en la posición correcta respecto al total de personas en el \gls{dataset}.
    \item \textit{\textbf{Ident F1 score} (idf1)}: corresponde con el \textit{F1\_score} global de los modelos de reconocimiento (ver apéndice \ref{subsec:recon} para más información).
    \item \textit{\textbf{Ident precision} (idp)}: corresponde con la precisión global de los modelos de reconocimiento.
\end{itemize}

La prueba del sistema da una detección como positiva si la posición predicha es igual a la del \gls{dataset} dentro de una holgura de 40 cm.

Siendo \textit{det\_tp} una posición predicha correctamente, \textit{det\_fp} una posición predicha incorrectamente, \textit{n\_gt} el número total de detecciones del \gls{dataset}, \textit{id\_tp} una predicción correcta de la identidad y \textit{id\_fp} una predicción incorrecta de la identidad, se definen las siguientes métricas:
\begin{itemize}
    \item $det\_p = \frac{det\_tp}{(det\_tp + det\_fp)}$
    \item $det\_r = \frac{det\_tp}{n\_gt}$
    \item $idf1 = \frac{2 * id\_tp}{2 * (id\_tp + id\_fp + (n\_gt - det\_tp))}$
    \item $idp = \frac{id\_tp}{(id\_tp + id\_fp)}$
\end{itemize}

Los modelos de todas las pruebas realizadas en esta sección se han ejecutado en el \gls{rt} de \textbf{TensorRT} (salvo YuNet, que utiliza CUDA por medio de OpenCV).

\section{Entorno de las pruebas}

Debido a que el \gls{rosbag} utilizado es un archivo pesado (41 \acrshort{gb}), en ciertas situaciones ha sido necesario reproducir dicho fichero desde una \textbf{fuente externa} al no disponer de almacenamiento local suficiente.

La comunicación con la fuente externa se realiza a través de una red \acrshort{lan} \textbf{Gigabit Ethernet}, que posee un ancho de banda limitado para la transmisión de imágenes \acrshort{rgbd} (máximo teórico de 125 \acrshort{mb}/s). La fuente externa debe de enviar $(|imagen\_RGB| + |imagen\_profundidad)*nºcams + |nube\_de\_puntos\_LiDAR|$ megabytes de datos cada 100 ms (que es la frecuencia a la que trabaja el sistema), siendo $|imagen\_RGB| = 40 \acrshort{mb}/s$, $|imagen\_profundidad| = 20 \acrshort{mb}/s$ y $|nube\_de\_puntos\_LiDAR| = 5 \acrshort{mb}/s$, lo que supera el ancho de banda máximo (125 \acrshort{mb}/s) utilizando únicamente dos cámaras (\textit{ncams}=2).

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imagenes/SYSTESTING.jpg}
    \caption{Reproducción y transmisión del \gls{rosbag} por la red}
    \label{fig:systesting}
\end{figure}

La figura \ref{fig:systesting} ilustra el proceso de transmisión de los datos por la red \acrshort{lan}.

Para reducir el ancho de banda necesario para la transmisión, se ha optado por desplegar nodos encargados de comprimir la imagen \acrshort{rgb} desde el equipo en el que se reproduce el \gls{rosbag} (\textit{remote side} en la figura \ref{fig:systesting}). La compresión se realiza por medio del paquete \textit{image\_transport} de \acrshort{ros}, que logra reducir hasta 10 veces el tamaño de la imagen \acrshort{rgb} (resultando en 4 \acrshort{mb}/s). Los nodos cámara (\textit{host side} en la figura \ref{fig:systesting}) se suscriben al tópico generado por el nodo que descomprime la imagen (\textit{image\_transport} \textit{decompress} en el \textit{host side}).

En el caso de la imagen de distancias, en sí no es un objeto pesado, por lo que no necesita compresión. Sin embargo, la frecuencia elevada de transmisión (30 Hz), hace que se requiera de un mayor ancho de banda. Como el resto de los sensores funcionan a 10 Hz, la tasa de la imagen de distancias puede reducirse a dicha frecuencia, de forma que el sistema no percibe el cambio y se logra reducir el ancho de banda. Con reducir la tasa de envío del nodo de \acrshort{ros} encargado de publicar la imagen sería suficiente, pero debido a que en el \gls{rosbag} dicho nodo se grabó a 30 Hz, se ha optado por ejecutar el paquete \textit{topic\_tools} de \acrshort{ros}, que permite publicar una réplica de un tópico a una menor frecuencia.

Debido a que el \gls{rosbag} solo contiene los datos de 2 cámaras, para realizar la prueba de escalabilidad con un mayor número de cámaras, ha sido necesario \textbf{replicar} los tópicos de las imágenes mediante el paquete de \acrshort{ros} \textit{topic\_tools} (figura \ref{fig:systesting}), de forma que los nuevos nodos se pueden suscribir a dichos tópicos.

\section{Resultados}

\input{contenido/tables/tests/scaleperf.tex}

La tabla \ref{tab:scaleperf} muestra los resultados de los dispositivos en función del número de cámaras del sistema en \textbf{\acrshort{ros} 1}. La columna \textit{Reference} muestra los resultados obtenidos en \cite{andrew}, en el que solo se registraron datos del sistema funcionando con 2 cámaras.

Atendiendo a los datos de precisión en detección y reconocimiento, el \textit{det\_r} indica que el número de positivos ha aumentado en los equipos de sobremesa respecto a la referencia y al mismo tiempo el número de negativos, como indica la ligera caída de la precisión en contra de los dispositivos del proyecto.

%La subida en el \textit{recall} demuestra que el sistema tiene una mayor capacidad de procesamiento, en cierta medida influida por la potencia de los procesadores Intel Core i7 respecto a Core i5 (equipo de referencia).

La precisión en la detección empeora al aumentar el número de cámaras,

%a que el nodo integrador debe de fusionar los datos procedentes de todos los sensores, al aumentar su número también se aumenta la cantidad de procesamiento. Por lo tanto, más tiempo se demorará en devolver una predicción, que se emparejará con el frame del \gls{dataset} con el mismo \gls{timestamp}, que corresponde a un instante real \textbf{anterior}.

El \textit{det\_r} también empeora al aumentar la cantidad de cámaras, debido a la alta demanda de cómputo del sistema (para 10 cámaras, se deben de procesar un total de 40 redes neuronales en 100 milisegundos) lo que hace que se pierdan los frames que no tengan espacio en la cola. Sin embargo, la caída de rendimiento en ambos dispositivos \textbf{no es crítica} y permite atender a la mayoría de las peticiones, aunque los resultados reflejan una clara congestión en los nodos.

La precisión de los reconocimientos no baja del \textbf{80\%} en todos los casos, lo que demuestra que los modelos de reconocimiento funcionan de manera eficaz ante un enorme estrés del sistema. El \textit{idf1} depende del \textit{det\_r}, por lo que es de esperar un valor bajo, compensado en cierta medida por la precisión en el reconocimiento.

\input{contenido/tables/tests/scaleresources.tex}

La tabla \ref{tab:scaleresources} muestra el uso de recursos durante la prueba para los equipos de sobremesa. Se puede apreciar que, hasta las 10 cámaras, el sistema se desempeña de forma fluida, con una media de poco más de la mitad de recursos utilizados.

Los recursos de la \acrshort{gpu} se llevan al límite con 10 cámaras. La gráfica del portátil (Ampere) frente a la del PC del laboratorio (Turing) es capaz de gestionar mejor la memoria (debido a TODO: \dots). Por otro lado, la \acrshort{gpu} Ampere es capaz de lidiar mejor con la carga, debido a TODO: el número de \glspl{sm} seguramente.

El uso elevado de recursos explica, junto a otros factores, la bajada en el \textit{det\_r} cuando se ejecutan 10 nodos cámara simultáneamente, más el nodo \acrshort{lidar} y el integrador.

\input{contenido/tables/tests/scaleros2.tex}

En la tabla \ref{tab:scaleros2} se muestra el rendimiento del sistema en \textbf{\acrshort{ros} 2} para los equipos Jetson Orin Nano y Jetson AGX Thor. Los resultados marcados con guion indican ausencia del dato cuando se realizó la prueba o imposibilidad de realizar dicha prueba (es el caso de la Jetson Orin Nano con 10 cámaras).

En estos dispositivos no es posible instalar \acrshort{ros} Noetic, al menos para poder contar con las últimas versiones de las librerías. Por los motivos comentados en la sección \ref{subsec:ROS2}, el sistema en \acrshort{ros} 2 no puede ejecutar el nodo \acrshort{lidar}, por lo que los resultados mostrados solo tienen en cuenta los nodos cámara.

El \textit{det\_r} se muestra claramente inferior respecto a los anteriores resultados, ya que el \acrshort{lidar} otorgaba cobertura completa a la reducida visión de las cámaras de aproximadamente 117º. Las 3 Jetson logran superar el valor de referencia en esta métrica, lo que demuestra el excelente rendimiento de las \acrshort{gpu}, capaces de compensar las limitaciones de una \acrshort{cpu} de \acrshort{arm} respecto a un Intel Core i5 (procesador del equipo de referencia).

La precisión en la detección empeora en los dispositivos Jetson respecto a los de sobremesa cuando se ejecutan 5 cámaras simultáneamente. Debido a la menor capacidad de procesamiento de la Jetson Orin Nano, es de esperar que muchos de los mensajes procesados por los nodos cámara llegan tarde al nodo integrador, que devuelve una posición desactualizada respecto al frame de referencia, por lo tanto, se asume la detección como \textbf{errónea}. TODO: y en la Jetson AGX Thor?, a pesar de la notable caída en la precisión, que es de la misma magnitud que en los dispositivos de sobremesa, el \textit{det\_r} se mantiene constante e incluso mejora con 5 cámaras (TODO: querrá decir que va sobrada).

\input{contenido/tables/tests/scaleresourcesros2.tex}

En la tabla \ref{tab:scaleresourcesros2} se muestra la proporción de recursos utilizados en las pruebas del sistema en \acrshort{ros} 2. TODO: Como en la tabla \ref{tab:scaleresources}, se expone la mediana de todas las lecturas de recursos en una ejecución completa del sistema. Se emplea la mediana, ya que es robusta frente a los picos de uso de recursos.

Ambos dispositivos pueden ejecutar holgadamente el sistema usando 2 cámaras, en el caso de las 5 cámaras es cuando se empiezan a dislumbrar las limitaciones de la Jetson Orin Nano, principalmente en términos de memoria, lo que explica la caída en la precisión de las detecciones, que no permite a los modelos reservar memoria para varias de las inferencias.

Los 8 GB de memoria rápidamente se quedan escasos, debido a todos los recursos necesarios a reservar, como contextos de CUDA, el modelo deserializado, \glspl{stream} de CUDA, entre otros, que pesan en torno a cientos de \acrshort{mb}, que se multiplican por 4 modelos y a su vez por 5 cámaras. Debido al excesivo uso de memoria, se han aplicado ciertas optimizaciones de los códigos de Python, que han permitido ahorrar hasta \textbf{400 \acrshort{mb}} en la inicialización del sistema, y que se citan en el apéndice \ref{chap:optimus}.

\section{Discusión}

En varios equipos, se ha logrado escalar el sistema hasta un número de cámaras más que suficiente para el caso de uso de este proyecto (10 cámaras sobran para cubrir un espacio de 360º), al menos para ser procesado por un solo dispositivo. La \textbf{Jetson Orin Nano} ha superado los resultados de \textbf{\textit{det\_r}} y el \textbf{\textit{idp}} para 2 cámaras respecto al equipo de referencia y ha conseguido mantener resultados aceptables hasta las 5 cámaras. Esto es especialmente interesante, debido a que este equipo es de muy bajo consumo y peso, lo que permite su fácil integración en un robot móvil.

El uso de la \acrshort{gpu} ha sido un factor clave en los resultados obtenidos, ya que, al ejecutar todo el procesamiento en la \acrshort{cpu}, no solo se saturan sus propios recursos, sino que la latencia de las inferencias aumenta y, por tanto, se devuelven predicciones en instantes de tiempo posteriores, cuya diferencia se va acumulando.

Es importante destacar que las pruebas se realizaron con \textbf{6 personas registradas en la base de datos}. Aumentar este número tendría consecuencias en los modelos de reconocimiento, ya que iteran toda la base de datos para calcular las distancias del vector con cada individuo, para devolver la mejor coincidencia. Según el dispositivo, esta operación tarda menos o alrededor de 1 milisegundo, lo que podría aumentar en varios milisegundos si se añaden por ejemplo 100 personas.