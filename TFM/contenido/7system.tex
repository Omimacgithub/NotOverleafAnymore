\chapter{Pruebas de escalabilidad del sistema}
\label{chap:syscal}

\lettrine{E}{n} este capítulo se realiza una evaluación del rendimiento del sistema al elevar el número de cámaras implicadas.

\section{Realización de las pruebas}

Para obtener resultados de precisión del sistema, se dispone de un \gls{dataset} que representa el \gls{gt} del nodo integración de sensores (ver sección \ref{sec:basearch}). La prueba de la que se ha extraído el \gls{dataset} corresponde con la misma del \gls{dataset} de las cámaras. En el apéndice \ref{sec:datsys} se comenta su composición.

Es importante remarcar que todas las pruebas de esta sección se realizan en \textbf{tiempo real}. Según el proyecto, la frecuencia del tiempo real puede variar, en este caso, se espera que el sistema trabaje a \textbf{10 Hz} (o 10 \acrshort{fps}), que es la frecuencia a la que ambos sensores \acrshort{lidar} y Kinect devuelven datos. Los modelos de todas las pruebas realizadas en este capítulo se han ejecutado en el \gls{rt} de \textbf{TensorRT} (salvo YuNet, que utiliza CUDA por medio de OpenCV).

Se inicializa el sistema con el nodo integración de sensores, el nodo \acrshort{lidar} y el número de nodos cámara deseado (ver sección \ref{sec:basearch}). Se reproduce el fichero \gls{rosbag} una vez inicializado el sistema, que lo nutrirá con la información generada por los sensores en \textbf{tiempo real}. Cada predicción generada por el nodo integrador se almacena en memoria y se vuelca en un fichero \acrshort{json} una vez el sistema se detiene (por ejemplo, con un Ctrl+C). Los datos guardados en el fichero \acrshort{json} se procesan contra el \textit{\gls{dataset}} y se devuelven los resultados finales en forma de las siguientes métricas:

%En cada iteración del procesado, se ejecutan \textbf{4 redes neuronales (\acrshort{cnn})}. 2 de ellas, YOLOv8n y YuNet, para la detección de cuerpos y de caras respectivamente. Las otras 2, OSNet\_x1 y ArcFace, para el reconocimiento de cuerpos y caras respectivamente. A pesar de lo que la figura \ref{fig:finalsys} muestra, las redes de detección YOLOv8n y YuNet \textbf{no se ejecutan en paralelo}, lo mismo sucede con las redes ArcFace y OSNet\_x1. Se ha considerado ejecutar en paralelo estas redes. Sin embargo, las limitaciones de memoria principal de la Jetson restringen tomar esta aproximación.

\begin{itemize}
    \item \textit{\textbf{Det precision} (det\_p)}: proporción de personas detectadas en la posición \textbf{\acrshort{3d}} correcta.
    \item \textit{\textbf{Det recall} (det\_r)}: se define igual que el \textit{recall} para los modelos de detección (ver apéndice \ref{subsec:det}). El cálculo de esta métrica \textbf{se ha modificado} respecto a \cite{andrew}, el \textit{TP + FN} del \textit{recall} se ha cambiado por el número total de detecciones del \gls{dataset}, a diferencia de \cite{andrew}, que considera el número total de detecciones del \gls{dataset} dentro de los \textbf{frames que coinciden} en \gls{timestamp} con la salida del nodo integrador, de modo que las caídas en el número de frames coincidentes (principalmente debido a la congestión del sistema) \textbf{no afectan} al valor del \textit{recall}, lo que no es deseable para analizar la escalabilidad del sistema. Este cambio en el cálculo \textbf{varía ligeramente} los resultados obtenidos del \textit{recall} respecto a \cite{andrew}.
          %siendo \textit{det\_tp} una detección positiva y \textit{n\_gt} el número de detecciones registradas en el \gls{dataset}, el \textit{det\_r} se calcula como sigue en la ecuaci
    \item \textit{\textbf{Ident F1 score} (idf1)}: se corresponde con el \textit{F1\_score} global de los modelos de reconocimiento (ver apéndice \ref{subsec:recon}).
    \item \textit{\textbf{Ident precision} (idp)}: es la precisión global de los modelos de reconocimiento.
\end{itemize}

La prueba del sistema da una detección como \textbf{positiva} si la posición \acrshort{3d} predicha es igual a la del \gls{dataset} dentro de un umbral de distancia máximo de \textbf{40 centímetros}. Adicionalmente, el retardo máximo para que un resultado del nodo cámara sea procesado por el nodo integrador es de \textbf{200 milisegundos}, como se expone en la sección \ref{sec:scal} (se han establecido estos valores, ya que son los mismos que los utilizados en \cite{andrew} para las pruebas del sistema).

Para cada prueba, también se mostrará la proporción de los recursos utilizados del dispositivo durante la ejecución del sistema (las pruebas se han realizado \textbf{sin el entorno gráfico}). Se empleará la mediana de todas las lecturas de recursos en una ejecución completa del sistema, ya que es robusta frente a los picos de uso de recursos. En dispositivos Jetson, el comando \textbf{tegrastats} devuelve toda la información necesaria, en cambio, para el resto de dispositivos se obtienen de la siguiente forma:
\begin{itemize}
    \item Carga de la \acrshort{cpu}: media de las mediciones devueltas por el comando \textbf{sar -u}.
    \item Ocupación de la memoria de la \acrshort{cpu}: media de las mediciones devueltas por el comando \textbf{sar -r}.
    \item Carga de la \acrshort{gpu}: media de las mediciones devueltas por el comando \textbf{nvidia-smi}.
    \item Ocupación de la memoria de la \acrshort{gpu}: media de las mediciones devueltas por el comando \textbf{nvidia-smi}.
    \item Ancho de banda de la transferencia de datos por la red: media de las mediciones devueltas por el comando \textbf{ifstat}.
\end{itemize}

\section{Entorno de las pruebas}

Debido a que el \gls{rosbag} utilizado es un archivo pesado (41 \acrshort{gb}), en ciertas situaciones ha sido necesario reproducir dicho fichero desde una \textbf{fuente externa} al no disponer de almacenamiento local suficiente.

\begin{equation}
    (|imagen\_RGB| + |imagen\_profundidad|)*ncams + |nube\_puntos\_LiDAR|
\end{equation}
\label{eq:transmision}

La comunicación con la fuente externa se realiza a través de una red \acrshort{lan} \textbf{Gigabit Ethernet}, que posee un ancho de banda limitado para la transmisión de imágenes \acrshort{rgbd} (máximo teórico de 125 \acrshort{mb}/s). La fórmula \ref{eq:transmision} muestra el tamaño de los datos que debe transmitir la fuente externa cada 100 ms (que es la frecuencia a la que trabaja el sistema). Siendo $|imagen\_RGB| = 40 \acrshort{mb}/s$ (resolución de 1280x1024), $|imagen\_profundidad| = 20 \acrshort{mb}/s$ (resolución de 640x480) y $|nube\_puntos\_LiDAR| = 5 \acrshort{mb}/s$ (los datos se han tomado del comando \textbf{rostopic bw}), el ancho de banda necesario es de \textbf{125 \acrshort{mb}/s} para dos cámaras (\textit{ncams}=2), lo que ya \textbf{equivale} al ancho de banda máximo teórico.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imagenes/SYSTESTING.jpg}
    \caption{Reproducción y transmisión del \gls{rosbag} por la red}
    \label{fig:systesting}
\end{figure}

La figura \ref{fig:systesting} ilustra el proceso de transmisión de los datos por la red \acrshort{lan}. Para reducir el ancho de banda necesario para la transmisión, se ha optado por desplegar nodos encargados de comprimir la imagen \acrshort{rgb} desde el equipo en el que se reproduce el \gls{rosbag} (\textit{remote side} en la figura \ref{fig:systesting}). La compresión se realiza por medio del paquete \textit{image\_transport} de \acrshort{ros}, que logra reducir hasta 10 veces el tamaño de la imagen \acrshort{rgb} (resultando en 4 \acrshort{mb}/s). Los nodos cámara (\textit{host side} en la figura \ref{fig:systesting}) se suscriben al tópico generado por el nodo que descomprime la imagen (\textit{image\_transport} \textit{decompress} en el \textit{host side}).

En el caso de la imagen de distancias, en sí no es un objeto pesado, por lo que no necesita compresión. Sin embargo, la frecuencia elevada de transmisión (30 Hz), hace que se requiera de un mayor ancho de banda. Como el resto de los sensores funcionan a 10 Hz, la tasa de la imagen de distancias puede reducirse a dicha frecuencia, de forma que el sistema no percibe el cambio y se logra reducir el ancho de banda. Con reducir la tasa de envío del nodo de \acrshort{ros} encargado de publicar la imagen sería suficiente, pero debido a que en el \gls{rosbag} dicho nodo se grabó a 30 Hz, se ha optado por ejecutar el paquete \textit{topic\_tools} de \acrshort{ros}, que permite publicar una réplica de un tópico a una menor frecuencia.

Debido a que el \gls{rosbag} solo contiene los datos de 2 cámaras, para realizar la prueba de escalabilidad con un mayor número de cámaras, ha sido necesario \textbf{replicar} los tópicos de las imágenes mediante el paquete de \acrshort{ros} \textit{topic\_tools} (figura \ref{fig:systesting}), de forma que los nuevos nodos se pueden suscribir a dichos tópicos.

\section{Resultados}
\label{sec:scaleresults}

A continuación se exponen los resultados de las pruebas de escalabilidad ejecutadas en la familia Intel (equipos de sobremesa) y en 2 dispositivos de la familia Jetson (Jetson Orin Nano y Jetson AGX Thor). Los resultados marcados con guion indican ausencia del dato por imposibilidad de realizar la prueba.

\subsection{Familia Intel}

\input{contenido/tables/tests/scaleperf.tex}

La tabla \ref{tab:scaleperf} muestra los resultados de la familia Intel en función del número de cámaras del sistema en \textbf{\acrshort{ros} 1}. La columna \textit{Reference} muestra los resultados obtenidos en el dispositivo de pruebas utilizado en \cite{andrew}, en el que solo se registraron datos del sistema funcionando con un máximo de 2 cámaras.

% TODO: En el caso de las 2 cámaras, no se ve una mejora respecto a los resultados de referencia, de hecho se experimenta una ligera caída en los resultados...

\input{contenido/tables/tests/scaleresources.tex}

En general, los resultados entre el PC del laboratorio y el portátil son \textbf{muy similares}, esto es especialmente relevante debido al esfuerzo adicional de compresión/descompresión y transmisión de mensajes por la red en el caso del portátil, lo que demuestra la efectividad de las herramientas de red de \acrshort{ros} y de la arquitectura propuesta (figura \ref{fig:systesting}), que han logrado reducir el ancho de banda necesario para la transmisión de mensajes (en la tabla \ref{tab:scaleresources} se muestra un 61\% de utilización respecto al máximo teórico de una red GigabitEthernet (125 \acrshort{mb}/s) en el procesamiento de 10 cámaras). La latencia asociada a la compresión en el origen, transmisión del mensaje y descompresión de un frame \acrshort{rgb} en el destino es de aproximadamente \textbf{50 milisegundos} (obtenido calculando la diferencia entre el tiempo actual y el \gls{timestamp} de generación del mensaje inicial) que, según los resultados, el portátil es capaz de compensar incluso con 10 cámaras.

El portátil escala correctamente hasta las 10 cámaras, a diferencia del PC del laboratorio, que experimenta una caída en el \textit{det\_r}. Dicha caída puede deberse a una carga excesiva en la \acrshort{gpu} del PC del laboratorio, mientras que la \acrshort{gpu} del portátil la consigue tolerar en mayor medida (en la tabla \ref{tab:scaleresources} se muestra una utilización del 84\% respecto al 72\% de la \acrshort{gpu} del portátil), principalmente por un mayor número de \glspl{sm} en la \acrshort{gpu} Ampere (20) respecto a la Turing (14) y del \textbf{casi triple} de núcleos de CUDA (2048 y 896 respectivamente).

% ESTO NO ES CIERTO, al reducir el intervalo a 200 ms baja el recall (a 47, que es MENOS que la referencia) pero no sube la precisión, esto puede deberse a que se acepta la llegada de los mensajes de las cámaras con un máximo de \textbf{500 milisegundos} de retardo respecto a los \textbf{200 milisegundos} configurados en la prueba de referencia, lo que hace que se devuelvan predicciones con posiciones más atrasadas y, por lo tanto, que superen la holgura de 40 centímetros para asumirlas correctas. En cuanto a la métrica \textit{idp} todos los sistemas se encuentran relativamente a la par.

%a que el nodo integrador debe de fusionar los datos procedentes de todos los sensores, al aumentar su número también se aumenta la cantidad de procesamiento. Por lo tanto, más tiempo se demorará en devolver una predicción, que se emparejará con el frame del \gls{dataset} con el mismo \gls{timestamp}, que corresponde a un instante real \textbf{anterior}.

%PRECISION DE FAMILIA INTEL: la caida en la precision no se debe al slope ni al tamaño de la cola. El problema debe de estar en el nodo LiDAR.

%PRECISIÓN AL AUMENTAR EL Nº DE CÁMARAS: creo q el nodo integrador incluye detecciones repetidas (con las mismas posiciones exactas) cuando no se conoce la identidad (la etiqueta como unk), ya que con entidades desconocidas no se guardan duplicados. Esto produce una proliferación de det_fps, ya que al haber puntos repetidos, hay más detecciones pero menos matches, por lo tanto aumentan los unmatches y así los det_fps.

% OLD: La precisión en los reconocimientos (métrica \textit{idp}) no experimenta caídas notorias (excepto por la situación de las 5 cámaras en el portátil, que sorprendente se recupera al subir a las 10 cámaras) al aumentar la cantidad de cámaras, lo que demuestra que los modelos de reconocimiento siguen funcionando correctamente ante un enorme estrés del sistema. El \textit{idf1} depende en cierta medida del \textit{det\_r}, por lo que es de esperar que dicho valor descienda junto al \textit{det\_r}, que en cierto modo es compensado por el \textit{idp}.

\subsection{Familia Jetson}

\input{contenido/tables/tests/scaleros2.tex}

En la tabla \ref{tab:scaleros2} se muestra el rendimiento del sistema en \textbf{\acrshort{ros} 2} para los equipos Jetson Orin Nano y Jetson AGX Thor. En estos dispositivos no es posible instalar \acrshort{ros} Noetic, al menos para poder contar con las últimas versiones de las librerías. Por los motivos comentados en la sección \ref{subsec:ROS2}, el sistema en \acrshort{ros} 2 no puede ejecutar el nodo \acrshort{lidar}, por lo que los resultados mostrados solo tienen en cuenta los nodos cámara junto al integrador.

El \textit{det\_r} se muestra claramente inferior respecto a los resultados de la familia Intel, ya que el \acrshort{lidar} otorgaba cobertura completa a la reducida visión de las cámaras. La Jetson AGX Thor logra superar el valor del equipo de referencia en esta métrica, mientras que la Jetson Orin Nano se queda a 3 puntos de superarlo. Los resultados de precisión (\textit{det\_p} e \textit{idp}) se mantienen entre todos los dispositivos. Estos datos demuestran el excelente rendimiento de las \acrshort{gpu}s de las Jetson, capaces de compensar las limitaciones de una \acrshort{cpu} de \acrshort{arm} respecto a un Intel Core i5 (procesador del equipo de referencia). También es importante recordar que el cálculo del \textit{det\_r} se ha modificado respecto a \cite{andrew}, por lo que se muestra un \textit{recall} ligeramente inflado en la referencia.

\input{contenido/tables/tests/scaleresourcesros2.tex}

Se experimenta una ligera caída en el \textit{det\_p} al aumentar a 5 cámaras en la Jetson AGX Thor, ya que se generan más resultados, por lo que aumenta la probabilidad de fusionar información de distintos instantes temporales en el nodo integrador. El \textit{det\_r} se mantiene constante, lo que significa que el dispositivo tolera sin problema la carga introducida (el bajo uso de recursos en la tabla \ref{tab:scaleresourcesros2} lo avala). Sin embargo, en la Jetson Orin Nano el \textit{det\_r} cae drásticamente debido a la saturación de los recursos (acorde a la tabla \ref{tab:scaleresourcesros2}), tanto de memoria, lo que impide la realización de las inferencias, como de procesamiento, lo que implica en un retardo de las respuestas y, por consiguiente, a que se consideren resultados desactualizados que conducen a una pérdida en la precisión.

% RECALL: En la Thor con 2 y 5 cámaras el n_gt se mantiene encima de los 6000 mientras q con 10 cams baja a 4300. Creo q el recall debe ser: det_fp/6259 y 6259 es el número de detecciones totales del ground truth, sin recortar nada.

La Jetson AGX Thor sufre una caída en el \textit{det\_p}, pero sobre todo en el \textit{det\_r} al procesar 10 cámaras. Esto se debe a la elevada utilización de la \acrshort{gpu} (como se muestra en la tabla \ref{tab:scaleresourcesros2}), que provoca una reducción en el número de detecciones totales, además de su precisión. Este hecho lo demuestra la ejecución del modelo YuNet en la \acrshort{cpu}, que ha logrado aumentar el \textit{det\_p} en 4 puntos respecto a ejecutarlo en \acrshort{gpu}. Se ha escogido YuNet, debido a que la reducción en la latencia mediante CUDA no es tan efectiva como en TensorRT y a que la gestión de la memoria realizada por OpenCV es \textbf{mucho menos eficiente} (en la tabla \ref{tab:scaleresourcesros2} se puede ver como el uso de memoria con 10 cámaras y YuNet en \acrshort{cpu} es \textbf{menor} que con 5 cámaras y YuNet en \acrshort{gpu}).

\section{Discusiones}

En los dispositivos de sobremesa y en la Jetson AGX Thor, se ha logrado escalar el sistema hasta un número de cámaras más que suficiente para el caso de uso de este proyecto (para cubrir un espacio de 360º no es necesario disponer de 10 cámaras), al menos para ser procesado por un solo dispositivo. En la prueba se llegan a detectar un máximo de \textbf{5 personas simultáneamente} entre las 2 cámaras, por lo que se podría asumir que en dichos dispositivos el sistema empieza a resentirse al detectar y reconocer a \textbf{26 personas de forma simultánea} (si se asume 3*6 + 2*4, siendo 6 el número de cámaras que detectan a 3 personas y 4 el número de cámaras que detectan a 2 personas). Por otro lado, la \textbf{Jetson Orin Nano} ha logrado resultados decentes para 2 cámaras, llegando a sufrir una caída crítica al operar con 5 cámaras. Este dispositivo en particular sería especialmente útil debido a su muy bajo consumo y peso, lo que permite su fácil integración en un robot móvil.

Los 8 \acrshort{gb}s de memoria de la Jetson Orin Nano se vuelven \textbf{insuficientes}, debido a todos los recursos necesarios a reservar, como los contextos de CUDA, que pesan en torno a cientos de \acrshort{mb}s, que se multiplican por 4 modelos y a su vez por 5 cámaras. Se ha intentado mitigar la caída en el rendimiento por medio de ciertas optimizaciones de uso de memoria en los códigos de Python, que han permitido ahorrar hasta \textbf{400 \acrshort{mb}s} en la inicialización del sistema, y que se citan en el apéndice \ref{chap:optimus}. Aun así, es necesario reducir la elevada demanda de procesamiento que se hace inabarcable para la Jetson Orin Nano y por ende impide al \textit{det\_r} despegar.

Es importante destacar que las pruebas se realizaron con \textbf{6 personas registradas en la base de datos}, cada una con 4 vectores descriptores (2 correspondientes a la cara y las otras 2 al cuerpo). Aumentar este número tendría consecuencias en los modelos de reconocimiento, ya que iteran toda la base de datos para calcular las distancias del vector con cada individuo, para devolver la mejor coincidencia. Según el dispositivo, esta operación tarda menos o alrededor de 1 milisegundo, lo que podría aumentar en varios milisegundos si se añaden por ejemplo 100 personas.

Por último, el uso de la \acrshort{gpu} ha permitido elevar al sistema más allá de lo que la \acrshort{cpu} puede afronta, no solo por la distribución de los recursos, sino por el uso de TensorRT, que minimiza el uso de la memoria y de la carga computacional, a diferencia de OpenCV integrado con CUDA, cuya gestión de recursos hace que, en el caso de la Jetson AGX Thor, sea más viable la ejecución en \acrshort{cpu}. Sin embargo, se han deslumbrado los límites computacionales de dicha aceleradora, por lo que se vuelve necesario aplicar técnicas, tanto software como hardware, para maximizar su utilización. La \textbf{\gls{quant} de los modelos a INT8} y/o el uso del hardware \textbf{Deep Learning Accelerator (DLA)} \cite{DLA} integrado en las Jetson son pasos que podrían contribuir a este proceso.