\chapter{Integración y extensión del sistema base}
\label{chap:sys}

\lettrine{E}{n} este capítulo se exponen los resultados de rendimiento del sistema propuesto para varias arquitecturas. Se ha partido de una implementación final del sistema pensada para la arquitectura x86, este capítulo explica las optimizaciones aplicadas para su ejecución en arquitecturas ARM64.

\section{Arquitectura del sistema}
\label{sec:sysarch}

El sistema de este proyecto posee la capacidad de fusionar diferentes fuentes de datos para otorgar un único resultado de reconocimiento para cada persona detectada. Los nodos que componen la arquitectura se detallan a continuación y se muestran en la figura \ref{fig:ANDRES}:

\begin{itemize}
    \item \textbf{Nodo cámara}: recibe como fuente los datos de una cámara \textbf{\acrshort{rgbd}} y se encarga de detectar, reconocer y obtener la posición \gls{3d} de las personas que aparecen en la imagen.
    \item \textbf{Nodo LiDAR}: recibe los datos de un sensor LiDAR (nube de puntos \gls{3d}) y se encarga de detectar y obtener la posición \gls{3d} de las personas del mapa (no se ha implementado el reconocimiento).
    \item \textbf{Nodo integrador de sensores}: fusiona los datos de los 2 nodos anteriores. La función de este nodo es el seguimiento de las personas reconocidas, más allá de lo que la cobertura de las cámaras ofrece (para un Kinect de la Xbox 360 son 57º).
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{imagenes/SYSARCH.png}
    \caption{Arquitectura general del sistema (figura extraída de \cite{andrew})}
    \label{fig:ANDRES}
\end{figure}

\section{Modificaciones propuestas}

\subsection{Ventanas de frames}
\label{subseq:secuenciation}

El sistema expuesto trabaja a nivel de frame, lo que otorga sencillez y una elevada frecuencia, pero al mismo tiempo un alto coste computacional y una no muy alta confianza (el reconocimiento depende enteramente del frame capturado). En este proyecto se ha optado por trabajar con \textbf{secuencias de frames}, de manera que se exprimen las capacidades de la GPU para trabajar por lotes (conjuntos de frames) y se otorgan mejores reconocimientos (basados en múltiples frames). El funcionamiento de esta técnica se muestra en la figura (TODO), se escoge un tamaño de secuencia (15 o 25 en las pruebas realizadas) y se extraen los frames. En cada frame, puede haber entre 0 y n entidades, como no se conoce la identidad de cada individuo en este punto, es necesario realizar un \textbf{rastreo} (tracking) de las entidades en cada uno de los frames. El tracking implementado calcula una matriz de costes entre las detecciones de un frame y el frame posterior, cada coste representa un valor de solape entre \glspl{bbox}, cuanto menor mayor es el solape, finalmente, se realiza una asignación óptima entre detecciones mediante el \textbf{método húngaro} (como se plantea en \cite{Hungarian}).

\subsubsection{Reidentificación de entidades}
Es posible que durante la detección de individuos, las \glspl{bbox} de una misma persona en frames consecutivos no se solapen, debido a la velocidad de movimiento del individuo por ejemplo. En este caso, existe el riesgo de que el método húngaro asocie incorrectamente la identificación a otra persona a la que le ocurre esta misma situación. Otro problema es el no seguimiento de la persona cuando esta se encuentra totalmente ocluida (ejemplo: se cruza un individuo justo delante) y vuelve a aparecer.

A partir del método de tracking propuesto no es posible recuperar el rastro de las entidades que desaparecieron de forma intermitente. Para resolver este problema se ha optado por calcular la distancia euclidiana entre la última \gls{bbox} del individuo en cuestión y la \gls{bbox} de una nueva detección, si la distancia entre ellas es menor a un umbral, se reasigna la detección al individuo.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imagenes/Seq.jpg}
    \caption{Funcionamiento de la secuenciación, como salida se obtiene una lista de frames agrupados por entidad}
    \label{fig:Seq}
\end{figure}

\subsection{Escalabilidad y tolerancia a fallos de las cámaras}
TODO: con el sistema inicial, solo se podían ejecutar 2 cámaras y tenían que funcionar las 2 a la vez. Por estas razones, se ha reemplazado la solución del \textit{ApproximateTimeSynchronizer} implementada en \cite{andrew} por la clase \textit{MessageFiltersCache}.

\subsection{Migración a ROS 2}
\label{subsec:ROS2}

Con el motivo del fin de soporte de ROS 1 \cite{ROSEOL}, se ha optado por migrar el sistema para ser ejecutado en ROS 2 con el fin de mantener su continuidad. Se han migrado los nodos cámara e integrador, la migración del nodo LiDAR requeriría actualizar el código a una versión soportada para Ubuntu 22.04 de la librería pcl, entre otros detalles que llevarían a rediseñar casi todo el código. TODO: Se probaron diferentes alternativas como RoboStack

\section{Optimización de inferencias con TensorRT}

TODO: Se han recortado bucles for y se ha exprimido la librería de numpy, se han reducido unos milisegundos.

En el campo de TinyML ya se han analizado varios modelos \cite{MITTAL2019428, rahmaniar2021real} y propuesto múltiples optimizaciones \cite{MITTAL2019428} para sistemas NVIDIA Jetson. Para lograr la ejecución eficiente de modelos en los dispositivos Jetson de este proyecto, se ha optado por utilizar el software \textbf{TensorRT}. Dicho software se aplica para la conversión de modelos en formato ONNX (previamente transformados de frameworks como TensorFlow y PyTorch) al formato optimizado de TensorRT y para ejecutar inferencias.

TODO: El procedimiento sigue el tutorial proporcionado por NVIDIA \cite{tutos}, en esta sección se tratará por partes de forma detallada.

\subsection{Conversión a ONNX}

Los modelos del proyecto se encuentran en diferentes formatos, existen integraciones para utilizar los modelos directamente de TensorFlow y de PyTorch. Sin embargo, se ha optado por usar exclusivamente el formato ONNX. Su principal desventaja frente a otros formatos son la incompatibilidad de ciertas capas de redes neuronales, se ha tenido. En cambio, es la solución que otorga el mayor rendimiento. Según el formato se han aplicado las siguientes herramientas en la exportación:

\begin{itemize}
    \item \textbf{PyTorch}: la API de Python ya proporciona una herramienta (torch.onnx.export).
    \item \textbf{TensorFlow}: se usa la librería de Python \textbf{tf2onnx}.
\end{itemize}

\subsection{Conversión a TensorRT engine}
TODO: El formato (.trt o .engine)

TODO: El rendimiento esperado en tiempo real para el sistema son los \textbf{10 Hz} (o 10 FPS), por lo tanto, todos los nodos deben de funcionar a 10 Hz. Las NVIDIA Jetson solo tienen que ocuparse de ejecutar los \textbf{nodos cámara a 10 Hz} y que otro equipo ejecute el nodo LiDAR e integrador del sistema. Que los nodos cámara trabajen a 10 Hz implica que a cada \textbf{100 ms se recibe un nuevo frame} (10 * 100 = 1000 ms = 1 segundo), la figura \ref{fig:cam} muestra el procesado realizado por dicho nodo en cada frame, que debe de finalizar \textbf{antes de que transcurran 100 ms}. En cada iteración del procesado, se ejecutan \textbf{4 redes neuronales (CNN)}. 2 de ellas, YOLOv8n y YuNet, para la detección de cuerpos y de caras respectivamente. Las otras 2, OSNet\_x1 y ArcFace, para el reconocimiento de cuerpos y caras respectivamente. A pesar de lo que la figura \ref{fig:cam} muestra, las redes de detección YOLOv8n y YuNet \textbf{no se ejecutan en paralelo}, lo mismo sucede con las redes ArcFace y OSNet\_x1. Se ha considerado ejecutar en paralelo estas redes. Sin embargo, las limitaciones de memoria principal de la Jetson restringen tomar esta aproximación.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{imagenes/CAM.png}
    \caption{Componentes del nodo cámara}
    \label{fig:cam}
\end{figure}