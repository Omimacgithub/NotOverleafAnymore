\chapter{Fundamentos tecnológicos}
\lettrine{E}{n} este capítulo se detallan las métricas de evaluación y herramientas hardware y software utilizadas, así como los conceptos necesarios para comprender el resto de la memoria.

\section{Hardware}
\subsection{Summit\_XL}
El sistema que se expondrá se ha construido para funcionar en el robot móvil Summit\_XL. Dicho robot es de tipo diferencial (permite el movimiento en todas las direcciones excepto en los laterales) y cuenta con 4 ruedas de goma, de gran utilidad para moverse en entornos de exteriores. Entre sus componentes, tiene instalado un sensor Velodyne \acrshort{lidar} que otorga nubes de puntos \acrshort{3d} del entorno y dos cámaras Kinect instaladas en los laterales.

\subsection{Cámaras RGBD}
Las Kinect de Microsoft otorgan una imagen \acrfull{rgb} más una imagen de distancia.

\subsection{Dispositivos}

A continuación se expone el listado de dispositivos sujetos de las pruebas de este proyecto.

\subsubsection{NVIDIA Jetson}
Es una familia de \textbf{\glspl{embsystem}} pensados para aplicaciones de \acrshort{ia}. Dichos dispositivos incluyen un kit de desarrollo llamado \textbf{JetPack}, diseñado para exprimir la potencia de la NVIDIA Jetson en aplicaciones como la robótica, \acrshort{ia} generativa y visión artificial \cite{jason}.

Actualmente existe una amplia variedad en la potencia y precio de estos dispositivos \cite{jason}. Para este proyecto se han empleado cuatro modelos diferentes que se exponen a continuación (se encuentran ordenados de menores a mayores prestaciones).

\textbf{Jetson Xavier NX}

Specs \cite{jxavier}

\acrshort{cpu}: 6-core NVIDIA Carmel Armv8.2 64-bit CPU 1.9 GHz

\acrshort{gpu}: 384-core NVIDIA Volta™ architecture GPU with 48 Tensor Cores 1.1 GHz

Memoria: 8GB 128-bit LPDDR4x 59.7GB/s

Red: Gigabit Ethernet

Consumo: entre 10 W y 20 W

AI Performance: 21 TOPS (TODO: en qué precisión? INT8?)

Desde 399\$ \cite{price}.


\textbf{Jetson Orin Nano}

Specs \cite{jorin}

\acrshort{cpu}: 6-core Arm® Cortex®-A78AE v8.2 64-bit CPU 1.5MB L2 + 4MB L3 1.7 GHz

\acrshort{gpu}: 1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores 1020 MHz

Memoria: 8 GB LPDDR5 128 bits 102 GB/s

Red: Gigabit Ethernet

Consumo: 7W - 15W - 25W

AI Performance: 67 TOPS INT8

Es la más barata desde 249\$ \cite{price}.

\textbf{Jetson AGX Orin}

Specs \cite{jorin}:

\acrshort{cpu}: 12-core Arm Cortex-A78AE v8.2 64-bit 3MB L2 + 6MB L3 CPU 2.2 GHz

\acrshort{gpu}: 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores 1.3 GHz

Memoria: 64GB 256-bit LPDDR5 204.8GB/s

Red: 10 Gigabit Ethernet

AI Performance: 275 TOPS

Consumo: entre 15 W y 60 W

La placa AGX Orin aumenta considerablemente las prestaciones de memoria principal y núcleos de CPU respecto la familia NX o Nano. En las placas AGX podría ser viable entrenar modelos \acrshort{cnn}.

1999\$ \cite{price}.

\textbf{Jetson AGX Thor}

Specs \cite{jthor}:

\acrshort{cpu}: 14-core Arm Neoverse-V3AE 64-bit CPU 1 MB L2 cache per core 16 MB shared system L3 cache 2.6 GHz

\acrshort{gpu}: 2560-core NVIDIA Blackwell architecture GPU with 96 fifth-gen Tensor Cores Multi-Instance GPU (MIG) with 10 TPCs 1.57 GHz

Memoria: 128 GB 256-bit LPDDR5X 273 GB/s

Red: 1 puerto RJ45 de 5 Gigabit Ethernet y 1 puerto QSFP28 de hasta 4 canales x 25 Gigabit Ethernet (TODO: lo mismo que 100 Gigabit Etherneta? <- mirar con lshw -class network)

AI Performance: 2070 TFLOPS (FP4—Sparse)

Consumo: entre 40 W y 130 W

Desde 3499\$ \cite{price}


\subsubsection{Portátil personal}
Cuenta con una CPU \textbf{Intel i7 i7-12650H}, una GPU \textbf{NVIDIA GeForce RTX 3050} con \textbf{4 GB de RAM}. Tiene una memoria principal de \textbf{16 GB} y el sistema operativo Linux Mint 21.3 (equivalente a un Ubuntu 22.04).

CPU: i7-12650H

Nº threads: 16

Freq: 4.7 GHz

GPU: GeForce RTX 3050 (4 GiB)

SMs: 18

SM Freq: 2.1 GHz

MEM Freq: 7 GHz

MEM: 15 GB


\subsubsection{PC del laboratorio del CITIC}
Cuenta con una CPU \textbf{Intel i7 i7-12700}, una GPU \textbf{NVIDIA GeForce GTX 1650} con \textbf{4 GB de RAM}. Tiene una memoria principal de \textbf{32 GB} y el sistema operativo Ubuntu 24.04.

CPU: i7-12700

Nº threads: 20

Freq: 4.9 GHz

GPU: GeForce GTX 1650 (4 GiB)

SMs: 14

SM Freq: 2.1 GHz

MEM Freq: 6 GHz

MEM: 33.4 GB

\section{Software}
\label{sec:sw}

\subsection{ROS}
\acrfull{ros} es un middleware de \textbf{código abierto} que incorpora las herramientas necesarias para la interacción y desarrollo de software en robots. Cuenta con una amplia biblioteca de distribuciones y librerías. TODO

Existe una herramienta del ecosistema \acrshort{ros} que graba y reproduce los datos capturados de la ejecución de un sistema \acrshort{ros} en un momento concreto en el tiempo. Los \glspl{dataset} creados en \cite{andrew} se construyeron partiendo de un fichero rosbag, que contiene toda la información generada por los diferentes sensores. En este proyecto se emplearán dichos ficheros rosbag para reproducir las pruebas en tiempo real.

\subsection{Optimizadores}

\textbf{TensorRT}
Es un software de \textbf{código abierto} desarrollado por NVIDIA para la optimización de inferencias de modelos de IA en aceleradoras de NVIDIA \cite{x36}. Permite ejecutar modelos entrenados en frameworks como Pytorch y TensorFlow, aunque se aconseja exportarlos al formato \acrfull{onnx} previamente, para aprovechar al completo las funcionalidades de este software. Se ha empleado TensorRT para optimizar la mayoría de modelos CNN tanto en equipos ARM (Jetson) como x86 (Intel).

\textbf{OpenVINO}
Es un software de \textbf{código abierto} desarrollado por Intel para la optimización de inferencias en \acrshort{cpu} (ARM,x86) y en aceleradoras de Intel (\acrshort{gpu},\acrshort{npu}) \cite{OpenVINO}. Permite la conversión directa de modelos entrenados en frameworks como Tensorflow y Pytorch a ficheros XML, que son versiones optimizadas de las \acrshort{cnn} y que se utilizan para realizar las inferencias. Se ha empleado para reducir la latencia de modelos a la hora de ejecutarlos en \acrshort{cpu}.
\subsection{Librerías de Python}
\textbf{PyCUDA}
Librería de Python de \textbf{código abierto} \cite{pycuda} que interacciona con el driver de CUDA por medio de un \gls{wrapper}. Se ha utilizado para programar los códigos de inferencia de los modelos que utilizan TensorRT.
\textbf{NumPy}
Librería de Python de \textbf{código abierto} usada en este proyecto para representar imágenes y la información generada por los modelos en forma de vectores y matrices multidimensionales. Estos elementos pueden manipularse por medio de una amplia cantidad de funciones matemáticas que dicha librería ofrece.

\textbf{Scipy}
Librería de Python de \textbf{código abierto} que implementa numerosos algoritmos relacionados con la computación científica. En este proyecto se ha usado para aplicar el método húngaro, hallar los parámetros que modelan una distribución de Weibull, calcular distancias coseno, entre muchas otras funcionalidades que dicho software ofrece.

\textbf{OpenCV}
La librería de \textbf{código abierto} por excelencia para visión por computadora de código abierto. Otorga herramientas para la obtención y procesado de las imágenes, además de un módulo de visión artificial (dnn), que incluye modelos ya entrenados de visión artificial. Dicha librería tiene implementación en CUDA (si se compila el código fuente), lo que permite ejecutar operaciones y kernels de modelos en aceleradoras de NVIDIA (ejemplo: NVIDIA Jetson).

\subsection{Docker}
Es un software de virtualización de \textbf{código abierto}. Permite realizar despliegues automatizados mediante ficheros de configuración, el software se ejecuta en un entorno aislado del sistema operativo llamado \textbf{contenedor}, dichos contenedores proporcionan seguridad y portabilidad. Esta herramienta ha sido de gran utilidad para desplegar el software del proyecto en los diferentes dispositivos.

\section{Otras herramientas}
\subsection{VS Code}
\subsection{Git}
\subsection{Trello}
\subsection{Draw.io}
