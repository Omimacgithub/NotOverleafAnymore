\chapter{Fundamentos tecnológicos}
\lettrine{E}{n} este capítulo se detallan las métricas de evaluación y herramientas hardware y software utilizadas, así como los conceptos necesarios para comprender el resto de la memoria.

\section{Hardware}
\label{sec:hw}

\subsection{Cámaras RGBD}
Para nutrir al sistema de imágenes se cuenta con las cámaras Kinect de Microsoft. Dichas cámaras otorgan una imagen \acrfull{rgb} más una imagen de profundidad (depth).

Para este proyecto, se cuenta con dos de estas cámaras, cada una con una cobertura de visión de 57º, lo que otorga un rango total de 117º, asumiendo que no existen solapes entre las cámaras.

\subsection{Summit\_XL}
\label{sec:summit}

El sistema que se expondrá se ha construido para funcionar en el robot móvil Summit\_XL. Dicho robot es de tipo diferencial (permite el movimiento en todas las direcciones excepto en los laterales) y cuenta con 4 ruedas de goma, de gran utilidad para moverse en entornos de exteriores. Entre sus componentes, tiene instalado un sensor Velodyne \acrshort{lidar} que otorga nubes de puntos \acrshort{3d} del entorno y dos cámaras Kinect instaladas en los laterales.

\subsection{Dispositivos de altas prestaciones}

\subsubsection{Arquitecturas de GPU}

Según la arquitectura de la \acrshort{gpu} de NVIDIA a tratar se agregan nuevas características que maximizan el rendimiento de la inferencia de los modelos (se ahondará en este tema en el capítulo \ref{chap:cnn}). Las arquitecturas que se muestran a continuación están ordenadas de menor a mayor generación:
\begin{itemize}
    \item Volta: se introducen los \textbf{tensor cores}, que son cores destinados para acelerar multiplicaciones y sumas de matrices (conocido también como MMA). La \textbf{Jetson Xavier NX} sigue esta arquitectura.
    \item Turing: . La \textbf{GeForce GTX 1650} (gráfica del PC lab) sigue esta arquitectura.
    \item Ampere: incluye las siguientes mejoras:
          \begin{itemize}
              \item Incorpora los tensor cores de tercera generación, que incluyen el tipo de dato \textbf{TF32}, que funciona como FP32 pero optimizado en \acrshort{gpu} sin realizar ningún cambio en el código \cite{ampere}
              \item Introduce la funcionalidad \acrfull{mig}, que permite la partición \textbf{física} de \acrshort{gpu}s para correr aplicaciones de CUDA. A la hora de asignar particiones, es necesario comprender los siguientes conceptos \cite{migconcepts}:
                    \begin{itemize}
                        \item \textbf{\acrshort{gpu} Engine}: es la parte de la \acrshort{gpu} que ejecuta el trabajo. Un ejemplo es el copy engine, que se encarga de realizar las operaciones relacionadas con la memoria (direct memory access, memcpy...).
                        \item \textbf{\acrshort{gpu} Memory Slice}: porción de los controladores de memoria y caché de la \acrshort{gpu}.
                        \item \textbf{\acrshort{gpu} SM Slice}: porción de los \glspl{sm} de la \acrshort{gpu}.
                        \item \textbf{\acrshort{gpu} Slice}: porción de la \acrshort{gpu} que combina un único \textbf{\acrshort{gpu} Memory Slice} y un único \textbf{\acrshort{gpu} SM Slice}.
                        \item \textbf{\acrshort{gpu} Instance}: conjunto de \textbf{\acrshort{gpu} slices} y \textbf{\acrshort{gpu} engines}.
                        \item \textbf{Compute Instance}: una o múltiples instancias que pertenecen a una \textbf{\acrshort{gpu} Instance}. Entre ellas se comparten las \textbf{\acrshort{gpu} Memory Slices} y los \textbf{\acrshort{gpu} Engines}, mientras que los \textbf{\acrshort{gpu} SM Slices} se \textbf{particionan}.
                    \end{itemize}
              \item Añade aceleración en las operaciones con matrices \textbf{dispersas} (\textit{sparsity}), que son matrices con una gran proporción de ceros entre sus valores.
          \end{itemize}
          La \textbf{Jetson Orin Nano}, \textbf{Jetson AGX Orin} y la \textbf{GeForce RTX 3050} (gráfica del portátil) siguen esta arquitectura.
    \item Blackwell: se introducen las siguientes mejoras:
          \begin{itemize}
              \item Incorpora los tensor cores de quinta generación, que permiten operar en precisión \textbf{FP4} y \textbf{FP6} (punto flotante de 4 y 6 bits respectivamente) sin apenas pérdida en la precisión, gracias a la incorporación de \textit{micro-tensor scaling}, que aplica diferentes factores de escalado a los elementos de un \gls{tensor} \cite{blackwell}.
          \end{itemize}
          La \textbf{Jetson AGX Thor} sigue esta arquitectura.
\end{itemize}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.5\linewidth]{imagenes/mastel.png}
    \caption{Asignación de bits según el tipo de dato, figura extraída de \cite{tf32}}
    \label{fig:types}
\end{figure}

La figura \ref{fig:types} muestra algunas de las precisiones vistas y que se aplicarán para las inferencias de los modelos de este proyecto (a excepción del formato BF16). TODO:

\subsubsection{Familia Jetson}

%A continuación se expone el listado de dispositivos sujetos de las pruebas de este proyecto.

Es una familia de \textbf{\glspl{embsystem}} pensados para aplicaciones de \acrshort{ia}. Dichos dispositivos incluyen un kit de desarrollo llamado \textbf{JetPack}, diseñado para exprimir la potencia de la NVIDIA Jetson en aplicaciones como la robótica, \acrshort{ia} generativa y visión artificial \cite{jason}.

\input{contenido/tables/jetsons.tex}

Actualmente existe una amplia variedad en la potencia y precio de estos dispositivos \cite{jason}. Para este proyecto, se han empleado cuatro modelos diferentes que se exponen en la tabla \ref{tab:jetsons} (se encuentran ordenados de menores a mayores prestaciones).

La Jetson AGX Thor soporta \acrshort{mig}, que como ya se ha comentado, divide la \acrshort{gpu} en varias particiones físicas, de forma que se puede asignar un conjunto de recursos exclusivos a una aplicación de CUDA, sin interferencias de otras aplicaciones. La Jetson AGX Thor tiene la siguiente configuración (información extraída de los comandos \textbf{nvidia-smi mig}):
\begin{itemize}
    \item 1 \textbf{\acrshort{gpu} Instance}: denominada \textbf{3g.0gb}, donde 0gb indica el tamaño de la memoria compartida en \acrshort{gb}s (TODO: debido a que la memoria de la \acrshort{gpu} es compartida con la \acrshort{cpu}, la \acrshort{gpu} no la particiona), y 3g, que indica 3 \acrshort{gpu} SM Slices (TODO: que equivalen a 3 \gls{sm}). Está compuesta por las siguientes \textbf{Compute Instances}:
          \begin{itemize}
              \item \textbf{1c.3g.0gb}:
                    \begin{itemize}
                        \item Instancias disponibles: 2 (TODO: 2 instancias de 3 \glspl{sm} cada una)
                        \item \gls{sm} dedicados: 6
                    \end{itemize}
              \item \textbf{2c.3g.0gb}:
                    \begin{itemize}
                        \item Instancias disponibles: 1 (TODO: 1 instancia de 6 \glspl{sm} cada una)
                        \item \gls{sm} dedicados: 8
                    \end{itemize}
              \item \textbf{3c.3g.0gb}
          \end{itemize}
\end{itemize}

Desafortunadamente, el software que controla el \acrshort{mig} en la Jetson no funciona adecuadamente, ya que no permite la reserva de las \textbf{Compute Instances}. Este error está relacionado con Jetpack 7.0 \cite{notworking}, por lo que instalar Jetpack 7.1 \textbf{podría} habilitar el \acrshort{mig} en la Jetson AGX Thor.

\subsubsection{Familia Intel}

\input{contenido/tables/x86.tex}

La tabla \ref{tab:x86} muestra los equipos con \acrshort{cpu}s Intel empleados en el proyecto. La columna \textit{Reference} muestra el equipo de pruebas utilizado en \cite{andrew} para realizar comparaciones de resultados.


\section{Software específico}
\label{sec:sw}

\subsection{ROS}
\label{sec:ros}

\acrfull{ros} es un middleware de \textbf{código abierto} que incorpora las herramientas necesarias para la interacción y desarrollo de software en robots. Cuenta con una amplia biblioteca de distribuciones y librerías.

Cualquier entorno de desarrollo de \acrshort{ros} está compuesto por \textbf{paquetes}, los paquetes son las unidades funcionales del entorno de \acrshort{ros} y estas contienen el código, archivos de configuración y de lanzamiento de los nodos, entre otros componentes.
\subsubsection{Arquitectura de la red}

\acrshort{ros} crea y mantiene una \textbf{red distribuida} en la que los nodos se ejecutan e intercambian mensajes. Sigue un patrón \textit{publish/subscribe}, en el que existen nodos que publican información por medio de \textbf{tópicos}, disponibles para cualquier nodo que se suscriba a dicho tópico. Según la versión, la arquitectura de la red varía en una de las siguientes:

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.5\linewidth]{imagenes/mastel.png}
    \caption{Arquitectura maestro-esclavo, figura extraída de \cite{mastel}}
    \label{fig:mastel}
\end{figure}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.5\linewidth]{imagenes/dds.jpg}
    \caption{Topología bus de Data Distribution Service (DDS), figura extraída de \cite{bus}}
    \label{fig:mastel}
\end{figure}

\begin{itemize}
    \item Maestro-esclavo (\acrshort{ros} 1): el maestro (denominado \acrshort{ros}\_MASTER) registra a todos los publicadores/subscriptores de la red, de forma que si un nodo quiere suscribirse/publicar un tópico, este se comunica con el maestro para registrar/conocer la manera de conectarse a dicho tópico \cite{mastel}. El principal inconveniente es que el nodo central es un \textbf{punto único de fallo} (si el maestro falla, todo el sistema se cae). La figura \ref{fig:mastel} describe el comportamiento descrito.
    \item Data Distribution Service (\acrshort{ros} 2): es un \textit{middleware} que permite la comunicación de los nodos mediante una topología tipo bus (figura ). Los nodos notifican su presencia en la red y esperan mensajes de respuesta con información sobre el resto de nodos existentes \cite{dds}. De esta forma, todos los nodos conocen el estado de la red sin depender de un maestro que gestione todas las comunicaciones.
\end{itemize}

Existe una amplia variedad de distribuciones de este software según la versión del \acrshort{so}. Para este proyecto, se han empleado las distribuciones Noetic, Humble y Rolling, para Ubuntu 20.04, 22.04 y 24.04 respectivamente, el Summit\_XL tiene la distribución Melodic, ya que tiene la versión 18.04 de Ubuntu. Melodic y Noetic corresponden a ROS 1 (descontinuado), mientras que Humble y Rolling pertenecen a ROS 2.

\subsubsection{TODO: lanzamiento de los threads}

\subsubsection{Rosbag}

Existe una herramienta del ecosistema \acrshort{ros} que graba y reproduce los datos capturados de la ejecución de un sistema \acrshort{ros} en un momento concreto en el tiempo. Los \glspl{dataset} creados en \cite{andrew} se construyeron partiendo de un fichero rosbag, que contiene toda la información generada por los diferentes sensores. En este proyecto se emplearán dichos ficheros rosbag para reproducir las pruebas en tiempo real.

\subsection{Optimizadores y \textit{backends} de inferencias}

\textbf{TensorRT}
Es un software de \textbf{código abierto} desarrollado por NVIDIA para la optimización de inferencias de modelos de IA en aceleradoras de NVIDIA \cite{x36}. Permite ejecutar modelos entrenados en \textit{frameworks} como Pytorch o TensorFlow, aunque se aconseja exportarlos al formato \acrfull{onnx} previamente, para aprovechar al completo las funcionalidades de este software. Se ha empleado TensorRT para optimizar la mayoría de modelos \acrshort{cnn} tanto en equipos \acrshort{arm} como x86. TensorRT versión 10 está disponible para las aceleradoras de NVIDIA con un \textit{compute capability} igual o superior a 7.5. En el caso de querer usar el software con una \textit{compute capability} inferior, es necesario instalar versiones anteriores descontinuadas (ejemplo: versión 8). El código generado en la versión 8 de la API \textbf{no es compatible} con la versión 10.

\textbf{OpenVINO}
Es un software de \textbf{código abierto} desarrollado por Intel para la optimización de inferencias en \acrshort{cpu} (ARM,x86) y en aceleradoras de Intel (\acrshort{gpu},\acrshort{npu}) \cite{OpenVINO}. Permite la conversión directa de modelos entrenados en frameworks como Tensorflow y Pytorch a ficheros XML, que son versiones optimizadas de las \acrshort{cnn} y que se utilizan para realizar las inferencias. Se ha empleado para reducir la latencia de modelos a la hora de ejecutarlos en \acrshort{cpu}.
\subsection{Librerías de Python}
\textbf{PyCUDA}
Librería de Python de \textbf{código abierto} \cite{pycuda} que interacciona con el driver de CUDA por medio de un \gls{wrapper}. Se ha utilizado para programar los códigos de inferencia de los modelos que utilizan TensorRT.
\textbf{NumPy}
Librería de Python de \textbf{código abierto} usada en este proyecto para representar imágenes y la información generada por los modelos en forma de vectores y matrices multidimensionales. Estos elementos pueden manipularse por medio de una amplia cantidad de funciones matemáticas que dicha librería ofrece.

\textbf{Scipy}
Librería de Python de \textbf{código abierto} que implementa numerosos algoritmos relacionados con la computación científica. En este proyecto se ha usado para aplicar el método húngaro, hallar los parámetros que modelan una distribución de Weibull, calcular distancias coseno, entre muchas otras funcionalidades que dicho software ofrece.

\textbf{OpenCV}
La librería de \textbf{código abierto} por excelencia para visión por computadora de código abierto. Otorga herramientas para la obtención y procesado de las imágenes, además de un módulo de visión artificial (dnn), que incluye modelos ya entrenados de visión artificial. Dicha librería tiene implementación en CUDA (si se compila el código fuente \cite{OpenCVCUDA}.), lo que permite ejecutar operaciones y kernels de modelos en aceleradoras de NVIDIA (ejemplo: NVIDIA Jetson).

\subsection{Docker}
Es un software de virtualización de \textbf{código abierto}. Permite realizar despliegues automatizados mediante ficheros de configuración, el software se ejecuta en un entorno aislado del sistema operativo llamado \textbf{contenedor}, dichos contenedores proporcionan seguridad y portabilidad. Esta herramienta ha sido de gran utilidad para desplegar el software del proyecto en los diferentes dispositivos.

\section{Software general}
\subsection{VS Code}
\subsection{Git}
\subsection{Trello}
\subsection{Draw.io}