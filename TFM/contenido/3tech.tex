\chapter{Fundamentos tecnológicos}
\lettrine{E}{n} este capítulo se detallan las herramientas hardware y software empleadas para el desarrollo y la gestión del proyecto.

\section{Dispositivos de altas prestaciones}

\subsection{Arquitecturas de GPU}

Según la arquitectura de la \acrshort{gpu} de NVIDIA, se agregan nuevas características que maximizan el rendimiento de la inferencia de los modelos (se ahondará en este tema en el capítulo \ref{chap:cnn}). Las arquitecturas que se muestran a continuación están ordenadas de menor a mayor generación.

Todas las arquitecturas que se exponen permiten el entrenamiento e inferencia de modelos en \textbf{precisión mixta}, es decir, escoger la precisión apropiada entre una lista de precisiones (ejemplo: \acrshort{fp}32+\acrshort{fp}16) para cada capa del modelo.

\subsubsection{Volta y Turing}
En la arquitectura Volta \cite{volta} (2017), se introducen los \textbf{tensor cores}, que son cores destinados a acelerar multiplicaciones y sumas de matrices (conocido también como MMA).

Volta tiene el código 7.0 como compute capability y es empleada por la \textbf{Jetson Xavier NX}.

En la arquitectura Turing (2018)... La \textbf{GeForce GTX 1650} (gráfica del PC lab) sigue esta arquitectura.

Turing tiene el código 7.0 como compute capability y es empleada por la \textbf{Jetson Xavier NX}.

\subsubsection{Ampere}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.5\linewidth]{imagenes/types.png}
    \caption{Asignación de bits según el tipo de dato, figura extraída de \cite{tf32}}
    \label{fig:types}
\end{figure}

Con la arquitectura Ampere \cite{ampere} (2020) se incorporan los tensor cores de tercera generación, que incluye los formatos de precisión \textbf{TF32} (Tensor Float 32). TF32 es un tipo optimizado para los \textbf{tensor cores}, híbrido entre \acrshort{fp}32 y \acrshort{fp}16, ya que de rango o exponente tiene los mismos bits que \acrshort{fp}32 y de mantisa o precisión tiene los mismos bits que \acrshort{fp}16 (como se muestra en la figura \ref{fig:types}). Debido a que TF32 acepta una entrada de tipo \acrshort{fp}32 y una salida también de tipo \acrshort{fp}32, dicho formato puede aplicarse sin realizar ningún cambio en el código.

Se introduce la funcionalidad \acrfull{mig}, que permite la partición \textbf{física} de \acrshort{gpu}s para correr aplicaciones de CUDA. A la hora de asignar particiones, es necesario comprender los siguientes conceptos \cite{migconcepts}:
\begin{itemize}
    \item \textbf{\acrshort{gpu} Engine}: es la parte de la \acrshort{gpu} que ejecuta el trabajo. Un ejemplo es el copy engine, que se encarga de realizar las operaciones relacionadas con la memoria (direct memory access, memcpy...).
    \item \textbf{\acrshort{gpu} Memory Slice}: porción de los controladores de memoria y caché de la \acrshort{gpu}.
    \item \textbf{\acrshort{gpu} SM Slice}: porción de los \glspl{sm} de la \acrshort{gpu}.
    \item \textbf{\acrshort{gpu} Slice}: porción de la \acrshort{gpu} que combina un único \textbf{\acrshort{gpu} Memory Slice} y un único \textbf{\acrshort{gpu} SM Slice}.
    \item \textbf{\acrshort{gpu} Instance}: conjunto de \textbf{\acrshort{gpu} slices} y \textbf{\acrshort{gpu} engines}.
    \item \textbf{Compute Instance}: una o múltiples instancias que pertenecen a una \textbf{\acrshort{gpu} Instance}. Entre ellas se comparten las \textbf{\acrshort{gpu} Memory Slices} y los \textbf{\acrshort{gpu} Engines}, mientras que los \textbf{\acrshort{gpu} SM Slices} se \textbf{particionan}.
\end{itemize}

Se añade aceleración en las operaciones con matrices \textbf{dispersas} (\textit{sparsity}), que son matrices con una gran proporción de ceros entre sus valores.

La \textbf{Jetson Orin Nano}, \textbf{Jetson AGX Orin} y la \textbf{GeForce RTX 3050} (gráfica del portátil) siguen esta arquitectura.

\subsubsection{Blackwell}

Blackwell \cite{blackwell} (2024) es la última arquitectura diseñada para \acrshort{gpu}s de NVIDIA, en ella se incorporan los tensor cores de quinta generación, que permiten operar en las precisiones \textbf{\acrshort{fp}4} y \textbf{\acrshort{fp}6} (punto flotante de 4 y 6 bits respectivamente).

Se lanza la segunda generación del \textbf{Transformer Engine} \cite{transformer}: introducido en las arquitecturas Ada Lovelace y Hopper (posteriores a Ampere), es una librería que permite manejar modelos en precisiones muy bajas (a partir de \acrshort{fp}8 para abajo) a través de múltiples técnicas que computan los \textbf{factores de escalado}, que permiten ajustar los valores de los \glspl{tensor} a un rango más compacto y representable por precisiones más pequeñas.

En esta nueva versión para la Blackwell, se incorpora el \textit{micro-tensor scaling}, que aplica diferentes factores de escalado al nivel de los elementos de un \gls{tensor}. Esta nueva técnica hace posible la ejecución en \textbf{\acrshort{fp}4} y \textbf{\acrshort{fp}6} sin apenas pérdida en la precisión.

La \textbf{Jetson AGX Thor} sigue esta arquitectura.

\subsection{Familia Jetson}

%A continuación se expone el listado de dispositivos sujetos de las pruebas de este proyecto.

Es una familia de \textbf{\glspl{embsystem}} pensados para aplicaciones de inteligencia artificial. Dichos dispositivos incluyen un kit de desarrollo llamado \textbf{JetPack}, diseñado para exprimir la potencia de la NVIDIA Jetson en aplicaciones como la robótica, inteligencia artificial generativa y visión artificial \cite{jason}.

\input{contenido/tables/jetsons.tex}

Actualmente existe una amplia variedad en la potencia y precio de estos dispositivos \cite{jason}. Para este proyecto, se han empleado cuatro modelos diferentes que se exponen en la tabla \ref{tab:jetsons} (se encuentran ordenados de menores a mayores prestaciones).

La Jetson AGX Thor soporta \acrshort{mig}, que como ya se ha comentado, divide la \acrshort{gpu} en varias particiones físicas, de forma que se puede asignar un conjunto de recursos exclusivos a una aplicación de CUDA, sin interferencias de otras aplicaciones. La Jetson AGX Thor tiene la siguiente configuración (información extraída de los comandos \textbf{nvidia-smi mig}):
\begin{itemize}
    \item 1 \textbf{\acrshort{gpu} Instance}: denominada \textbf{3g.0gb}, donde 0gb indica el tamaño de la memoria compartida en \acrshort{gb}s (TODO: debido a que la memoria de la \acrshort{gpu} es compartida con la \acrshort{cpu}, la \acrshort{gpu} no la particiona), y 3g, que indica 3 \acrshort{gpu} SM Slices (TODO: que equivalen a 3 \gls{sm}). Está compuesta por las siguientes \textbf{Compute Instances}:
          \begin{itemize}
              \item \textbf{1c.3g.0gb}:
                    \begin{itemize}
                        \item Instancias disponibles: 2 (TODO: 2 instancias de 3 \glspl{sm} cada una)
                        \item \gls{sm} dedicados: 6
                    \end{itemize}
              \item \textbf{2c.3g.0gb}:
                    \begin{itemize}
                        \item Instancias disponibles: 1 (TODO: 1 instancia de 6 \glspl{sm} cada una)
                        \item \gls{sm} dedicados: 8
                    \end{itemize}
              \item \textbf{3c.3g.0gb}
          \end{itemize}
\end{itemize}

Desafortunadamente, el software que controla el \acrshort{mig} en la Jetson no funciona adecuadamente, ya que no permite la reserva de las \textbf{Compute Instances}. Este error está relacionado con Jetpack 7.0 \cite{notworking}, por lo que instalar Jetpack 7.1 \textbf{podría} habilitar el \acrshort{mig} en la Jetson AGX Thor.

\subsection{Familia Intel}

\input{contenido/tables/x86.tex}

La tabla \ref{tab:x86} muestra los equipos con \acrshort{cpu}s Intel empleados en el proyecto. La columna \textit{Reference} muestra el equipo de pruebas utilizado en \cite{andrew} para realizar comparaciones de resultados.

\subsection{Comparación entre familias}
Las diferencias y similitudes clave a la hora de comparar la familia Jetson con la Intel se describen a continuación.
\begin{itemize}
    \item La familia Intel posee \acrshort{gpu}s y \acrshort{cpu}s de \textbf{alta frecuencia} respecto a la familia Jetson.
    \item Las \acrshort{gpu}s de la familia Intel \textbf{no poseen tensor cores}.
    \item Las \acrshort{gpu}s de la familia Intel son \textbf{discretas}, es decir, tienen su propia memoria (\acrshort{dram}) separada de la \acrshort{cpu}, con la que se comunica mediante PCIe. Por otro lado, las \acrshort{gpu}s de la familia Jetson se encuentran \textbf{integradas con la \acrshort{cpu}}, es decir, ambas comparten la memoria del sistema.

          Las \acrshort{gpu}s integradas pueden beneficiarse de la \textbf{memoria fijada mapeada} (o pinned mapped memory) de CUDA. La memoria fijada es una región de memoria que no puede ser paginada de vuelta a disco, de forma que siempre se mantiene en \acrshort{dram}, lo que acelera las transferencias de datos. Si la \acrshort{gpu} es integrada, entonces el acceso a la memoria entre el \textit{host} (o \acrshort{cpu}) y el \textit{device} (o \acrshort{gpu}) es \textbf{directo} (se pueden mapear directamente las direcciones de memoria ente el \textit{host} y el \textit{device}), de este modo, se \textbf{evitan copias de memoria} (cudaMemcpy), lo que permite ahorrar la latencia asociada a estas operaciones \cite{leimao}.

          La figura \ref{fig:memcpy} muestra un esquema de interconexión de una \acrshort{gpu} discreta (familia Intel) y una \acrshort{gpu} integrada (familia Jetson), en la que la primera requiere un cudaMemcpy para poder disponer de los datos en la \acrshort{gpu} y la segunda aplica la técnica \textbf{zero-copy}, que mediante la memoria fijada mapeada evita realizar transferencias.
\end{itemize}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.75\linewidth]{imagenes/memcpy.png}
    \caption{Arquitectura de interconexión de una \acrshort{gpu} discreta y una \acrshort{gpu} integrada. Figura extraída de \cite{zerocpy}}
    \label{fig:memcpy}
\end{figure}

\section{Hardware}
\label{sec:hw}

\subsection{Summit\_XL}
\label{sec:summit}

\begin{figure}[tbp]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{imagenes/KINECT.png}
        \caption{Cámara Microsoft Kinect}
        \label{fig:kin1}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \includegraphics[width=\textwidth]{imagenes/SUMKINECTS.png}
        \caption{Cámaras Kinect instaladas}
        \label{fig:kin2}
    \end{subfigure}
    \caption{Summit\_XL con los sensores instalados}
\end{figure}

El sistema del proyecto se ha construido para funcionar en el robot móvil Summit\_XL \cite{rb}. Dicho robot es de tipo \textbf{diferencial} (es decir, permite el movimiento en todas las direcciones excepto en sus laterales) y dispone de 4 ruedas de goma para su movilidad en una amplia variedad de entornos. Entre sus componentes, tiene instalado un sensor Velodyne \acrshort{lidar} \cite{lidar} (sensor circular de la figura \ref{fig:kin2}) que otorga nubes de puntos \acrshort{3d} del entorno y 1 cámara \acrshort{rgb} frontal, que no se ha empleado en este proyecto debido a su baja calidad de imagen. A mayores, se instalaron \textbf{2 cámaras Kinect \acrshort{rgbd}} en los laterales (figura \ref{fig:kin2}).

El Summit\_XL utiliza una arquitectura basada en el \textbf{\textit{middleware} \acrshort{ros}} \cite{ROS}, que permite la fácil interacción e integración de múltiples sensores. Este cuenta con soporte para las versiones de \acrshort{ros} 2, sin embargo, el modelo adquirido por el \acrshort{citic} emplea \acrshort{ros} 1, que a fecha de esta memoria ya ha alcanzado su \textbf{final de vida} \cite{ROSEOL}.

\subsection{Cámaras \acrshort{rgbd}}

Para nutrir al sistema de imágenes se cuenta con las cámaras \acrshort{rgbd} Kinect de Microsoft \cite{kinect} (figura \ref{fig:kin1}). Dichas cámaras capturan la información del entorno en forma de imágenes \acrfull{rgb} y realiza reconstrucciones \acrshort{3d} en tiempo real, que otorga en forma de imágenes de profundidad o de distancias (depth).

Para este proyecto, se cuenta con 2 de estas cámaras, cada una con una cobertura de visión de 57º. Dichas cámaras se disponen de forma que \textbf{no existan solapes} (en los laterales del robot, como se muestra en la figura \ref{fig:kin2}), lo que otorga un rango de visión total de 117º.

\section{Software específico}
\label{sec:sw}

\subsection{ROS}
\label{sec:ros}

\acrfull{ros} \cite{ROS} es un \textit{middleware} de \textbf{código abierto} que incorpora las herramientas necesarias para la interacción y desarrollo de software en robots. Cuenta con una amplia biblioteca de distribuciones y librerías.

Cualquier entorno de desarrollo de \acrshort{ros} está compuesto por \textbf{paquetes}, los paquetes son las unidades funcionales del entorno de \acrshort{ros} y estas contienen el código, archivos de configuración y de lanzamiento de los nodos, entre otros componentes.

\acrshort{ros} crea y mantiene una \textbf{red distribuida} en la que los nodos se ejecutan e intercambian mensajes. Sigue un patrón \textbf{\textit{publish/subscribe}} (publicador/subscriptor), en el que existen nodos que publican información por medio de \textbf{tópicos}, disponibles para cualquier nodo que se suscriba a dicho tópico.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.75\linewidth]{imagenes/mastel.png}
    \caption{Arquitectura maestro-esclavo, figura extraída de \cite{mastel}}
    \label{fig:mastel}
\end{figure}

En \acrshort{ros} 1, se sigue una arquitectura \textbf{maestro-esclavo}. El maestro (denominado \acrshort{ros}\_MASTER) registra a todos los publicadores/subscriptores de la red, de forma que si un nodo quiere suscribirse/publicar un tópico, este se comunica con el maestro para registrar/conocer la manera de conectarse a dicho tópico \cite{mastel}. El principal inconveniente es que el nodo central es un \textbf{punto único de fallo} (si el maestro deja de estar disponible, todo el sistema falla). \acrshort{ros} 2 presciende de un nodo maestro, por lo que no sufre de dicha limitación. La figura \ref{fig:mastel} describe el comportamiento descrito.

Existe una amplia variedad de distribuciones de este software según la versión del \acrshort{so}. Para este proyecto, se han empleado las distribuciones Noetic, Humble y Rolling, para Ubuntu 20.04, 22.04 y 24.04 respectivamente, el Summit\_XL tiene la distribución Melodic, ya que tiene la versión 18.04 de Ubuntu. Melodic y Noetic corresponden a \acrshort{ros} 1 (descontinuado), mientras que Humble y Rolling pertenecen a \acrshort{ros} 2.

\subsection{Librerías de Python}
Se procede a comentar las librerías de Python más importantes a la hora de desarrollar el código del proyecto (todas las librerías descritas son de \textbf{código abierto}):
\begin{description}
    \item[PyCUDA] \cite{pycuda} Es un \gls{wrapper} que interacciona con el driver de CUDA. \textbf{CUDA (\textit{Compute Unified Device Architecture})} es un conjunto de herramientas, que incluye un compilador para compilar código en lenguaje CUDA, que es una variante del lenguaje C. Dicho código permite la ejecución de algoritmos en \acrshort{gpu}s de propósito general de NVIDIA. Se ha utilizado principalmente para generar los contextos y reservar la memoria en la \acrshort{gpu}, con el propósito de realizar las inferencias de los modelos.
    \item[NumPy] \cite{np} Es una librería que incluye estructuras de datos y funciones para operar con matrices multidimensionales. Se ha empleado en este proyecto para representar las imágenes y la información generada por los modelos en forma de vectores y matrices multidimensionales. Estos elementos pueden manipularse por medio de una amplia cantidad de funciones matemáticas que dicha librería ofrece.
    \item[Scipy] \cite{scipy} Implementa numerosos algoritmos relacionados con la computación científica. En este proyecto se ha usado para aplicar el método húngaro, hallar los parámetros que modelan una distribución de Weibull, calcular distancias coseno, entre otras aplicaciones relacionadas con cálculos científicos.
    \item[OpenCV] \cite{cv2} Es la librería por excelencia para visión por computadora de código abierto. Se ha utilizado para la obtención y procesado de las imágenes. La librería contiene un módulo de redes neuronales profundas (dnn), el cual se ha utilizado para realizar operaciones de procesamiento de las entradas y salidas de las redes neuronales (ejemplo: aplicar \gls{nms} a las \glspl{bbox} de salida de un modelo de detección de objetos).
\end{description}


\subsection{Optimizadores y runtimes de inferencias}

Es el software encargado de optimizar la implementación de los modelos y ejecutar sus inferencias. En este proyecto se han utilizado los siguientes (todos son de \textbf{código abierto}):
\begin{description}
    \item[TensorRT] \cite{x36} Es un ecosistema de herramientas (con \acrshort{api} de Python incluida) para la optimización de inferencias de modelos de inteligencia artificial en aceleradoras de NVIDIA. Permite ejecutar modelos entrenados en \textit{frameworks} como Pytorch o TensorFlow, aunque se aconseja exportarlos al formato \acrfull{onnx} previamente, para aprovechar al completo las funcionalidades de este software. Se ha empleado en este proyecto para optimizar y ejecutar inferencias de la mayoría de modelos \acrshort{cnn} tanto en equipos \acrshort{arm} como x86.

        La versión 10 de TensorRT está disponible para las aceleradoras de NVIDIA con un \textit{compute capability} igual o superior a 7.5. En el caso de querer usar el software con una \textit{compute capability} inferior (ejemplo: Jetson Xavier NX), es necesario instalar la versión 8 u otra anterior. El código de la \acrshort{api} generado en la versión 8 \textbf{no es compatible} con la versión 10.

    \item[OpenVINO] \cite{OpenVINO} Desarrollado por Intel para la optimización de inferencias en \acrshort{cpu}s (ARM,x86) y en aceleradoras de Intel (\acrshort{gpu},\acrshort{npu}). Permite la conversión directa de modelos entrenados en frameworks como Tensorflow y Pytorch a ficheros XML, que son versiones optimizadas de las \acrshort{cnn} y que se utilizan para realizar las inferencias. Se ha empleado para optimizar y ejecutar las inferencias de los modelos a la hora de ejecutarlos en \acrshort{cpu}.

    \item[OpenCV] \cite{cv2} El módulo de redes neuronales profundas (dnn) incluye modelos ya entrenados y un \gls{rt} para realizar inferencias. En este proyecto se ha utilizado un modelo de detección de caras ofrecido por dicha librería. Dicho módulo puede ejecutarse en \textbf{CUDA}, aunque solo si se \textbf{compila el código fuente}. El proceso seguido para compilar OpenCV es el mismo que en \cite{OpenCVCUDA}.
\end{description}

\subsection{Docker}
Docker \cite{docker} es un software de \textbf{virtualización} de \textbf{código abierto}, que permite realizar despliegues automatizados mediante ficheros de configuración. El software se ejecuta en un entorno aislado del sistema operativo llamado \textbf{contenedor}, dichos contenedores proporcionan seguridad y portabilidad. Esta herramienta ha sido de gran utilidad para desplegar el software del proyecto en los diferentes dispositivos.

\section{TODO: Software general}
\subsection{VS Code}
\subsection{Git}
\subsection{Trello}
\subsection{Draw.io}