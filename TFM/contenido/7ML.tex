\chapter{Reconocimiento facial adaptativo}
\label{chap:adaptation}

\lettrine{E}{n} este capítulo se ahonda en los fundamentos y detalles de implementación de la funcionalidad \textit{Open-World} y demás componentes para otorgar la adaptación.

\section{Fundamentos}
\label{subsec:erikfounds}

TODO: El método propuesto en \cite{Erik} y que conforma la base del sistema implementado, ha sido probado en un contexto de video vigilancia bajo el \textit{\gls{dataset}} \textbf{FACE COX} \cite{cox}. También se ha utilizado el \textit{\gls{dataset}} \textbf{Youtube Faces} (YTF) \cite{ytf} que, a diferencia del anterior, se encuentra accesible de forma pública. En este proyecto se utiliza un \textit{\gls{dataset}}\dots
El método utiliza \textbf{secuencias de video} como entrada para devolver las predicciones acerca de un IoI o para determinar una identidad desconocida.

\subsubsection{Comités de SVM}
TODO: El sistema

Las características extraídas por ArcFace se utilizan
como datos para entrenar las SVM para la clasificación.
OSDe-SVM se basa en el concepto de que múltiples SVM simples (ejemplo: lineales) como conjunto (\textbf{comité}) \textbf{generalizan mejor} que una única SVM compleja (ejemplo: sigmoide) \cite{malisiewicz2011ensemble}. Otras ventajas de los comités son la incorporación de nueva información sin tener que re-entrenar las SVM, lo que recorta una cantidad de tiempo significativa durante la operación del sistema, y la flexibilidad, que permite eliminar parte de la información que no es relevante borrando la SVM redundante. Para cada IoI se asigna un comité de SVM, cada SVM del comité se entrena con n muestras de la propia entidad (1 en la idea original \cite{malisiewicz2011ensemble}, 5 adaptado a este proyecto) y \textbf{m-n} muestras negativas (fotogramas de otras entidades), siendo \textbf{m} el número de IoI registrados en el sistema.

\subsection{Arquitectura del sistema adaptativo}
\label{subsec:adaptarch}

El sistema se estructura como muestra la figura \ref{fig:FACESYS}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{imagenes/FACESYS.drawio.png}
    \caption{a}
    \label{fig:FACESYS}
\end{figure}

\section{Implementación}
A partir de los mecanismos expuestos en el capítulo \ref{chap:video} se extraen las secuencias de frames que servirán como entrada del pipeline visto en la sección \ref{subsec:adaptarch} y que se expone en detalle a continuación. La mayoría de los componentes se han implementado basándose en \cite{Erik} TODO: como menciono también a César?.

\subsection{Inicialización}

La primera fase del sistema y sin lugar a dudas la más crítica. En este proyecto se han probado 2 aproximaciones:
\begin{itemize}
    \item \textbf{Supervisado}: el operador realiza una selección de los 5-10 frames más representativos para cada individuo.
    \item \textbf{No supervisado}: el sistema recoge los frames cuando aparecen un mínimo de 5 entidades simultaneas en escena (el mínimo necesario de puntos para poder aplicar Weibull). A partir de los frames de las cámaras, que estarán sincronizadas, se obtienen las \glspl{bbox} que encierran los rostros y se procede a la creación de los comités con los recortes de caras generados. \textbf{No se requiere de ninguna intervención del operador}, el sistema realiza la selección en base a unos criterios preestablecidos (ejemplo: la cara debe estar completamente dentro del rango de la cámara). En esta aproximación, es crucial el método de seguimiento (tracking) para recolectar las secuencias de caras.
\end{itemize}

Independientemente de la aproximación escogida, se obtienen las características de cada frame de la secuencia y se crea la \acrshort{svm} inicial con la que se \textbf{inicializa el comité}. Dicha \acrshort{svm} se entrena con las características del usuario, que compondrá el set positivo, y un subconjunto de las características del resto de usuarios, que compondrá el set de negativos (escogidas aleatoriamente). Dichas características son generadas por el modelo ArcFace \cite{deng2019arcface}, es la misma red que se utiliza en \cite{Erik, andrew} y que ha demostrado ser de las mejores en el \acrshort{sota} del reconocimiento facial.

La SVM inicial es la que define la identidad del comité. Si dicha SVM está compuesta por recortes de caras de baja calidad (ejemplo: borrosas o parcialmente ocluidas), entonces el comité \textbf{no estará bien definido} y, por lo tanto, generará \textbf{mayor confusión} a la hora de aplicar Weibull para los reconocimientos.

El resto de módulos del sistema que se exponen a continuación se ejecutan por cada secuencia de caras recolectada en cada secuencia de frames (figura \ref{fig:FACESYS}).

\subsection{\textit{Ensemble Decision Function} (EDF)}

Una vez inicializado los comités, el método determina identidades en base a secuencias de entrada. Por cada secuencia de entrada, siendo esta una secuencia de características extraídas de los recortes faciales, se calculan las \textbf{puntuaciones (scores) para cada comité}. La puntuación de un comité es a su vez un valor consensuado entre los resultados de las predicciones de las SVM que lo conforman, el criterio de consenso (o de fusión) se basa en un percentil (generalmente la media). Aplicar percentiles es igual a escoger un conjunto mas grande o pequeño de SVM, ya que pueden existir SVM dañinas para el comité (ejemplo: corresponden a otra persona). De esta forma, el percentil logra tolerar dichas puntuaciones dañinas. Para cada comité, se ejecutan las siguientes funciones:
\begin{itemize}
    \item \textbf{FDF} (Frame Decision Function): se encarga de \textbf{fusionar las salidas} (o puntuaciones) de todas las SVM del comité cuando se analiza \textbf{un frame} de la nueva secuencia.
    \item \textbf{SDF} (Sequence Decision Function): se fusionan todas las puntuaciones de la función FDF para obtener un único resultado que representa a la \textbf{secuencia}.
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{imagenes/De-SVM.png}
    \caption{Pipeline del método De-SVM, figura extraída de \cite{Erik}}
    \label{fig:De-SVM}
\end{figure}

\subsection{\textit{Recognition Decision Function} (RDF)}
Esta función determina si la entidad detectada se corresponde a un individuo previamente reclutado o a una entidad \textbf{desconocida}.

Las SVM sólo pueden discernir dentro del conjunto de datos con las que fueron entrenadas (\textit{Closed-Set}), por lo que un dato desconocido se clasificaría erróneamente como una de las clases del entrenamiento \cite{rudd2017extreme}. Varias investigaciones \cite{rudd2017extreme, scorenorm} TODO: (y en \cite{Erik}) utilizan el \textbf{\textit{Extreme Value Theory}} (EVT o teorema de Fisher–Tippett–Gnedenko) para identificar clases no reclutadas. El \textit{Extreme Value Theory} determina que el conjunto de máximos o mínimos de una muestra sigue una distribución de \textbf{Weibull}. El EVT otorga un conocimiento robusto, ya que convierte scores concretos de un algoritmo (en este caso las SVM) en probabilidades que siguen una teoría estadística. Esto permite la \textbf{fusión de datos de diferentes fuentes} como nuevas redes de reconocimiento o nuevas cámaras diferentes a las Kinect en el sistema.

La distribución de Weibull se modela a partir de las puntuaciones devueltas por cada comité (salida de la función EDF) \textbf{excepto la mejor puntuación} de todos los \textit{ensembles} (que se corresponde con la más baja). Debido a que la entidad en cuestión sólo puede coincidir con un comité, se comprueba si el mejor resultado (o lo que se supone que es la entidad de la secuencia) \textbf{es un extremo} respecto de la distribución de puntuaciones no coincidentes (resto de \textit{ensembles}), en caso afirmativo, se asigna la etiqueta del usuario del comité con la mejor puntuación, en caso contrario, el individuo se considera \textbf{desconocido} (no coincide con ningún comité). Para determinar si la puntuación es un extremo, se calcula la probabilidad (\gls{pdf}) del mejor \textit{score} a partir de la función \gls{pdf} con los parámetros obtenidos de la distribución de Weibull modelada. Si el \gls{pdf} se corresponde con un valor muy cercano al 0, la puntuación se considera que está \textbf{al extremo de la distribución} \cite{scorenorm}. La decisión de si una puntuación es un extremo o no se toma en base a un threshold (Tw), para el resultado de la función de Weibull (función \gls{pdf}).

TODO: El algoritmo compone una función de Weibull ajustando los parámetros de la misma para que converga con la cola de la distribución. Dicha distribución está formada por las distancias de cada puntuación con la mediana, excluyendo la puntuación más baja (siendo teóricamente la puntuación que coincide para la entidad) y las puntuaciones por encima de la mediana. Finalmente, se determina la probabilidad de pertenencia de la puntuación más baja de la distribución, si la probabilidad es lo suficientemente baja como para concluir que no pertenece a la distribución, entonces el reconocimiento es correcto y se asigna la entidad. En caso contrario, se concluye que la entidad es desconocida. Se establece un threshold (Tw, como se muestra en el algoritmo \ref{coud:weib}).

\input{contenido/codes/weib}

(TODO: explicación?) El Extreme Value Theory  determina que el valor \textbf{máximo} de una muestra sólo puede converger en una de las siguientes tres distribuciones: Fréchet, Gumbel o Weibull. Cada una de estas distribuciones poseen propiedades únicas (ejemplo: Fréchet es una distribución cuya cola decrece en menor medida que la de Gumbel) que las hacen adecuadas para determinados campos de estudio (TODO: ejemplo: Fréchet para predicción de inundaciones). TODO: En el campo del reconocimiento de personas, sería interesante hallar el mínimo por el que se puede determinar si cierto individuo pertenece o no a la distribución. Para este proyecto, el mínimo es la \textbf{mínima distancia entre el punto y el hiperplano creado por la SVM}, como se aplican varias SVM, el valor final será una fusión de distancias del conjunto de SVM. Como la muestra sólo puede pertenecer a una entidad, entonces se deberían de obtener resultados no coincidentes (TODO: distancias negativas que fueron normalizadas usando no se que método) con el resto de individuos registrados. La distribución de los valores mínimos, en este caso los resultados no coincidentes, sigue una de las 3 distribuciones ya comentadas (TODO: es la de Weibull porque el comportamiento de la cola es acotado, es decir, hay un cierto valor x que devuelve 0).

\subsection{Update Module}
Es el módulo que implementa la actualización de los comités tras el reconocimiento realizado en la anterior función. Dado un valor c, siendo este el resultado de la función RDF, pueden darse los siguientes dos escenarios:
\begin{itemize}
    \item \textbf{Entidad conocida (c<Tw)}: se crea una nueva SVM que
          será incluida en el comité ganador (el que tiene la puntuación más baja).
          La SVM se entrena con los embeddings de la secuencia de entrada como
          conjunto de positivos. Como conjunto de negativos, se realiza un muestreo aleatorio de embeddings \textbf{del resto de comités}.
    \item \textbf{Entidad desconocida (c>Tw)}: se crea una SVM en un nuevo comité representando a la entidad.
          El conjunto de positivos es el mismo que en el caso anterior. En el caso de los negativos,
          las muestras de cualquier comité son válidas.
\end{itemize}

Una condición necesaria para que este módulo se ejecute es que la
secuencia de entrada para el individuo \textbf{contenga un mínimo de frames para su inicialización},
dicho mínimo es fijado por el operador de antemano. Otra precondición,
que aplica a las entidades conocidas, es comprobar si las caras (o muestras) a añadir son lo suficientemente representativas.
Las puntuaciones cerca del cero indican que las muestras se encuentran en el borde de lo que es nuevo y lo que la SVM
ya conoce. A partir de un valor de umbral (\textit{update\_th}) se decide si dichas muestras añaden información nueva al comité.

\textbf{El tamaño del conjunto de positivos y de negativos es prefijado por el operador.}

\subsection{Limitation Module}
\label{seq:limmod}

El limitation module es una función que se encuentra inherente al módulo de actualización. Si un comité excede un número prefijado de SVM almacenadas (10 en este caso), se toma una decisión para eliminar una de las SVM según los siguientes criterios:
\subsubsection{Diversidad}
Este criterio mide el valor de aportación de una SVM respecto al resto del comité. Se coge un conjunto aleatorio de n embeddings (siendo n=50) de entre todos los comités y se generan los scores utilizando las m SVM del comité, lo que resulta en n*m scores. TODO: \textbf{Basándose en el signo de los scores}, se acumula el producto de los signos entre scores, de forma que un valor discordante afecta en mayor medida a la propia SVM y en menor medida al resto de SVM. Un valor alto indica bajo nivel de diversidad, por ende un valor \textbf{pobre de aportación al comité}. La figura \ref{fig:diversity} muestra la fórmula de diversidad, que se calcula como en \cite{Erik}.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{imagenes/Diversity.png}
    \caption{Fórmula de diversidad}
    \label{fig:diversity}
\end{figure}

\subsubsection{Coherencia}
Este criterio viene a determinar la precisión de una SVM en el reconocimiento cuando no se dispone de un \textit{\gls{dataset}} para su evaluación (ejemplo: entorno operacional). El valor de coherencia determina cuantas veces una SVM devuelve el mismo signo que el resultado consensuado del comité, si los signos coinciden, se suma 1 al valor de coherencia, en caso contrario, se resta -1 a dicho valor. Un valor alto indica buena precisión. En la creación de una SVM, este valor se inicializa a 0.

\subsubsection{Favorabilidad}
Finalmente, los dos criterios se fusionan en un valor llamado \textbf{índice de favorabilidad}, la SVM con el menor valor de dicho índice \textbf{se elimina del comité}. El signo del valor de diversidad \textbf{se invierte} a la hora de la fusión.

\subsection{Creación de nuevos comités (\textit{Open-World})}
Cuando se reconoce a un usuario como desconocido (supuestamente un individuo no antes reclutado), se registra su identidad en forma de un nuevo comité con una SVM inicial. De esta forma, el sistema adquiere la capacidad de expandir su conocimiento a partir de personas nunca antes vistas. Dicha SVM inicial tiene de muestras positivas las de la propia secuencia actual y de muestras negativas las del resto de individuos ya conocidos.

\subsection{Coherencia entre cámaras}

TODO: no es mas conveniente ejecutarlo antes del update module?

Si se dispone de varias cámaras sincronizadas en el mismo instante de tiempo, se puede asumir que no es posible que aparezca una misma persona en más de una cámara (aunque es necesario atender al grado de solape que pueda existir entre las cámaras). Si más de una cámara da una misma predicción, puede ocurrir que una o varias predicciones sean incorrectas, o que todas las predicciones sean erróneas. Se ha desarrollado el siguiente pipeline para intentar resolver las predicciones incorrectas: TODO.


\subsection{otra cosa}
En resumen, todo el pipeline comentado \textbf{realiza x*j*y*z iteraciones}, siendo x el nº de personas, j el nº de frames por persona, y el nº de comités (o entidades registradas) y z el nº de SVM por comité \textbf{para cada secuencia de entrada}.

TODO: YuNet en OpenCV \textbf{no admite batching} (o quizá YuNet en si), varias opciones:
\begin{itemize}
    \item (Más simple) secuencial: por cada frame que se recibe, ya se procesa por YuNet y se envia la lista cuando se llame a on\_frame (sabemos que la inferencia va a tardar menos de 100 ms).
\end{itemize}


