\chapter{Introducción}
\label{chap:introduccion}
\lettrine{E}{n} este capítulo se expone la motivación y los objetivos de este proyecto. Adicionalmente, se comenta la estructura por capítulos de esta memoria y brevemente su contenido.

\section{Motivación}
\label{sec:motivo}

El auge del Deep Learning (DL) y las \acrshort{cnn} han impulsado numerosos avances en aplicaciones como la visión artificial, procesado de nubes de puntos, etc \cite{Erik, ImageNet, MITTAL2019428}. Sin embargo, debido a sus elevados requerimientos computacionales, los modelos de \textit{Deep Learning} suelen ejecutarse en entornos en la nube \cite{MITTAL2019428}, lo que no es una solución viable en aplicaciones con ejecución en tiempo real, debido a la latencia en las respuestas. En este contexto, surge el concepto de \textbf{computación en el borde o Edge Computing}, que es un paradigma que consiste en procesar los datos desde su fuente de origen o desde un dispositivo muy cercano. De esta forma, se reduce la latencia de transmisión y se reparte el volumen de datos entre múltiples dispositivos \cite{EDCOM}. Un ejemplo sería procesar los datos de una cámara conectada directamente a un equipo, de esta forma se ahorraría todo el ancho de banda que implicaría transmitir una imagen por la red.

(TODO: aquí introduzco los robots guía y lo q se quiere hacer) Gracias de robots guía Gracias a los grandes avances en el aprendizaje máquina, la implementación de complejos sistemas dentro del campo de la robótica son posibles, incluso en dispositivos de bajas prestaciones. Este proyecto pretende extender un sistema de detección y reconocimiento capaz de agregar a nuevos usuarios bajo su confirmación. Se buscaron dispositivos que fueran potentes, pero a su vez de bajo consumo y fácilmente acoplables a un robot móvil, de forma que todas las operaciones del sistema no dependan de un ordenador central que limite la movilidad del robot. Esta funcionalidad es muy útil para implementar un robot móvil guía que asista a los usuarios proporcionando un trato cercano en todo momento (ejemplo: saludarlos por su nombre).

Por otro lado, existe el concepto de \textbf{TinyML} \cite{OLIVEIRA2024101153}, que consiste en la implementación de modelos compactos de forma eficiente en \glspl{embsystem} como microcontroladores (\textit{Low-end} TinyML, ejemplo: Arduino) u ordenadores de placa única (\textit{High-end} TinyML, ejemplo: NVIDIA Jetson), más potentes que los microcontroladores, pero menos económicos.

Aunque las \acrshort{cnn} han demostrado excelentes resultados, sin embargo, son fuertemente dependientes del conjunto de datos de entrenamiento, lo que acaba resultando en una pérdida en la precisión a la hora de manejar datos diferentes a los del entrenamiento, es decir, no generalizan bien \cite{Erik, malisiewicz2011ensemble}. El primer desafío consiste en reconocer cuando aparece una clase (identidad) que no pertenezca al conjunto de entrenamiento, es decir, detectar clases desconocidas (es lo que se conoce como \textit{Open-Set}). El segundo desafío será adaptar el sistema para reconocer esas nuevas entidades en el futuro. La perspectiva del aprendizaje logra superar esta limitación \cite{Erik}, al otorgar la capacidad de añadir nuevos datos durante el tiempo de operación (también conocido como \textit{Open-World}). El objetivo es que el sistema sea capaz de evolucionar y extenderse a nuevos contextos (entidades) sin necesidad de reentrenar las redes neuronales, lo que puede ser un proceso extremadamente costoso y que, a falta de un conjunto muy amplio y representativo de datos correctamente etiquetados, puede dar resultados pobres o al menos muy sesgados.

\section{Objetivos}

El primer objetivo de este proyecto es desarrollar un nuevo método de aprendizaje máquina para detectar desconocidos (\textit{Open-Set}) e incluir nuevos individuos (\textit{Open-World}) basado en las ideas mencionadas en la sección \ref{sec:motivo}. Concretamente, el sistema debe ser capaz de realizar lo siguiente:
\begin{itemize}
    \item Construir y procesar simultáneamente las secuencias de todas las caras detectadas en todas las cámaras, aplicando técnicas para el seguimiento y reidentificación de personas.
    \item Usar comités basados en clasificadores débiles (máquina de vectores de soporte o SVM) para identificar clases (usando votación mayoritaria).
    \item Adaptar los comités (añadiendo y limitando sus SVM) para seguir los cambios de esa entidad (\textit{\textbf{concept-drift}}) y crear una nueva identidad (comité) para cada desconocido.
    \item Aplicar la distribución de Weibull para detectar clases desconocidas.
    \item Inicializar el sistema de forma apropiada, bien de forma manual (las caras que pasan a formar parte del sistema se seleccionan minuciosamente), bien cuando el sistema detecta varias caras en el mismo instante de tiempo.
\end{itemize}

El segundo objetivo del proyecto será optimizar todo el software para funcionar de forma eficiente en la familia de sistemas embebidos NVIDIA Jetson y otros dispositivos relacionados (NVIDIA GeForce GTX y RTX). Se evaluará el rendimiento del sistema tanto de forma aislada (cada uno de sus módulos principales o con más carga computacional) como en conjunto con el sistema completo. Las pruebas se realizarán a partir de muestras (videos) de un entorno de operación real en interiores, con movimientos bruscos del robot (y de las cámaras) y entrecruzado de los individuos. También se realizarán diversas pruebas de estrés para analizar el comportamiento del sistema con distintas cargas de trabajo y también para comprobar hasta qué punto escala, por ejemplo, aumentando el número de cámaras a procesar.

Para facilitar su despliegue y mantenimiento, el sistema se empaquetará en contenedores de Docker y se migrará a \acrshort{ros} 2.TODO: ver este paper \cite{murthy2020investigations}.


\section{Trabajo relacionado}

\subsection{Aprendizaje incremental con adaptación a los cambios}
\label{sec:incrlearning}

En el contexto de este proyecto (detección y reconocimiento de personas en tiempo real), no es posible preparar un \gls{dataset} de entrenamiento compuesto por una gran multitud de etiquetas, debido a la naturaleza cambiante de los datos en el mundo visual \cite{Erik}, tampoco sería posible debido a la ley de protección de datos, que restringe la capacidad de recolección de estos datos. Por lo que es necesario adoptar un método que permita a los modelos de redes neuronales aprender, partiendo de un conjunto de datos muy escaso.

En este proyecto se explora la implementación de un algoritmo de aprendizaje incremental, que permite a los modelos poder integrar nueva información sin necesidad de reentrenar desde 0. Los algoritmos desarrollados en este campo de estudio se reúnen bajo los conceptos de \textbf{\textit{Open-Set}} y \textbf{\textit{Open-World}} \cite{rudd2017extreme, Erik, CESAR}. Mientras que el primero se limita a distinguir las clases conocidas de las desconocidas, el segundo permite incluir conocimiento al modelo a partir de las entradas, de forma que se logra adaptar a los múltiples factores cambiantes que entraña el mundo real.

El aprendizaje incremental se ha enfocado en dos vías, cada una con sus propios desafíos. Por un lado, se realizan predicciones, sin ningún aprendizaje previo, a partir de un \gls{batch} de datos, que se itera una y otra vez (\textit{batch learning}) \cite{Erik, rodeo, rodeo1}. Esta actualización resulta lenta de ejecutar para cada entrada e inoperativa para aplicaciones de sistemas embebidos en tiempo real \cite{rodeo}. Por el otro lado, se agrega el conocimiento de forma gradual a partir de pequeñas muestras de datos (\textit{streaming learning}), que permite un aprendizaje rápido y adaptable a los cambios en tiempo real \cite{Erik, rodeo, streaming2}.

A la hora de diseñar un nuevo algoritmo que incluya nuevo conocimiento, es fundamental establecer un balance entre añadir y actualizar las muestras ante los nuevos cambios producidos (\textit{concept-drift}) y evitar \textbf{borrar} el conocimiento previo (\textit{catastrophic forgetting}) \cite{Erik, CESAR, rodeo, streaming2, rodeo1}.

El uso de \textbf{comités}, que no son más que un conjunto de clasificadores, ha demostrado ser eficaz para tratar el problema del \textit{catastrophic forgetting} \cite{Erik, ensembles0, ensembles3}. Los comités permiten agregar conocimiento acerca de una entidad entrenando un nuevo clasificador o eliminar dicho conocimiento descartando el clasificador correspondiente. De esta forma, se obtiene una representación de un individuo a partir de un conjunto de clasificadores ligeros, sin necesidad de reentrenar un clasificador complejo desde cero \cite{malisiewicz2011ensemble}. Por otro lado, se ha demostrado que múltiples \acrshort{svm} simples (ejemplo: lineales) como conjunto (comité) \textbf{generalizan mejor} que una única \acrshort{svm} compleja (ejemplo: sigmoide) \cite{malisiewicz2011ensemble}.

A la hora de implementar el reconocimiento \textit{Open-Set}, destaca el uso del \acrfull{evt} \cite{Erik,CESAR,rudd2017extreme,scorenorm}. El \acrshort{evt} es una teoría estadística, que devuelve la probabilidad de que un evento represente un \textbf{extremo} respecto a una distribución de máximos o mínimos.

Los métodos propuestos en \cite{rudd2017extreme, Erik, CESAR} han sido probados en los \glspl{dataset} \textbf{FACE COX} \cite{cox}, \textbf{Youtube Faces} (YTF) \cite{ytf} e \textbf{ImageNet} \cite{ImageNet}, cuya adquisición de muestras se realiza en entornos controlados (buena iluminación, en las que suele aparecer un único objeto). Para probar el algoritmo desarrollado en este proyecto, se emplea un \gls{dataset} creado en \cite{andrew}. Dicho \gls{dataset} contiene frames borrosos, con múltiples individuos en escena entrecruzándose, caras ocluidas, etc, que son condiciones comunes en un entorno realista.

El procesamiento de video permite aprovechar la coherencia espacio-temporal, sobretodo en el contexto de las personas, cuyo movimiento es lento y predecible. Por otra parte, los métodos de seguimiento pueden ayudar a obtener muestras difíciles de reconocer para el aprendizaje de los modelos. Otro aspecto importante a la hora del seguimiento es la \textbf{reidentificación}, es decir, como localizar al individuo una vez se ha perdido su rastro (ejemplo: ha sido ocluido por otro individuo o por un objeto).

Para implementar el seguimiento y la \textbf{reidentificación} de objetos, se suele emplear el método húngaro combinado con el filtro de kalman \cite{Hungarian,MangoYOLO}. El \textbf{filtro de Kalman} predice la próxima posición del individuo a partir de la probabilidad Gausiana, de tal forma que puede mantenerse el rastro cuando la persona desaparece por unos frames. A pesar de ser una técnica efectiva, el filtro necesita de un mínimo de frames para converger cuando una nueva persona aparece \cite{MangoYOLO}, lo que no es beneficioso para secuencias de frames cortas.

\subsection{Aplicaciones de redes neuronales en sistemas embebidos}

La ejecución de modelos de inteligencia artificial en sistemas embebidos (o TinyML) sigue suponiendo un gran reto a día de hoy \cite{OLIVEIRA2024101153}.

\cite{tflitework} ha logrado la ejecución en tiempo real de dos deep conventional neural networks (DCNNs) en la Raspberry Pi 4 usando TensorFlow Lite, relegando en técnicas como la \gls{quant} de rango dinámico (los valores se cuantizan a enteros de 8 bits en tiempo de ejecución) y \gls{quant} en \acrshort{fp}16.

A la hora de diseñar aceleradores hardware para aplicaciones de redes neuronales es necesario mantener el balance entre precisión, rendimiento y bajo consumo. La NVIDIA Jetson es una familia de sistemas embebidos que persigue maximizar dicho balance \cite{MITTAL2019428, rahmaniar2021real}. Un claro ejemplo es el uso de la Jetson TX2 en aplicaciones como el reconocimiento facial para el procesamiento de videos en resolución Full HD (1080p) \cite{qi2018iot} o la detección de personas con cámaras \acrshort{rgb} en resolución HD (720p) \cite{rahmaniar2021real}, ambos casos de uso han reportado un rendimiento satisfactorio en tiempo real.

TensorRT es una pieza clave para la optimización de modelos en aceleradoras de NVIDIA, habilitando así la ejecución de aplicaciones de inteligencia artificial en tiempo real \cite{MITTAL2019428, rahmaniar2021real}.

\cite{mdpi}.


\section{Estructura de la memoria}

Esta memoria se divide en capítulos, cada uno con una explicación detallada del trabajo realizado. Se expone a continuación la estructura de esta memoria, explicando brevemente su contenido:

\begin{enumerate}
    \item \textbf{Introducción}: presente capítulo en el que se exponen los motivos de realización de este proyecto, los objetivos planteados, el trabajo relacionado y la estructura de la memoria resultante.
    \item \textbf{Fundamentos teóricos}: capítulo en el que se presentan los conceptos que se referenciarán a lo largo de la memoria.
    \item \textbf{Fundamentos tecnológicos}: capítulo que presenta los dispositivos sujetos de pruebas de este proyecto, así como las arquitecturas de las \acrshort{gpu}s que incorporan. También se exponen las herramientas hardware y software utilizadas.
    \item \textbf{Gestión del proyecto}: en este capítulo se presentan los requisitos, actores y casos de uso que definen el sistema propuesto, el plan de gestión de los riesgos, la metodología de desarrollo empleada y una vista detallada de la planificación del proyecto y su posterior seguimiento.
    \item \textbf{Arquitectura del sistema propuesto}: capítulo en el que se expone la arquitectura general del sistema, la nueva arquitectura propuesta para este proyecto y la explicación en detalle de cada uno de los módulos planteados.
    \item \textbf{Implementación del sistema}: capítulo en el que se detallan algunos aspectos de la implementación.
    \item \textbf{Análisis de rendimiento}: capítulo en el que se presentan los modelos de redes neuronales utilizados y las pruebas a realizar. Se analizan en detalle y se discuten los resultados en todos los dispositivos del proyecto.
    \item \textbf{Pruebas y resultados}: análisis de los resultados e hitos alcanzados en el proyecto y propuesta de varias líneas de investigación y desarrollo que podrían mejorar los resultados obtenidos.
    \item \textbf{Conclusiones y trabajo futuro}: capítulo en el que se arrojan las conclusiones acerca de los resultados e hitos alcanzados, así como de los errores cometidos. También se proponen varias líneas de investigación y desarrollo para extender el trabajo del presente proyecto.
\end{enumerate}