\chapter{Introducción}
\label{chap:introduccion}
\lettrine{E}{n} este capítulo se expone la motivación y los objetivos de este proyecto, así como el trabajo relacionado. Adicionalmente, se comenta la estructura por capítulos de esta memoria.% y brevemente su contenido.

\section{Motivación}
\label{sec:motivo}

En el panorama social actual, la robótica inteligente se halla cada vez más integrada en las vidas de las personas. El desarrollo de sistemas robóticos capaces de interactuar con los seres humanos constituye un aspecto esencial, en la medida en que estos sistemas deben favorecer un trato cálido y centrar la atención humana en los objetivos verdaderamente relevantes.

Con el auge del Deep Learning (DL) y de las \acrfull{cnn}, se han impulsado numerosos avances en la robótica y en aplicaciones como la visión artificial o el procesado de nubes de puntos \cite{Erik, ImageNet, MITTAL2019428}. No obstante, debido a sus elevados requerimientos computacionales, estos modelos suelen ejecutarse en entornos de computación en la nube \cite{MITTAL2019428}, lo que no resulta adecuado para aplicaciones en tiempo real debido a la latencia introducida. En este contexto, la \textbf{computación en el borde o Edge Computing} surge como una alternativa que permite procesar los datos cerca de su origen, reduciendo la latencia y distribuyendo la carga de procesamiento entre distintos dispositivos \cite{EDCOM}. Dentro de este paradigma, a su vez nace el concepto de \textbf{TinyML} \cite{OLIVEIRA2024101153}, que consiste en la implementación de modelos de redes neuronales de forma eficiente en \glspl{embsystem} como microcontroladores (\textit{Low-end} TinyML, ejemplo: Arduino) u ordenadores de placa única (\textit{High-end} TinyML, ejemplo: NVIDIA Jetson), más potentes que los microcontroladores, pero menos económicos.

Aunque las \acrshort{cnn} han demostrado excelentes resultados, sin embargo, son fuertemente dependientes del conjunto de datos de entrenamiento, lo que acaba resultando en una pérdida en la precisión a la hora de manejar datos diferentes a los del entrenamiento, es decir, no generalizan bien \cite{Erik, malisiewicz2011ensemble}. Para abordar este problema, surge el concepto de la \textbf{adaptación} (o aprendizaje incremental), que permite a los modelos aprender nuevo conocimiento durante la operación del sistema. Este enfoque permite al sistema evolucionar y extenderse a nuevos contextos %(en este caso personas)
 sin necesidad de reentrenar las redes neuronales, lo que puede ser un proceso extremadamente costoso y que, a falta de un conjunto muy amplio y representativo de datos correctamente etiquetados, puede dar resultados pobres o al menos muy sesgados.

Este proyecto propone la integración de un algoritmo de aprendizaje incremental (definido en \cite{Erik}) en un sistema de detección y reconocimiento de personas (definido en \cite{andrew}) para un robot móvil equipado con un sensor \acrshort{lidar} y múltiples cámaras \acrshort{rgbd}. Dicho algoritmo será capaz de, en primer lugar, reconocer clases (individuos) que no pertenezcan al conjunto de entrenamiento, es decir, detectar clases desconocidas (es lo que se conoce como \textit{Open-Set}). En segundo lugar, el algoritmo permitirá al sistema adaptarse para reconocer nuevas entidades en el futuro, agregando la información de estas en forma de conocimiento (también conocido como \textit{Open-World}). El sistema se optimizará para ejecutarse bajo la familia de placas NVIDIA Jetson, con la finalidad de sustituir el antiguo procesamiento realizado por medio de un ordenador central, lo que permitiría tener una unidad de procesamiento acoplada al robot, dotándole así de mayor movilidad e independencia.

\section{Objetivos}

\textcolor{red}{La principal funcionalidad de este proyecto es la detección y reconocimiento de caras en tiempo real sobre secuencias de imágenes de múltiples cámaras en un robot móvil con capacidad para adaptarse a su entorno. Por lo tanto, el primer objetivo es desarrollar [...]. El segundo objetivo es [...] nuevo método de aprendizaje [...]}

El primer objetivo de este proyecto es desarrollar un nuevo método de aprendizaje máquina para detectar individuos desconocidos (\textit{Open-Set}) e incluirlos (\textit{Open-World}) basado en las ideas mencionadas en la sección \ref{sec:motivo}. Concretamente, el sistema debe ser capaz de realizar lo siguiente:
\begin{itemize}
    \item Construir y procesar simultáneamente las secuencias de todas las caras detectadas en todas las cámaras, aplicando técnicas para el seguimiento y reidentificación de personas.
    \item Usar comités basados en clasificadores débiles (máquina de vectores de soporte o \acrshort{svm}) para identificar clases (usando votación por mayoría).
    \item Adaptar los comités (añadiendo y limitando sus \acrshort{svm}) para seguir los cambios de la entidad (\textit{\textbf{concept-drift}}) y crear una nueva identidad (comité) para cada desconocido.
    \item Aplicar la distribución de Weibull para detectar clases desconocidas (individuos).
    \item Inicializar el sistema de forma apropiada, bien de forma manual (las caras que pasan a formar parte del sistema se seleccionan minuciosamente), bien cuando el sistema detecta varias caras en el mismo instante de tiempo.
\end{itemize}

El segundo objetivo del proyecto será optimizar todo el software para funcionar de forma eficiente en la familia de sistemas embebidos NVIDIA Jetson y otros dispositivos relacionados (NVIDIA GeForce GTX y RTX). Se evaluará el rendimiento del sistema tanto de forma aislada (cada uno de sus módulos principales o con más carga computacional) como en conjunto con el sistema completo. Las pruebas se realizarán a partir de muestras (videos) de un entorno de operación real en interiores, con movimientos %bruscos 
del robot (y de las cámaras) y entrecruzado de los individuos. También se realizarán diversas pruebas de estrés para analizar el comportamiento del sistema con distintas cargas de trabajo y también para comprobar hasta qué punto escala, por ejemplo, aumentando el número de cámaras a procesar.

Para facilitar su despliegue y mantenimiento, el sistema se empaquetará en contenedores de Docker y se migrará a \acrshort{ros} 2.

\section{Trabajo relacionado}

Este trabajo persigue la ejecución eficiente de un sistema compuesto por múltiples redes neuronales convolucionales (\acrshort{cnn}s), que amplían su conocimiento a partir de la información del entorno (aprendizaje incremental). Para lograr dichos objetivos, se han tenido que explorar los siguientes campos de investigación.

\subsection{Aprendizaje incremental}
\label{sec:incrlearning}

Debido a la naturaleza cambiante de los datos en el mundo visual, preparar un \gls{dataset} que englobe todas las posibles variaciones \textbf{no sería práctico} \cite{Erik}. Adicionalmente, debido al contexto de este proyecto (detección y reconocimiento de personas en tiempo real), tampoco sería posible recoger grandes cantidades de datos debido a la ley de protección de datos, que restringe la capacidad de recolección de los mismos. Por estos motivos, se vuelve necesario adoptar un método que permita a los modelos de redes neuronales \textbf{adaptarse a los cambios} del entorno partiendo de un conjunto de muestras \textbf{muy escaso}. La perspectiva de la \textbf{adaptación (o aprendizaje incremental)} \cite{rodeo} permite a los modelos incluir dichas variaciones a lo largo del tiempo.

%Muchos sistemas que implementan el aprendizaje incremental lo hacen por medio de \glspl{batch} (\textit{batch learning}), es decir, tienen que esperar a obtener grandes conjuntos de datos antes de ejecutar la adaptación \cite{Erik, rodeo, rodeo1}. Esta actualización resulta lenta de ejecutar para cada entrada e inoperativa para aplicaciones de sistemas embebidos en tiempo real \cite{rodeo}. Por el otro lado, el sistema que se quiere implementar en este proyecto, aborda el paradigma del \textit{streaming learning} \cite{Erik} se agrega el conocimiento de forma gradual a partir de pequeñas muestras de datos (\textit{streaming learning}), que permite un aprendizaje rápido y adaptable a los cambios en tiempo real \cite{Erik, rodeo, streaming2}.

En este proyecto, se explora la implementación del algoritmo de aprendizaje incremental propuesto en \cite{Erik}, que permite al modelo poder integrar nuevo conocimiento sin necesidad de reentrenarlo desde cero. Los algoritmos desarrollados en este campo de estudio se reúnen bajo los conceptos de \textbf{\textit{Open-Set}} y \textbf{\textit{Open-World}} \cite{bendale2015towards, rudd2017extreme, Erik, CESAR}. Mientras que el primero se enfoca en distinguir nuevas clases (desconocidas) respecto a las ya conocidas del entrenamiento, el segundo permite incluir nuevo conocimiento al modelo a partir de las entradas, de forma que se logra adaptar a los múltiples factores cambiantes que entraña el mundo visual.

Los métodos de aprendizaje incremental propuestos en \cite{rudd2017extreme, Erik, CESAR} han sido probados en los \glspl{dataset} \textbf{FACE COX} \cite{cox}, \textbf{Youtube Faces} (YTF) \cite{ytf} e \textbf{ImageNet} \cite{ImageNet}, cuya adquisición de muestras se realiza en entornos controlados (buena iluminación, en las que suele aparecer un único objeto), lo que \textbf{no representa} una aplicación en un contexto \textbf{real}. Para probar la eficacia del algoritmo en condiciones reales, se emplean un conjunto de pequeños \glspl{dataset} creados en \cite{andrew}, que contienes frames borrosos, con múltiples individuos en escena entrecruzándose, caras ocluidas, entre otros factores comunes en un entorno real.

La mayoría de sistemas de reconocimiento se reducen a determinar la(s) puntuación(es) que encaja(n) en la clase más probable y dichos sistemas triunfan cuando la presunta correspondencia se vuelve cierta. Cada clase representa a una \textbf{distribución} de puntos y por medio de \textbf{puntuaciones} se valora el grado de pertenencia de la entrada a dicha distribución. Si esto se extrapola al espacio de clases conocidas, entonces la entrada producirá una puntuación \textbf{coincidente} en una clase y puntuaciones \textbf{no coincidentes} en el resto de clases. Al tomar las \textbf{mejores puntuaciones} (máximos) de cada clase y componer una distribución, entonces la \textbf{cola} de la distribución (es decir, la mejor puntuación entre las clases) es la \textbf{puntuación coincidente}. Se ha demostrado que dicha distribución de máximos siempre corresponde a una de las 3 distribuciones que siguen el \textbf{\acrfull{evt}}: \textbf{Gumbel, Frechet o Weibull} \cite{Erik,CESAR,rudd2017extreme,scorenorm,jain2014multi}.  Gumbel y Frechet son distribuciones \textbf{no acotadas}, mientras que Weibull es una distribución acotada. Para sistemas de reconocimiento que calculan las distancias o similitudes entre puntuaciones, la distribución de puntos para cada clase sigue una distribución de \textbf{Weibull}, ya que los valores se encuentran \textbf{acotados} \cite{scorenorm}.

Según el \textbf{\acrfull{evt}}, las puntuaciones que se encuentran en la cola de la distribución de máximos se denominan \textbf{extremos}, que representan casos alejados de dicha distribución. Combinando lo anterior con un \textbf{valor de umbral} fijado, se logra la capacidad de distinguir lo conocido de lo desconocido, lo que permite implementar el reconocimiento \textbf{\textit{Open-Set}} \cite{Erik,CESAR,jain2014multi}.

Es fundamental para todo algoritmo que incluya algún tipo de conocimiento establecer un balance entre añadir y actualizar las muestras ante los nuevos cambios producidos (\textit{concept-drift}) y evitar \textbf{borrar el conocimiento previo} (\textit{catastrophic forgetting}) \cite{Erik, CESAR, rodeo, streaming2, rodeo1}. El uso de \textbf{comités}, que no son más que un conjunto de clasificadores, ha demostrado ser eficaz para tratar el problema del \textbf{\textit{catastrophic forgetting}} \cite{Erik, ensembles0, ensembles3}. Los comités permiten agregar conocimiento acerca de una clase entrenando un nuevo clasificador o eliminar dicho conocimiento descartando el clasificador correspondiente. De esta forma, se obtiene una representación de la clase a partir de un conjunto de clasificadores ligeros, sin necesidad de reentrenar un clasificador complejo desde cero \cite{malisiewicz2011ensemble}.

\subsection{Seguimiento de objetos en secuencias de frames}
\label{sec:trackerwork}

El seguimiento de objetos permite aprovechar la coherencia espacio-temporal, sobre todo en el contexto de las personas, cuyos movimientos son lentos y relativamente predecibles. Los métodos de seguimiento pueden ayudar a obtener muestras difíciles de reconocer para el aprendizaje de los modelos \cite{Erik}. Otro aspecto importante es la \textbf{reidentificación}, es decir, como localizar al individuo una vez se ha perdido su rastro (ejemplo: ha sido ocluido por otro individuo o por un objeto o simplemente ha salido del campo de visión de las cámaras).

Para implementar el seguimiento y la reidentificación de objetos, se suele emplear el \textbf{método húngaro} combinado con el \textbf{filtro de Kalman} \cite{Hungarian,MangoYOLO}. El \textbf{método húngaro} \cite{Hungarian} es un algoritmo que resuelve el problema de la asignación óptima. Dada una matriz de costes en el que cada fila representa a un trabajador y cada columna representa a una tarea, dicho algoritmo buscará la \textbf{relación de menor coste} entre tarea y trabajador, de forma que cada tarea es asignada a \textbf{un solo trabajador}. Aplicado al seguimiento de objetos, el algoritmo busca la relación de menor coste entre \textbf{dos detecciones en dos frames adyacentes}. El \textbf{filtro de Kalman} \cite{Hungarian,MangoYOLO} predice la próxima posición del individuo a partir de la probabilidad Gausiana, de tal forma que puede mantenerse el rastro cuando la persona desaparece por unos frames. A pesar de ser una técnica efectiva, el filtro necesita de un mínimo de frames para converger cuando una nueva persona aparece \cite{MangoYOLO}, lo que no es beneficioso para secuencias de frames cortas.

\subsection{Aplicaciones de redes neuronales en sistemas embebidos}

A la hora de diseñar aceleradores de hardware para aplicaciones de redes neuronales es necesario mantener el balance entre precisión, rendimiento y bajo consumo. La \textbf{NVIDIA Jetson} es una familia de \textbf{sistemas embebidos} que destaca por el elevado rendimiento que ofrece por vatio gracias a sus núcleos de \acrshort{gpu} de bajo consumo. Otra virtud de la Jetson es la posibilidad de programar en \textbf{\gls{CUDA}} y en librerías como \textbf{cuDNN}, mientras que otros dispositivos, como la Raspberry Pi, requieren del uso de OpenCL, ya que no disponen de las librerías específicas para su arquitectura \cite{MITTAL2019428, rahmaniar2021real}.

En \cite{rahmaniar2021real} se utiliza la \textbf{Jetson TX2 y la Jetson Nano} para la \textbf{detección de personas} a partir de los videos de cámaras \acrshort{rgb} en resolución \textit{High Definition} (1280x720 píxeles). Los diferentes videos de prueba muestran a personas moviéndose a diferentes velocidades, coincidiendo simultáneamente en la visión de la cámara y en varias habitaciones con diferente luminosidad. Las pruebas arrojan unos remarcables \textbf{17.32 y 26.03 \acrshort{fps}} en la Jeton Nano y TX2 respectivamente en el modelo SSD MobileNet V2, lo que las hace aptas para su ejecución en \textbf{tiempo real}.

% NO HACE RECONOCIMIENTO FACIAL -> También en \cite{qi2018iot} se ha empleado la Jetson TX2 para el reconocimiento facial en videos con resolución Full HD (1080p).

%\cite{tflitework} ha logrado la ejecución en tiempo real de dos deep conventional neural networks (DCNNs) en la Raspberry Pi 4 usando TensorFlow Lite, relegando en técnicas como la \gls{quant} de rango dinámico (los valores se cuantizan a enteros de 8 bits en tiempo de ejecución) y \gls{quant} en \acrshort{fp}16.

Referente a los últimos modelos que existen de la familia Jetson y que se usarán en este proyecto, en \cite{rey2025performance} se ha logrado obtener una velocidad de \textbf{37, 40 y 44 \acrshort{fps}} en la inferencia del modelo \textbf{YOLOv8n} aplicando la \gls{quant} \acrshort{fp}32, 16 e INT8 (entero de 8 bits) respectivamente en la \textbf{Jetson Orin Nano}. YOLOv8n fue uno de los modelos escogidos en el diseño del sistema que se pretende optimizar, siendo este el principal cuello de botella, con una velocidad de tan solo \textbf{5.6 \acrshort{fps}} en una \acrshort{cpu} Intel i5 \cite{andrew}. Los \acrshort{fps} de YOLOv8n logrados en la Jetson Orin Nano demuestran la potencia de procesamiento de su \acrshort{gpu} al mismo tiempo que su reducido consumo, que no alcanza los \textbf{9 W} \cite{rey2025performance}.

\textbf{TensorRT} es una pieza clave para la optimización de modelos en aceleradoras de NVIDIA, habilitando así la ejecución de aplicaciones de inteligencia artificial en \textbf{tiempo real} \cite{MITTAL2019428, rahmaniar2021real}. Su uso por medio de modelos en formato \acrshort{onnx} ha demostrado un \textbf{speed up del doble} en la latencia en redes de clasificación de imágenes como \textbf{MobileNet y SqueezeNet} respecto a otros \glspl{rt} en \acrshort{gpu} como PyTorch, también destaca por su uso eficiente y escalable de la memoria de la \acrshort{gpu} \cite{trtworkflows}. Otra virtud de TensorRT es su capacidad de \textbf{realizar inferencias en diferentes precisiones}, desde \acrshort{fp}32 hasta \textbf{INT8} (entero de 8 bits), hasta el reciente \textbf{\acrshort{fp}4} en la arquitectura de \acrshort{gpu} \textbf{Blackwell} \cite{fp4}. En \cite{xu2018deep} se compara el rendimiento de la inferencia en \acrshort{fp}32 e INT8 del modelo \textbf{ResNet50}, cuyos resultados arrojan un \textbf{speed up del 3.7} en INT8 respecto a \acrshort{fp}32 (siendo el máximo teórico de 4), con una pérdida mínima (0.02\% - 0.18\%) en la precisión del modelo.

\section{Estructura de la memoria}

Esta memoria se divide en capítulos, cada uno con una explicación detallada del trabajo realizado. Se expone a continuación la estructura de esta memoria, explicando brevemente su contenido:

\begin{enumerate}
    \item \textbf{Introducción}: presente capítulo en el que se exponen los motivos de realización de este proyecto, los objetivos planteados, el trabajo relacionado y la estructura de la memoria resultante.
    \item \textbf{Fundamentos teóricos}: capítulo en el que se presentan los conceptos que se referenciarán a lo largo de la memoria.
    \item \textbf{Fundamentos tecnológicos}: capítulo que presenta los dispositivos sujetos de pruebas de este proyecto, así como las arquitecturas de las \acrshort{gpu}s que incorporan. También se exponen las herramientas hardware y software utilizadas.
          %\item \textbf{Gestión del proyecto}: en este capítulo se presentan los requisitos, actores y casos de uso que definen el sistema propuesto, el plan de gestión de los riesgos, la metodología de desarrollo empleada y una vista detallada de la planificación del proyecto y su posterior seguimiento.
    \item \textbf{Arquitectura del sistema propuesto}: capítulo en el que se expone la arquitectura general del sistema, la nueva arquitectura propuesta para este proyecto y la explicación en detalle de cada uno de los módulos planteados.
    \item \textbf{Implementación del sistema}: capítulo en el que se detallan algunos aspectos de la implementación de los componentes del nuevo sistema.
    \item \textbf{Análisis de rendimiento}: capítulo en el que se presentan los modelos de redes neuronales utilizados y las pruebas realizadas. Se analizan en detalle y se discuten los resultados en todos los dispositivos del proyecto.
    \item \textbf{Pruebas de escalabilidad del sistema}: capítulo en el que se mide la capacidad del sistema de escalar en varios de los dispositivos del proyecto.
    \item \textbf{Pruebas y resultados del sistema extendido}: capítulo se analiza el rendimiento de los nuevos componentes implementados a partir de múltiples pruebas.
    \item \textbf{Conclusiones y trabajo futuro}: último capítulo en el que se arrojan las conclusiones acerca de los resultados e hitos alcanzados, así como de los errores cometidos. También se proponen varias líneas de investigación y desarrollo para extender el trabajo del presente proyecto.
\end{enumerate}

\textcolor{red}{Comentar que, en aras de la simplicidad, se ha puesto en los apéndices aquella información que puede ser de utilidad pero no es crítica para entender el grueso del trabajo realizado. }