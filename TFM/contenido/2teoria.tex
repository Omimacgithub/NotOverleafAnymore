\chapter{Fundamentos teóricos}
\lettrine{E}{n} este capítulo se exponen los conceptos de clasificador y aprendizaje incremental, necesarios para comprender el resto de la memoria, junto a sus tipos.

\section{Clasificadores}

Los clasificadores o algoritmos de clasificación \cite{DREISEITL2002352} resuelven la tarea de decidir la membresía de un nuevo dato a una clase a partir de un \gls{dataset}. Aparte de las extendidas \textbf{redes neuronales artificiales} (ANN), se suelen emplear los siguientes algoritmos en el campo de la detección de objetos y en otros problemas de clasificación:

\begin{description}
    \item [\textit{k-nearest neighbors (knn)}] \cite{peterson2009k} Es un método basado en las distancias de la muestra respecto a sus muestras más cercanas (\textbf{vecinos}). El parámetro \textbf{k} establece la cantidad de dichos vecinos con los que relacionarse, de forma que se asigna la clase predominante de entre todos los vecinos de la propia muestra.
    \item [\textit{Decision tree}] \cite{ali2012random} Dicho método obtiene las características (\textit{features}) que mejor dividen el conjunto de datos del \gls{dataset} (las características con mayor \textbf{\textit{information gain}}), este proceso se repite de forma recursiva para cada subconjunto, de forma que se construye un \textbf{árbol binario} de decisiones. La clasificación empieza desde el \textbf{nodo raíz} y termina en uno de los \textbf{nodos hoja}, que representa a un conjunto de datos pertenecientes a una misma clase.
          %, en el que un nodo \textbf{no hoja} (\textit{decision node}) representa una característica compartida entre clases y un nodo \textbf{hoja} una característica específica de una clase, de forma que una rama representa el conjunto de características de una clase
    \item [\textit{\acrfull{svm}}] \cite{svn,DREISEITL2002352} Trata de dibujar el hiperplano óptimo en cuanto al margen que separa las clases de datos. A diferencia de los anteriores clasificadores, las predicciones devueltas por las \acrshort{svm} son puramente \textbf{dicotómicas}, es decir, no devuelve probabilidades de pertenencia para cada clase (al menos de forma nativa).
\end{description}

En este proyecto, se ha optado por el \textbf{entrenamiento discriminativo}, que consiste en resaltar las diferencias entre las muestras de la clase que se representa (positivos) respecto al resto del mundo (negativos). Debido a que las muestras negativas se pueden compartir entre diferentes clasificadores, se obtiene una gran capacidad de generalización partiendo de un conjunto reducido de datos \cite{malisiewicz2011ensemble}. Los clasificadores como \textit{knn, decision tree} o las redes neuronales artificiales no poseen la capacidad de aprender de los datos negativos y, en cambio, necesitan grandes \glspl{dataset} para converger correctamente \cite{malisiewicz2011ensemble}. Las \acrshort{svm} sí que cuentan con dicha capacidad y, por este motivo, son relevantes para el presente proyecto.

\subsection{\acrfull{svm}}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.5\linewidth]{imagenes/LINSVM.png}
    \caption{Clasificación en espacio \acrshort{2d} mediante el uso de las \acrshort{svm}, figura extraída de \cite{svn}.}
    \label{fig:linsvm}
\end{figure}

Las \textbf{Máquinas de Vectores de Soporte} (o \acrshort{svm}) \cite{svn,DREISEITL2002352} trazan un hiperplano entre categorías de datos, buscando siempre el mayor margen (distancia entre los puntos que definen las fronteras de las categorías, denominados \textit{support vectors}), de modo que agrega cierta tolerancia al posible ruido producido. En la figura \ref{fig:linsvm} se muestra un ejemplo de clasificación en el espacio \acrshort{2d}, los cuadrados grises representan los \textit{support vectors}, que definen el margen máximo de separación entre clases.

Las \acrshort{svm} requieren de un conjunto inicial de datos para su entrenamiento (aprendizaje supervisado) y no son capaces de discernir entre clases fuera del conjunto de datos de entrenamiento (\textit{Closed-Set}), por lo que un dato desconocido se clasificaría erróneamente como una de las clases del entrenamiento \cite{rudd2017extreme}.

\subsection{Comités}

Los métodos basados en comités (o \textit{ensembles}) se han explorado como alternativa para \textbf{reducir} los \textbf{errores en la generalización} de los clasificadores \cite{zhang2002neural}. También se han estudiado como solución para \textbf{evitar el borrado del conocimiento previo} de los modelos (\textit{catastrophic forgetting}) \cite{Erik, ensembles0, ensembles3}, dentro del campo del aprendizaje incremental.

Dichos métodos se basan en un sistema de votación en el que un conjunto de clasificadores participan, lo que permite tolerar los fallos de un clasificador en particular \cite{zhang2002neural}. En el caso de este proyecto, el comité estaría formado por un \textbf{conjunto de \acrshort{svm}s}. Se ha demostrado que múltiples \acrshort{svm} simples (ejemplo: lineales) como conjunto \textbf{generalizan mejor} que una única \acrshort{svm} compleja (ejemplo: sigmoide) \cite{malisiewicz2011ensemble}.

Un ejemplo de un sistema formado por comités de clasificadores es el \textbf{\textit{Random Forest}}. Este método construye un conjunto de \textbf{decision trees} entrenados a partir de una selección aleatoria de las muestras. Las predicciones de cada árbol se \textbf{fusionan} para obtener un único resultado mediante la \textbf{votación por mayoría} \cite{ali2012random}. Dicho método de combinación determina la clase final a partir del conteo de la salida de cada clasificador, que representa un voto para una clase determinada, la clase con más votos entre los clasificadores es finalmente escogida \cite{ponti2011combining}. En el contexto de un comité de \acrshort{svm}s, se puede implementar la votación por mayoría a través de la \textbf{mediana}, ya que las \acrshort{svm} devuelven salidas numéricas o \textbf{puntuaciones} \cite{Erik}.

\section{Aprendizaje incremental}
\label{sec:incrtypes}

Como ya se ha introducido en la sección \ref{sec:incrlearning}, el aprendizaje incremental \cite{rodeo} permite añadir nuevo conocimiento al sistema durante su operación o fase de inferencia (\textit{online training}). Esta aproximación permite al sistema adaptarse a nuevos datos que no corresponden a los etiquetados en los \glspl{dataset} utilizados durante su fase de entrenamiento (\textit{offline training}). Dentro de este campo se encuentran los siguientes tipos de configuración:
\begin{itemize}
    \item \textbf{No supervisado} \cite{sharma2012unsupervised}: el sistema se entrena con datos obtenidos durante su operación (muestras o datos \textit{online}), es decir, en \textbf{completa ausencia de datos etiquetados}. El sistema, de forma autónoma, recolecta los datos de los casos de prueba utilizando \textbf{métodos de seguimiento} de los objetos. Este tipo de aprendizaje debe ser capaz de manejar el \textbf{ruido inevitable} de las muestras \textit{online}.
    \item \textbf{Semisupervisado} \cite{zhu2005semi}: el sistema parte de un conjunto \textbf{reducido} de datos etiquetados, que se expanden y modifican posteriormente con datos \textit{online}. A partir de los datos etiquetados se logra suprimir fácilmente el ruido en las muestras iniciales, que puede ser un factor crítico en el rendimiento del sistema \cite{andrew}.
\end{itemize}

No se menciona un tipo supervisado, ya que esto supondría no utilizar datos \textit{online} para el aprendizaje, lo que contradice el principio principal del aprendizaje incremental.

Dentro del aprendizaje incremental, los conceptos \textbf{\textit{Closed-Set}, \textit{Open-Set} y \textit{Open-World}} \cite{bendale2015towards, rudd2017extreme, Erik, CESAR} han surgido para etiquetar a los sistemas según el tipo de reconocimiento que implementan. A continuación, se explican los detalles de cada uno.

\begin{description}
    \item[\textit{Closed-Set}] \cite{bendale2015towards} Opera en un conjunto \textbf{cerrado} de objetos y no implementa ningún tipo de aprendizaje en absoluto. Se entrena puramente con datos etiquetados y está limitado a reconocer las clases con las que se inicializó (clases de entrenamiento). Este es el modo en el que trabaja el sistema base \cite{andrew}. Asumiendo que el sistema no va a reconocer nuevas clases, se estrecha enormemente el universo de clases que este conoce, lo que permite implementar métodos de clasificación basados en la clase más probable.
    \item[\textit{Open-Set}] \cite{rudd2017extreme} Extiende al modo \textit{Closed-Set}, de forma que puede reconocer nuevas clases nunca vistas en el entrenamiento, o clases \textbf{desconocidas}. Al ampliar el universo de categorías a la categoría desconocida, deja de ser válida la clasificación por la clase más probable. El reconocimiento de nuevas clases sigue siendo un tópico activo en la investigación, donde se apuesta fuertemente por el teorema \acrfull{evt} combinado con un valor de umbral.
    \item[\textit{Open-World}] Este modo a su vez extiende al modo \textit{Open-Set}. Atendiendo a la definición de \cite{rudd2017extreme}, un sistema de reconocimiento \textit{Open-World} debe de ser capaz de realizar las siguientes 4 tareas:
        \begin{itemize}
            \item Detectar desconocidos (\textbf{\textit{Open-Set}}): identificar cuando una muestra de entrada no pertenece al conjunto de datos del entrenamiento.
            \item Escoger las muestras que puedan aportar información del desconocido al sistema.
            \item Etiquetar dichas muestras, por ejemplo, con un número o un pseudónimo.
            \item Actualizar el sistema.
        \end{itemize}
\end{description}

En un principio puede resultar natural aplicar los algoritmos de reconocimiento válidos en \textit{Open-Set} al modo \textit{Open-World}. Sin embargo, no todos los algoritmos son adecuados para las actualizaciones incrementales o la escalabilidad en la cantidad de clases \cite{bendale2015towards}.