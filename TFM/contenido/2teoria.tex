\chapter{Fundamentos teóricos}
\lettrine{E}{n} este capítulo se exponen los conceptos necesarios para comprender la memoria.

\section{Tipos de aprendizaje máquina semisupervisado}
El aprendizaje máquina semisupervisado parte de una fase inicial de los modelos entrenados con datos perfectamente \textbf{etiquetados}, dichos datos se complementan o se actualizan con muestras obtenidas durante la operación del sistema (no etiquetadas), de esta forma el modelo amplia su conocimiento de manera autónoma. Existen 3 tipos diferentes de aprendizaje máquina semisupervisado que se aplicarán a lo largo de esta memoria y que son los siguientes:

\subsection{\textit{Closed-Set}}
Tipo de aprendizaje que opera en un conjunto \textbf{cerrado} de objetos, siendo en este caso personas registradas de antemano. Es el modo en el que trabaja el sistema base \cite{andrew}.

\subsection{\textit{Open-Set}}
Extiende al modo \textit{Closed-Set}, de forma que puede reconocer a usuarios que no pertenezcan al conjunto de entrenamiento, es decir, a usuarios \textbf{desconocidos}.

\subsection{\textit{Open-World}}

Atendiendo a la definición de \cite{rudd2017extreme}, un sistema de reconocimiento \textit{Open-World} debe de ser capaz de realizar las siguientes 4 tareas:
\begin{itemize}
    \item Detectar desconocidos (\textbf{\textit{Open-Set}}): identificar cuando una muestra de entrada no pertenece al conjunto de datos del entrenamiento.
    \item Escoger las muestras que puedan aportar información del desconocido al modelo.
    \item Etiquetar dichas muestras, por ejemplo, con un número o un pseudónimo.
    \item Actualizar el modelo.
\end{itemize}

\section{Clasificadores}

Se suelen mencionar los siguientes algoritmos en el campo de la detección de objetos y en otros problemas de clasificación \cite{DREISEITL2002352}:

\begin{itemize}
    \item \textbf{\textit{k-Nearest neighbors}}
    \item \textbf{\textit{Decision tree}}: es un método de clasificación que sigue una estructura de árbol, en el que un nodo representa una característica y una rama un conjunto de características. La clasificación empieza desde el nodo raíz y termina en uno de los nodos hoja \cite{ali2012random}.
    \item \textbf{\textit{Neural Networks}}: Learning rate, batch size, network architecture, regularization strength
    \item \textbf{\textit{Support Vector Machines}}:
\end{itemize}

En este proyecto, se ha optado por el entrenamiento discriminativo, que consiste en resaltar las diferencias entre las muestras de la clase que se representa (positivos) respecto al resto del mundo (negativos). Debido a que las muestras negativas se pueden compartir entre diferentes clasificadores, se obtiene una gran capacidad de generalización partiendo de un conjunto reducido de datos.

Los clasificadores como KNN o las redes neuronales no poseen la capacidad de aprender de los datos negativos y, en cambio, necesitan grandes \glspl{dataset} para converger correctamente \cite{malisiewicz2011ensemble}. Las \acrshort{svm} sí que cuentan con dicho poder discriminatorio y, por este motivo, son relevantes para el presente proyecto.

\subsection{Support Vector Machine (SVM)}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.5\linewidth]{imagenes/LINSVM.png}
    \caption{Clasificación en espacio \acrshort{2d} mediante el uso de las \acrshort{svm}, figura extraída de \cite{svn}.}
    \label{fig:linsvm}
\end{figure}

Es un algoritmo de \textbf{clasificación} que dibuja un hiperplano entre categorías de datos, buscando siempre el mayor margen (distancia entre los puntos que definen las fronteras de las categorías, denominados \textit{support vectors}), de modo que agrega cierta tolerancia al posible ruido producido. En la figura \ref{fig:linsvm} se muestra un ejemplo de clasificación en el espacio \acrshort{2d}, los cuadrados grises representan los \textit{support vectors}, que definen el margen máximo de separación entre clases.

Las \acrshort{svm} requieren de un conjunto inicial de datos para su entrenamiento (aprendizaje supervisado) y no son capaces de discernir entre clases fuera del conjunto de datos de entrenamiento (\textit{Closed-Set}), por lo que un dato desconocido se clasificaría erróneamente como una de las clases del entrenamiento \cite{rudd2017extreme}.

\subsection{Comités}

Los métodos basados en comités (o \textit{ensembles}) se han explorado como alternativa para reducir los errores en la generalización de los clasificadores \cite{zhang2002neural}. También se han estudiado como solución para evitar el borrado del conocimiento previo de los modelos (\textit{catastrophic forgetting}) \cite{Erik, ensembles0, ensembles3}, dentro del campo del aprendizaje incremental.

Dichos métodos se basan en un sistema de votación en el que un conjunto de clasificadores participan, lo que permite tolerar los fallos de un clasificador en particular \cite{zhang2002neural}. En el caso de este proyecto, el comité estaría formado por un conjunto de \acrshort{svm}s. Se ha demostrado que múltiples \acrshort{svm} simples (ejemplo: lineales) como conjunto \textbf{generalizan mejor} que una única \acrshort{svm} compleja (ejemplo: sigmoide) \cite{malisiewicz2011ensemble}.

A continuación se exponen algunos de los tipos más conocidos:

\begin{itemize}
    \item \textbf{\textit{Gradient Boosting}}: Learning rate, number of boosting rounds, maximum tree depth
    \item \textbf{\textit{Random Forest}}: es un conjunto de árboles de clasificación creados a partir de muestras escogidas aleatoriamente del conjunto de entrenamiento. Las predicciones se realizan mediante la votación por mayoría \cite{ali2012random}.
    \item \textbf{\textit{Median Fusion}}:
\end{itemize}