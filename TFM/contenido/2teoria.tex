\chapter{Fundamentos teóricos}
\lettrine{E}{n} este capítulo se exponen los conceptos de clasificador y aprendizaje incremental, necesarios para comprender el resto de la memoria, junto a sus tipos.

\section{Clasificadores}
\label{sec:clasf}

Los clasificadores o algoritmos de clasificación \cite{DREISEITL2002352} resuelven la tarea de decidir la membresía de un dato a una clase por medio de \textbf{puntuaciones} o \textbf{probabilidades}. La probabilidad de pertenencia a una clase se puede definir como $P(y|x)$ \cite{DREISEITL2002352}, donde \textit{y} representa a una de las clases (o categorías) conocidas del \gls{dataset} y \textit{x} representa un nuevo dato (o muestra) del que se quiere determinar su clase. Aparte de las extendidas \textbf{redes neuronales artificiales} (ANN), se suelen emplear los siguientes algoritmos de \textbf{aprendizaje supervisado} en el campo de la detección de objetos y en otros problemas de clasificación:

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.5\linewidth]{imagenes/LINSVM.jpg}
    \caption{Clasificación en espacio \acrshort{2d} mediante el uso de las \acrshort{svm}, figura extraída de \cite{svn}.}
    \label{fig:linsvm}
\end{figure}

\begin{description}
    \item [\textit{k-nearest neighbors (knn)}] \cite{peterson2009k} Es un método basado en las distancias de la muestra respecto a sus muestras más cercanas (\textbf{vecinos}). El parámetro \textbf{\textit{k}} establece la cantidad de dichos vecinos con los que relacionarse. La probabilidad de pertenencia a una clase ($P(y|x)$) se calcula como el ratio de miembros de la clase \textit{y} entre los \textit{k} vecinos más cercanos de \textit{x}.
    \item [\textit{Decision tree}] \cite{ali2012random, DREISEITL2002352} Dicho método obtiene las características (\textit{features}) que mejor dividen el conjunto de datos del \gls{dataset} (las características con mayor \textbf{\textit{information gain}}), este proceso se repite de forma recursiva para cada subconjunto, de forma que se construye un \textbf{árbol binario} de decisiones. La clasificación empieza desde el \textbf{nodo raíz} y termina en uno de los \textbf{nodos hoja}, que es un conjunto de datos que contiene varias categorías (se denomina \textbf{nodo puro} al nodo hoja con un conjunto de datos perteneciente a una única clase, por lo que no sería necesario ampliar la profundidad de la rama). La probabilidad de pertenencia a una categoría ($P(y|x)$) se calcula como el ratio de miembros de la clase \textit{y} de entre todos los elementos del nodo hoja que contiene la muestra \textit{x}.
    \item [\textit{\acrfull{svm}}] \cite{svn,DREISEITL2002352} Trata de dibujar el hiperplano óptimo en cuanto al margen que separa las clases de datos, es decir, la distancia entre los puntos que definen las fronteras de las clases, denominados \textit{support vectors}, de modo que se agrega cierta tolerancia al posible ruido contenido en las muestras. En la figura \ref{fig:linsvm} se muestra un ejemplo de clasificación en el espacio \acrshort{2d}, los cuadrados grises representan los \textit{support vectors}, que definen el margen máximo de separación entre clases. A diferencia de los anteriores métodos, las predicciones devueltas por las \acrshort{svm} son puramente \textbf{dicotómicas}, es decir, no devuelve probabilidades de pertenencia para cada clase (al menos de forma nativa). En su lugar, se devuelven \textbf{puntuaciones} otorgadas a los datos de entrada, cuyo valor representa el grado de confianza en términos de la distancia de la muestra respecto al hiperplano, mientras que el \textbf{signo} representa a \textbf{una de las dos clases} que el hiperplano separa. A diferencia de las probabilidades, dichos valores \textbf{no se encuentran limitados} y \textbf{no son directamente comparables} entre otras clases.
\end{description}

En este proyecto, se ha optado por el \textbf{entrenamiento discriminativo}, que consiste en resaltar las diferencias entre las muestras de la clase que se representa (positivos) respecto al resto del mundo (negativos). Debido a que las muestras negativas se pueden compartir entre diferentes clasificadores, se obtiene una \textbf{gran capacidad de generalización} partiendo de un \textbf{conjunto reducido de datos} a diferencia de un esquema tipo \textit{nearest-neighbor} \cite{malisiewicz2011ensemble}. Las \acrshort{svm} \textbf{cuentan con dicha capacidad discriminativa} y, por este motivo, son relevantes para el presente proyecto.

\subsection{Comités}

Los métodos basados en comités (o \textit{ensembles}) \cite{zhang2002neural} se han explorado como alternativa para \textbf{reducir} los \textbf{errores en la generalización} de los clasificadores. También se han estudiado como solución para \textbf{evitar el borrado del conocimiento previo} de los modelos (o \textit{catastrophic forgetting}) \cite{Erik, CESAR, ensembles0, ensembles3}, dentro del campo del aprendizaje incremental.

Dichos métodos se basan en un sistema de votación en el que un conjunto de clasificadores participan, lo que permite tolerar los fallos de un clasificador en particular \cite{zhang2002neural}. En el caso de este proyecto, el comité estaría formado por un \textbf{conjunto de \acrshort{svm}s}. Se ha demostrado que múltiples \acrshort{svm} simples (ejemplo: lineales) como conjunto \textbf{generalizan mejor} que una única \acrshort{svm} compleja (ejemplo: sigmoide) \cite{malisiewicz2011ensemble}.

Un ejemplo de un sistema formado por comités de clasificadores es el \textbf{\textit{Random Forest}} \cite{ali2012random}. Este método construye un conjunto de \textbf{decision trees} entrenados a partir de una selección aleatoria de las muestras. Las predicciones de cada árbol se \textbf{fusionan} para obtener un único resultado mediante la \textbf{votación por mayoría} \cite{ponti2011combining}. Dicho método de combinación determina la predicción final a partir del conteo de las salidas de cada clasificador, que representa un voto para una clase determinada, la clase con más votos entre los clasificadores es finalmente escogida. En el contexto de un comité de \acrshort{svm}s, se puede implementar la votación por mayoría a través de la \textbf{mediana} \cite{Erik}, ya que las \acrshort{svm} devuelven salidas numéricas o \textbf{puntuaciones}.

\section{Aprendizaje incremental}
\label{sec:incrtypes}

Como ya se ha introducido en la sección \ref{sec:incrlearning}, el aprendizaje incremental \cite{rodeo} permite añadir nuevo conocimiento al sistema durante su operación (\textit{online training}), en sustitución del entrenamiento con datos etiquetados (\textit{offline training}), lo que permite al sistema adaptarse a los cambios en la distribución inicial de dichos datos. Sin embargo, es importante manejar el \textbf{ruido} \cite{sharma2012unsupervised} de las muestras \textit{online} (ejemplo: error en el seguimiento de un objeto), que presentan \textbf{desalineaciones} respecto a las muestras de la clase a la que pertenecen. Dentro de este campo se encuentran los siguientes tipos de aprendizaje:
\begin{description}
    \item[No supervisado] \cite{sharma2012unsupervised} El sistema se entrena \textbf{únicamente} con datos obtenidos durante su operación (muestras o datos \textit{online}), es decir, en \textbf{completa ausencia de datos etiquetados}. El sistema, de forma autónoma, recolecta los datos de los casos de prueba utilizando \textbf{métodos de seguimiento} de objetos.
    \item[Semisupervisado] \cite{zhu2005semi} El sistema parte de un conjunto \textbf{reducido} de datos etiquetados, que se expanden y modifican posteriormente con datos \textit{online}. A partir de los datos etiquetados se logra \textbf{suprimir fácilmente el ruido} en las muestras iniciales, que puede ser un factor crítico en el rendimiento del sistema \cite{andrew}.
\end{description}

No se menciona un tipo supervisado, ya que esto supondría no utilizar datos \textit{online} para el aprendizaje, lo que contradice el principio principal del aprendizaje incremental.

Dentro del aprendizaje incremental, los conceptos \textbf{\textit{Closed-Set}, \textit{Open-Set} y \textit{Open-World}} \cite{bendale2015towards, rudd2017extreme, Erik, CESAR} han surgido para nombrar a los sistemas según el tipo de reconocimiento que implementan. A continuación se explican los detalles de cada uno.

\begin{description}
    \item[\textit{Closed-Set}] \cite{bendale2015towards} Opera en un conjunto \textbf{cerrado} de categorías y no implementa ningún tipo de aprendizaje incremental en absoluto. Se entrena puramente con datos etiquetados y está limitado a reconocer las clases con las que se inicializó (clases de entrenamiento). Este es el modo en el que trabaja el sistema base \cite{andrew}. Asumiendo que el sistema no va a reconocer nuevas clases, se estrecha enormemente el universo de categorías existentes, lo que permite implementar métodos de clasificación basados en la clase \textbf{más probable}, como los ya vistos en la sección \ref{sec:clasf} y que pertenecen a este tipo de reconocimiento.
    \item[\textit{Open-Set}] \cite{rudd2017extreme} Extiende al modo \textit{Closed-Set}, de forma que puede reconocer nuevas clases nunca vistas en el entrenamiento o clases \textbf{desconocidas}. Al ampliar el universo de categorías a la categoría desconocida, deja de ser válida la clasificación por la clase más probable, por lo que deben de integrarse clasificadores \textbf{discriminativos} (ejemplo: \acrshort{svm}s) \cite{scheirer2012toward}.
    \item[\textit{Open-World}] Este modo a su vez extiende al modo \textit{Open-Set}. Atendiendo a la definición de \cite{rudd2017extreme}, un sistema de reconocimiento \textit{Open-World} debe de ser capaz de realizar las siguientes 4 tareas:
        \begin{itemize}
            \item Detectar desconocidos (\textbf{\textit{Open-Set}}): identificar cuando una muestra de entrada no pertenece al conjunto de datos del entrenamiento.
            \item Escoger las muestras que puedan aportar información del desconocido al sistema.
            \item Etiquetar dichas muestras, por ejemplo, con un número o un pseudónimo.
            \item Actualizar el sistema.
        \end{itemize}
\end{description}

En un principio puede resultar natural aplicar los algoritmos de reconocimiento válidos en \textit{Open-Set} al modo \textit{Open-World}. Sin embargo, no todos los algoritmos son adecuados para las actualizaciones incrementales o la escalabilidad en la cantidad de clases \cite{bendale2015towards}.