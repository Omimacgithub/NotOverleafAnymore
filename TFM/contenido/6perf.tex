\chapter{Análisis de rendimiento}
\label{chap:cnn}

\lettrine{E}{n} este capítulo se comentan los modelos de redes neuronales utilizados para las pruebas y se expone el procedimiento seguido para su realización, junto a una discusión detallada acerca de los resultados de rendimiento en los diferentes dispositivos del proyecto.

\section{Modelos de redes neuronales utilizados}
\label{sec:models}

A continuación se exponen las \acrshort{cnn} sujetas de las pruebas de este capítulo y que conforman el sistema. Dichos modelos también se emplearon en \cite{andrew} para la implementación del nodo cámara.

\begin{description}
    \item[YuNet] \cite{wu2023yunet} Es un modelo de \textbf{detección facial} diseñado para sistemas con recursos muy limitados, con tan solo \textbf{75856 parámetros} . Debido a los múltiples \textit{outputs} del modelo, complejos de procesar, se ha optado por utilizar la versión de OpenCV, que simplifica todo el proceso de pre y postprocesado. El modelo en formato \acrshort{onnx} se encuentra en este enlace \footnote{https://github.com/opencv/opencv\_zoo/blob/main/models/face\_detection\_yunet/face\_detection\_yunet\_2023mar.onnx}.
    \item[ArcFace] \textit{Additive Angular Margin Loss} o ArcFace \cite{deng2019arcface} es una \textbf{función de pérdida} (\gls{lossfunc}) que persigue mejorar el poder discriminativo de los modelos de \textbf{reconocimiento facial}, es decir, maximizar el margen que separa a las clases y minimizar el margen de los datos que pertenecen a una misma clase. Los \glspl{embedding} con los que opera ArcFace pertenecen al modelo ResNet-100. Se emplea el archivo de pesos de una implementación en TensorFlow Lite, accesible mediante el siguiente enlace \footnote{https://www.digidow.eu/f/datasets/arcface-tensorflowlite/model.tflite}.
    \item[YOLO] \textit{You Only Look Once} (YOLO) \cite{redmon2016you, mdpi} es una red de \textbf{detección de objetos} centrada en otorgar la mayor velocidad de procesamiento en tiempo real sin sacrificar la precisión. Existen múltiples versiones de YOLO, en \cite{andrew} se probaron las versiones 3, 5 y 8 en el tamaño nano, más ligero y veloz, pero al mismo tiempo más impreciso. En este proyecto a mayores se ha probado la versión 11 (anterior a la reciente versión 26), en su tamaño nano. El modelo se puede obtener mediante la \acrshort{api} de Python de ultralytics, que permite exportarlo con múltiples ajustes (ver apéndice \ref{sec:exports}).
    \item[OSNet] \textit{Omni-Scale Network} (OSNet) \cite{zhou2019omni} es un modelo de \textbf{reidentificación de personas} que emplea características de diferentes tamaños (\textit{omni-scale features}) para la clasificación (ejemplo: el cuerpo de una persona o el logo de su camiseta). Destaca por ser extremadamente ligero y por su elevada precisión. Se realizó una exportación a \acrshort{onnx} a partir de la implementación de PyTorch de \cite{andrew}.
\end{description}

\section{Realización de las pruebas}
\label{sec:probes}

A modo de evaluar el rendimiento en términos de precisión de los modelos, se parte de un \gls{dataset} (en el apéndice \ref{sec:datcam} se expone su composición) compuesto de la información extraída de un fichero \gls{rosbag}, que contiene los datos generados por las cámaras Kinect \acrshort{rgbd} y el sensor \acrshort{lidar} del Summit\_XL durante la realización de una prueba en el laboratorio de robótica del \acrshort{citic}, en el que 6 personas caminan alrededor del robot y se entrecruzan durante 105 segundos (aproximadamente). El robot también se desplaza en su propio eje, generando así imágenes borrosas y cambios impredecibles en la posición de los individuos en escena.

Durante la ejecución de una prueba, la red neuronal procesa los frames \acrshort{rgb} de los 2 vídeos (1 por cámara) del \gls{dataset}, de forma que al momento de terminar el procesamiento de un frame, el modelo puede continuar inmediatamente con el siguiente. La salida devuelta por la \acrshort{cnn} se contrasta con la información del \textit{\gls{dataset}} de la respectiva cámara, que devuelve un resultado final de precisión al finalizar la prueba. En cuanto a la latencia, se mide el tiempo de la inferencia, en el caso de los modelos de detección, y de la inferencia más el reconocimiento en el caso de los modelos de reconocimiento. En ambas situaciones, se emplea la función \textbf{time} del módulo \textbf{time} de Python para obtener los tiempos en cada frame. Finalmente, todas las latencias se fusionan a partir de la media.

%En las pruebas de los modelos de detección, se obtienen los frames directamente de los vídeos. 
Para los modelos de detección, el test determina una detección como coincidente si los porcentajes de intersección entre la \gls{bbox} obtenida respecto a la de referencia y viceversa superan o igualan un \textbf{umbral}. En este caso, dicho umbral se ha fijado en el \textbf{50\% de intersección} \cite{andrew}. Por el lado de los modelos de reconocimiento, se determina un reconocimiento como correcto si la predicción obtenida es la misma que la etiqueta (\textit{\gls{gt}}) recogida en el \gls{dataset}. Es importante mencionar que los modelos de reconocimiento no utilizan directamente los vídeos, sino que parten de una lista de frames \textbf{ya recortados} con las detecciones, simulando así una entrada otorgada por un modelo de detección.

Como ya se ha comentado en la sección \ref{sec:basearch}, existen varios métodos para escoger la identidad ganadora dentro de un conjunto de identidades probables, por simplicidad, siempre se escoge la primera predicción de la lista como la ganadora.

Para la ejecución de inferencias, se han empleado los \glspl{rt} de OpenVINO, TensorRT y OpenCV, este último para el caso concreto de la red YuNet. OpenVINO se ha utilizado para ejecutar las inferencias \textbf{puramente en \acrshort{cpu}}, mientras que TensorRT utiliza la \textbf{\acrshort{gpu}}. OpenCV permite realizar las inferencias \textbf{únicamente en \acrshort{cpu}} o por medio de \gls{CUDA}, que aplica la \acrshort{gpu}. Se han configurado los modelos para la inferencia en \textbf{precisión mixta \acrshort{fp}32 y \acrshort{fp}16} (y TensorFloat 32 en el caso de arquitecturas Ampere y superiores) en dichos \glspl{rt}, es decir, se ejecutan las capas de un modelo en la precisión que menor latencia reporte dentro de una lista de precisiones (ejemplo: \acrshort{fp}16 o \acrshort{fp}32) \cite{mixed}. Esta funcionalidad puede emplearse siempre que la arquitectura de los dispositivos la implemente.

\section{Resultados}
\label{sec:modelic}

\input{contenido/tables/tests/tabequips}

En la tabla \ref{tab:equips} se muestran los resultados de \textbf{latencia (en milisegundos)} y de la métrica \textbf{\textit{F1\_score}} para los modelos de detección y \textbf{\textit{precision}} para los modelos de reconocimiento (ver métricas en el apéndice \ref{sec:modeleval}). Cada resultado refleja la media y la desviación típica de \textbf{5 ejecuciones} sin contar la ejecución previa del test a modo de calentamiento para cada modelo.

En el caso del modelo YOLO, los 2 equipos de sobremesa y la Jetson AGX Thor se quedan por debajo de los 30 milisegundos en la versión de OpenVINO, mientras que la latencia en el resto de dispositivos es significativamente mayor. Parte de la diferencia en la latencia viene del \textbf{costoso pre y postprocesado de YOLO} (ver optimización del postprocesado en el apéndice \ref{sec:postyolo}), que se realiza en la \acrshort{cpu}, donde los equipos de sobremesa destacan. Entre el portátil y el PC del laboratorio hay \textbf{8 milisegundos de diferencia} a favor del último, ya que los modelos de \acrshort{cpu}s para portátiles pretenden ser eficientes energéticamente, por lo que se distribuyen con una menor potencia respecto a las \acrshort{cpu}s de sobremesa, que pueden hacer \textit{overclock} a mayor potencia sin importar el consumo, logrando así acelerar la ejecución de las operaciones. %siendo ambas \acrshort{cpu}s de la misma generación (Intel Core i7 12th).

Al trasladar la carga de la inferencia de YOLO a la \acrshort{gpu} cuando se utiliza TensorRT, el tiempo de inferencia es hasta \textbf{7 veces} menor en el caso de la Jetson AGX Thor y \textbf{5 veces} menor en el caso de la Jetson Xavier NX. Las \textbf{Jetson Xavier NX y Orin Nano}, que son los dispositivos con el menor consumo energético, logran alcanzar los \textbf{35 y 45 \acrshort{fps}} respectivamente, lo que hace posible la ejecución en \textbf{tiempo real} del modelo en estos dispositivos.

\begin{figure}[tbp]
    \centering
    \begin{subfigure}[t]{0.7\textwidth}
        \includegraphics[width=\textwidth]{imagenes/LAP.jpg}
        \caption{Ejecución en portátil}
        \label{fig:nsyslap}
    \end{subfigure}
    %\vspace{2cm}
    \begin{subfigure}[t]{0.7\textwidth}
        \includegraphics[width=\textwidth]{imagenes/JET.jpg}
        \caption{Ejecución en Jetson AGX Thor}
        \label{fig:nsysjet}
    \end{subfigure}
    \caption{Desglose de la ejecución de una inferencia de YOLO en Nsight Systems}
    \label{fig:nsys}
\end{figure}

La \textbf{AGX Thor} logra la \textbf{menor latencia de YOLO11n en TensorRT} (\textbf{227 \acrshort{fps}}), cuya velocidad en OpenVINO dista notablemente de los equipos de sobremesa. La figura \ref{fig:nsys} explica el motivo, atendiendo a las filas del reporte de Nsight Systems, \textbf{CUDA API} refleja las llamadas de la \acrshort{cpu} para encolar las transferencias y los kernels en el \gls{stream} reservado y \textbf{Stream} la ejecución de las operaciones en la \acrshort{gpu} en dicho \gls{stream}. En el caso del portátil (figura \ref{fig:nsyslap}), se muestra como la operación \textbf{cuMemcpyDtoHAsync} se demora en la \acrshort{gpu} (\textbf{1.5 milisegundos}, la diferencia de latencia entre el portátil y la AGX Thor) debido a la ejecución pendiente de los \glspl{kernel}, en cierto modo arrastrados por la latencia de la operación \textbf{cuMemcpyHtoDAsync}. Por el lado de la AGX Thor (figura \ref{fig:nsysjet}), la operación \textbf{cuMemcpyDtoHAsync} se ejecuta en \acrshort{gpu} sin apenas latencia desde su emisión por la \acrshort{cpu}, debido a que las copias de memoria entre \textit{device} y \textit{host} se realizan en el \textbf{mismo chip} (\acrshort{gpu} integrada), sin tener que utilizar una conexión \acrshort{PCIe}. Otro aspecto importante es la ejecución de los \glspl{kernel} \textbf{en paralelo en múltiples \glspl{stream}} que realiza la Jetson AGX Thor, a diferencia del portátil, que llega a acumular gran cantidad de operaciones en la cola del \gls{stream} reservado. Otro aspecto implicado en las veloces transferencias es que el chip de la AGX Thor soporta \textbf{coherencia de dos vías entre la \acrshort{cpu} y la \acrshort{gpu}}, de forma que la \acrshort{cpu} puede acceder directamente a los contenidos de la caché de la \acrshort{gpu} y viceversa, sin problemas de coherencia en las lecturas (\cite{zerocpy} sección 3.2).

En el caso de YuNet, el uso de \gls{CUDA} como \gls{rt} supone un salto \textbf{menor} respecto a TensorRT, ya que OpenCV integrado con \gls{CUDA} no aprovecha todas las capacidades de la arquitectura de la \acrshort{gpu}, como los \textbf{tensor cores}. Excepto en las Jetson Xavier NX y AGX Thor, el resto de dispositivos no llegan a recortar ni 1 milisegundo o, en el caso de la Jetson AGX Orin, incluso llegan a \textbf{aumentar la latencia}. Por otra parte, el postprocesado del modelo entraña operaciones costosas que se ejecutan en la \acrshort{cpu}, como el algoritmo \gls{nms} (al igual que YOLO). Esta podría ser la principal causa de que los equipos de sobremesa destaquen sobre las Jetson.

En el caso del modelo ArcFace, el uso de TensorRT reduce a la \textbf{mitad o 3 veces} la latencia en los dispositivos, llegando a reducirse hasta \textbf{5 veces} en el caso del portátil, que tarda únicamente \textbf{1.3 milisegundos} de media en la inferencia y el reconocimiento (cálculo de distancias entre los vectores descriptores de 6 personas). %TODO: estaría bien (no sería necesario) comentar porqué ArcFace en ARM va mejor q en x86, ChatGPT dice q esto es normal.

OSNet\_x1, en comparación con ArcFace, no representa muchas variaciones en la latencia para los equipos de sobremesa. En cambio, en los dispositivos Jetson el salto es más notorio. A diferencia de YuNet o YOLO, las fases de pre y postprocesado no resultan un problema, de hecho, aprovechando la disponibilidad del código de PyTorch, se ha implementado el preprocesado internamente en el modelo, de forma que se ejecuta en la \acrshort{gpu} junto con los \glspl{kernel} de la inferencia (ver apéndice \ref{sec:preosnet} para más detalles).

\section{Discusiones}
\label{sec:discuss}

Las \acrshort{gpu}s de la familia Intel rinden de una forma excelente, consiguiendo un salto de mejora equiparable al de los dispositivos Jetson, cuya principal virtud son las veloces comunicaciones entre la \acrshort{cpu} y la \acrshort{gpu}. Sin embargo, puede afirmarse que las latencias de ejecución de los modelos en dichos dispositivos son \textbf{directamente proporcionales} al volumen de los datos de entrada y de salida a transferir, a diferencia de la AGX Thor en la ejecución de YOLO11n, que logra sobrepasar la limitación de las comunicaciones entre la \acrshort{cpu} y la \acrshort{gpu}.

%CREO Q ES PORQUE LA AGX ORIN SOLO OPERA A 60 W, MIENTRAS Q LA THOR VA A 130 W (https://docs.nvidia.com/cuda/cuda-for-tegra-appnote/ dice q la Thor soporta Full Coherency, es decir, q la CPU puede leer la cache de la GPU y viceversa, puedo indagar en esto si eso...).
La Jetson AGX Orin logra una ligera mejora en el rendimiento respecto a la Orin Nano con un consumo energético similar (30 W y 25 W respectivamente), al mismo tiempo que se queda atrás de los resultados logrados en la AGX Thor. Un ejemplo es la ejecución de YOLO11n en TensorRT, cuya diferencia entre ambas AGX es de \textbf{15 milisegundos} a favor de la AGX Thor. Los factores principales no se deben únicamente a la potencia (130 W) y a los recursos computacionales que la AGX Thor ofrece (ejemplo: caché L3 de 16 \acrshort{mb}s en la que cabe el modelo YOLO11n de 8 \acrshort{mb}s), sino a una mejor comprensión del hardware, que permite a herramientas como \gls{CUDA} y TensorRT explotar los recursos mediante la ejecución paralela de múltiples \glspl{stream} sin que el programador tenga que reservarlos explícitamente, entre otras técnicas como la coherencia de dos vías entre las cachés de la \acrshort{cpu} y la \acrshort{gpu} (\cite{zerocpy} sección 3.2).

Una característica fundamental de la Jetson AGX Thor es su \textbf{arquitectura Blackwell} de \acrshort{gpu}. Las arquitecturas anteriores a Blackwell demuestran ser más conservadoras a la hora de aplicar optimizaciones, mientras que esta última opta por las tácticas más rápidas, aunque impliquen una pérdida en la precisión. Un claro ejemplo es el modelo \textbf{OSNet\_x1}, la tabla \ref{tab:equips} muestra los resultados del modelo compilado en la Jetson AGX Thor con un nivel de optimización de \textbf{cero} por medio de la herramienta \textit{trtexec} (ver apéndice \ref{seq:execflags} para más detalles), es decir, sin aplicar ninguna optimización de la arquitectura y escoger la primera táctica que funcione correctamente. Al aumentar el nivel de optimización, la Jetson AGX Thor consigue unos excelentes \textbf{1.7 milisegundos}, a costa de una precisión en torno al \textbf{40\%}.

La ejecución de los modelos en \acrshort{gpu} resulta muy beneficiosa, con una reducción de hasta \textbf{2 veces} la latencia en todos los casos salvo YuNet, debido a la gestión de la inferencia de OpenCV por medio de \gls{CUDA} y a que de por sí el modelo se ha diseñado de forma ligera para su ejecución optimizada en \acrshort{cpu}.

TensorRT exprime todas las bondades que ofrece cada arquitectura de \acrshort{gpu}s de NVIDIA, demostrando resultados de latencia más que compatibles con el tiempo real. La arquitectura Blackwell de la Jetson AGX Thor llega a \textbf{superar en rendimiento} a los equipos de la \textbf{familia Intel} en el caso de YOLO11n, sin embargo, las optimizaciones aplicadas pueden afectar significativamente a la precisión al ser demasiado agresivas, como es el caso de OSNet.