\chapter{Análisis de rendimiento}
\label{chap:cnn}

\lettrine{E}{n} este capítulo se comentan los modelos de redes neuronales utilizados para las pruebas y se expone el procedimiento seguido para su realización, junto a una discusión detallada acerca de los resultados de rendimiento en los diferentes dispositivos del proyecto.

\section{Modelos de redes neuronales utilizados}
\label{sec:models}

A continuación se exponen las \acrshort{cnn} sujetas de las pruebas de este capítulo y que conforman el sistema. Dichos modelos también se emplearon en \cite{andrew} para la implementación del nodo cámara.

\begin{description}
    \item[YuNet] \cite{wu2023yunet} Es un modelo de \textbf{detección facial} diseñado para sistemas con recursos muy limitados, con tan solo \textbf{75856 parámetros} . Debido a los múltiples \textit{outputs} del modelo, complejos de procesar, se ha optado por utilizar la versión de OpenCV, que simplifica todo el proceso de pre y postprocesado. El modelo en formato \acrshort{onnx} se encuentra en este enlace \footnote{https://github.com/opencv/opencv\_zoo/blob/main/models/face\_detection\_yunet/face\_detection\_yunet\_2023mar.onnx}.
    \item[ArcFace] \textit{Additive Angular Margin Loss} o ArcFace \cite{deng2019arcface} es una \textbf{función de pérdida} (\gls{lossfunc}) que persigue mejorar el poder discriminativo de los modelos de \textbf{reconocimiento facial}, es decir, maximizar el margen que separa a las clases y minimizar el margen de los datos que pertenecen a una misma clase. Los \glspl{embedding} con los que opera ArcFace pertenecen al modelo ResNet-100. Se emplea el archivo de pesos de una implementación en TensorFlow Lite, accesible mediante el siguiente enlace \footnote{https://www.digidow.eu/f/datasets/arcface-tensorflowlite/model.tflite}.
    \item[YOLO] \textit{You Only Look Once} (YOLO) \cite{redmon2016you, mdpi} es una red de \textbf{detección de objetos} centrada en otorgar la mayor velocidad de procesamiento en tiempo real sin sacrificar la precisión. Existen múltiples versiones de YOLO, en \cite{andrew} se probaron las versiones 3, 5 y 8 en el tamaño nano, más ligero y veloz, pero al mismo tiempo más impreciso. En este proyecto a mayores se ha probado la versión 11 (anterior a la reciente versión 26), en su tamaño nano. El modelo se puede obtener mediante la \acrshort{api} de Python de ultralytics, que permite exportarlo con múltiples ajustes (ver apéndice \ref{sec:exports}).
    \item[OSNet] \textit{Omni-Scale Network} (OSNet) \cite{zhou2019omni} es un modelo de \textbf{reidentificación de personas} que emplea características de diferentes tamaños (\textit{omni-scale features}) para la clasificación (ejemplo: el cuerpo de una persona o el logo de su camiseta). Destaca por ser extremadamente ligero y por su elevada precisión. Se realizó una exportación a \acrshort{onnx} a partir de la implementación de PyTorch de \cite{andrew}.
\end{description}

\section{Realización de las pruebas}
\label{sec:probes}

A modo de evaluar el rendimiento en términos de precisión de los modelos, se parte de un \gls{dataset} (en el apéndice \ref{sec:datcam} se expone su composición) compuesto de la información extraída de un fichero \gls{rosbag}, que contiene los datos generados por las cámaras Kinect \acrshort{rgbd} y el sensor \acrshort{lidar} del Summit\_XL durante la realización de una prueba en el laboratorio de robótica del \acrshort{citic}, en el que 6 personas caminan alrededor del robot y se entrecruzan durante 105 segundos (aproximadamente). El robot también se desplaza en su propio eje, generando así imágenes borrosas y cambios impredecibles en la posición de los individuos en escena.

Durante la ejecución de una prueba, la red neuronal procesa los frames \acrshort{rgb} de los 2 vídeos (1 por cámara) del \gls{dataset}, de forma que al momento de terminar el procesamiento de un frame, el modelo puede continuar inmediatamente con el siguiente. La salida devuelta por la \acrshort{cnn} se contrasta con la información del \textit{\gls{dataset}} de la respectiva cámara, que devuelve un resultado final de precisión al finalizar la prueba. En cuanto a la latencia, se mide el tiempo de la inferencia, en el caso de los modelos de detección, y de la inferencia más el reconocimiento en el caso de los modelos de reconocimiento. En ambas situaciones, se emplea la función \textbf{time} del módulo \textbf{time} de Python para obtener los tiempos en cada frame. Finalmente, todas las latencias se fusionan a partir de la media.

%En las pruebas de los modelos de detección, se obtienen los frames directamente de los videos. 
Para los modelos de detección, el test determina una detección como coincidente si los porcentajes de intersección entre la \gls{bbox} obtenida respecto a la de referencia y viceversa superan o igualan un \textbf{umbral}. En este caso, dicho umbral se ha fijado en el \textbf{50\% de intersección} \cite{andrew}. Por el lado de los modelos de reconocimiento, se determina un reconocimiento como correcto si la predicción obtenida es la misma que la etiqueta (\textit{\gls{gt}}) recogida en el \gls{dataset}. Es importante mencionar que los modelos de reconocimiento no utilizan directamente los videos, sino que parten de una lista de frames \textbf{ya recortados} con las detecciones, simulando así una entrada otorgada por un modelo de detección.

Como ya se ha comentado en la sección \ref{sec:basearch}, existen varios métodos para escoger la identidad ganadora dentro de un conjunto de identidades probables, por simplicidad, siempre se escoge la primera predicción de la lista como la ganadora.

Para la ejecución de inferencias, se han empleado los \glspl{rt} de OpenVINO, TensorRT y OpenCV, este último para el caso concreto de la red YuNet. OpenVINO se ha utilizado para ejecutar las inferencias \textbf{puramente en \acrshort{cpu}}, mientras que TensorRT utiliza la \textbf{\acrshort{gpu}}. OpenCV permite realizar las inferencias \textbf{únicamente en \acrshort{cpu}} o por medio de \gls{CUDA}, que aplica la \acrshort{gpu}. Se han configurado los modelos para la inferencia en \textbf{precisión mixta \acrshort{fp}32 y \acrshort{fp}16} (y TensorFloat 32 en el caso de arquitecturas Ampere y superiores) en dichos \glspl{rt}, es decir, se ejecutan las capas de un modelo en la precisión que menor latencia reporte dentro de una lista de precisiones (ejemplo: \acrshort{fp}16 o \acrshort{fp}32) \cite{mixed}. Esta funcionalidad puede emplearse siempre que la arquitectura de los dispositivos la implemente.

\section{Resultados}
\label{sec:modelic}

\input{contenido/tables/tests/tabequips}

En la tabla \ref{tab:equips} se muestran los resultados de \textbf{latencia (en milisegundos)} y de la métrica \textbf{\textit{F1\_score}} para los modelos de detección y \textbf{\textit{precision}} para los modelos de reconocimiento (ver métricas en el apéndice \ref{sec:modeleval}). Cada resultado refleja la media y la desviación típica de \textbf{5 ejecuciones} sin contar la ejecución previa del test a modo de calentamiento para cada modelo.

En el caso del modelo YOLO, los 2 equipos de sobremesa y la Jetson AGX Thor se quedan por debajo de los 30 milisegundos en la versión de OpenVINO, mientras que la latencia en el resto de dispositivos es significativamente mayor. Parte de la diferencia en la latencia viene del \textbf{costoso pre y postprocesado de YOLO} (ver optimización del postprocesado en el apéndice \ref{sec:postyolo}), que se realiza en la \acrshort{cpu}, donde los equipos de sobremesa destacan. Entre el portátil y el PC del laboratorio hay \textbf{8 milisegundos de diferencia} a favor del último, ya que los modelos de \acrshort{cpu}s para portátiles pretenden ser eficientes energéticamente, por lo que se distribuyen con una menor potencia respecto a las \acrshort{cpu}s de sobremesa, que pueden hacer \textit{overclock} a mayor potencia sin importar el consumo, logrando así acelerar la ejecución de las operaciones. %siendo ambas \acrshort{cpu}s de la misma generación (Intel Core i7 12th).

Al trasladar la carga de la inferencia de YOLO a la \acrshort{gpu} cuando se utiliza TensorRT, el tiempo de inferencia es hasta \textbf{7 veces} menor en el caso de la Jetson AGX Thor y \textbf{5 veces} menor en el caso de la Jetson Xavier NX. Las Jetson Xavier NX y Orin Nano logran alcanzar los \textbf{35 y 45 \acrshort{fps}} respectivamente, lo que hace posible la ejecución en tiempo real del modelo en estos dispositivos. La AGX Thor ha logrado la menor latencia de YOLO en TensorRT, cuya velocidad en OpenVINO dista notablemente de los equipos de sobremesa.

%TODO: En la tabla \ref{tab:equips} se muestran los resultados obtenidos en 2 equipos x86, comparándolos con la Jetson Orin Nano, el sistema embebido con el que se ha obtenido el mejor rendimiento. En equipos x86, la latencia experimentada  es hasta \textbf{5.3 veces menor} (YOLOv8n) para los modelos ejecutados en CPU y hasta \textbf{4.9 veces menor} (YuNet) para los modelos ejecutados en GPU que la obtenida por la Jetson Orin Nano. Debido a que OpenVINO está pensado para hardware de Intel, las optimizaciones en la Jetson no suponen ninguna mejora, al contrario que en los equipos x86 con una CPU \textbf{Intel i7}. Por otro lado, es sorprendente que también existan diferencias significativas respecto a la Jetson \textbf{cuando se realizan las inferencias en GPU}. La diferencia más notoria es con YuNet en el PC del laboratorio, que posee una \textbf{GeForce GTX 1650}, que está \textbf{una generación por detrás} de la GPU de la Jetson Orin (las arquitecturas son Turing y Ampere respectivamente) \cite{archs}. Esto implica que, para la carga de trabajo de este proyecto, \textbf{la GPU no es el factor predominante} (TODO: no lo creo, no tiene mucho sentido si digo que se reduce la latencia hasta 7 veces usando la GPU, y que la AGX Thor es la que mejor ejecuta YOLO cuando se utiliza la GPU).

En el caso de YuNet, el uso de \gls{CUDA} como \gls{rt} supone un salto menor respecto a TensorRT, ya que OpenCV integrado con \gls{CUDA} no aprovecha todos los componentes de la arquitectura de la \acrshort{gpu}, como los \textbf{tensor cores}. Excepto en las Jetson Xavier NX y AGX Thor, el resto de dispositivos no llegan a recortar ni 1 milisegundo o, en el caso de la Jetson AGX Orin, incluso llegan a \textbf{aumentar la latencia}. También es importante considerar que YuNet está compuesto por una arquitectura muy ligera, por lo que no hay mucho margen de mejora en la latencia de inferencia, en cambio, el postprocesado entraña operaciones costosas que se ejecutan en la \acrshort{cpu}, como el algoritmo \gls{nms} (al igual que YOLO). Esta podría ser la principal causa del excelente rendimiento en los equipos de sobremesa.

En el caso del modelo ArcFace, el uso de TensorRT reduce a la \textbf{mitad o 3 veces} la latencia en los dispositivos, llegando a reducirse \textbf{5 veces} en el caso del portátil, que tarda únicamente \textbf{1.3 milisegundos} de media en la inferencia y el reconocimiento (cálculo de distancias entre los vectores descriptores de 6 personas).

OSNet\_x1, en comparación con ArcFace, no representa muchas variaciones en la latencia para los equipos de sobremesa. En cambio, en los dispositivos Jetson el salto es más notorio. A diferencia de YuNet o YOLO, las fases de pre y postprocesado no resultan un problema, de hecho, aprovechando la disponibilidad del código de PyTorch, se ha implementado el preprocesado internamente en el modelo, de forma que se ejecuta en la \acrshort{gpu} junto con los \glspl{kernel} de la inferencia (ver apéndice \ref{sec:preosnet} para más detalles). La ejecución del preprocesado en \acrshort{gpu} ha supuesto una mejora de aproximadamente \textbf{1 milisegundo} en la ejecución.

\section{Discusiones}
\label{sec:discuss}

%La arquitectura Turing (gráfica del PC del laboratorio) ha conseguido un speed up del 3.2 en el caso del modelo ArcFace, menor que el 5.5 obtenido en el mismo modelo  .

Las \acrshort{gpu}s de los equipos de sobremesa rinden de una forma excelente, consiguiendo un salto de mejora equiparable al de los dispositivos Jetson, cuyas \acrshort{gpu}s poseen los tensor cores, más potentes que los núcleos de \gls{CUDA}. El resultado de \textbf{1.3 milisegundos} de ArcFace indica que las \acrshort{gpu}s GeForce GTX 1650 y RTX 3050 no son únicamente rápidas en la inferencia, sino también en las operaciones de transferencia de datos (cudaMemcpy) y sincronización.

%Los resultados tan bajos de latencia en los equipos de sobremesa son especialmente sorprendentes, teniendo en cuenta de que las operaciones de copia de memoria y sincronización deberían de añadir una sobrecarga notoria en las inferencias.

%El uso de memoria fijada mapeada en los dispositivos Jetson supone una ligera reducción en la latencia (en torno a 1 milisegundo según las pruebas), que no compensa la latencia de procesamiento. De todos modos, se consiguen ahorrar las llamadas a cudaMemcpy en los sistemas con \acrshort{gpu}s integradas, lo que ayuda a reducir la congestión de la cola de operaciones de un \gls{stream} en la \acrshort{gpu}.

La Jetson AGX Orin, a pesar de doblar los recursos computacionales de la Orin Nano, apenas consigue una mejora en cuanto a reducción de la latencia, al mismo tiempo que se queda atrás de los resultados logrados en la AGX Thor. Un ejemplo es la ejecución de YOLO11n en TensorRT, cuya diferencia entre ambas AGX es de \textbf{15 milisegundos} a favor de la AGX Thor. Varios de los factores implicados podrían ser una caché más grande, que reduce los accesos a la \acrshort{dram} (YOLO11n pesa en torno a 8 \acrshort{mb}s, por lo que puede almacenarse enteramente en la caché L3 de 16 \acrshort{mb}s), las propias características de la \acrshort{cpu}, \acrshort{gpu} y memoria y/o fundamentalmente la arquitectura Blackwell, de la que TensorRT consigue sacar provecho.

Las arquitecturas anteriores a Blackwell demuestran ser más conservadoras a la hora de aplicar optimizaciones, mientras que esta última arquitectura opta por las tácticas más rápidas, aunque impliquen una pérdida en la precisión. Un claro ejemplo es el modelo \textbf{OSNet\_x1}, la tabla \ref{tab:equips} muestra los resultados del modelo compilado en la Jetson AGX Thor con un nivel de optimización de \textbf{cero} por medio de la herramienta \textit{trtexec} (ver apéndice \ref{seq:execflags} para más detalles), es decir, sin aplicar ninguna optimización de la arquitectura y escoger la primera táctica que funcione correctamente. Al aumentar el nivel de optimización, la Jetson AGX Thor consigue unos excelentes \textbf{1.7 milisegundos}, llegando a ser la triunfadora, a costa de una precisión en torno al \textbf{40\%}.

La ejecución de los modelos en \acrshort{gpu} resulta muy beneficiosa, con una reducción de la latencia de mínimo la mitad en todos los casos a excepción del modelo YuNet, debido a la gestión de la inferencia de OpenCV por medio de \gls{CUDA} y a que de por sí el modelo se ha diseñado de forma ligera para su ejecución optimizada en \acrshort{cpu}.

TensorRT exprime todas las bondades que ofrece cada arquitectura de \acrshort{gpu}s de NVIDIA, demostrando resultados de latencia más que compatibles con el tiempo real. La arquitectura Blackwell de la Jetson AGX Thor llega a superar en rendimiento a equipos con \acrshort{cpu}s de Intel en el caso de YOLO11n, sin embargo, las optimizaciones aplicadas pueden afectar significativamente a la precisión al ser demasiado agresivas, como es el caso de OSNet.