\chapter{Análisis de rendimiento}
\label{chap:cnn}

\lettrine{E}{n} este capítulo se comentan los modelos de redes neuronales utilizados para las pruebas y se expone el procedimiento seguido para su realización, junto a una discusión detallada acerca de los resultados de rendimiento en los diferentes dispositivos del proyecto.

\section{Modelos de redes neuronales utilizados}
\label{sec:models}

A continuación se exponen las \acrshort{cnn} sujetas de las pruebas de este capítulo y que conforman el sistema. Dichos modelos también se emplearon en \cite{andrew} para la implementación del nodo cámara.

\begin{description}
    \item[YuNet] \cite{wu2023yunet} Es un modelo de \textbf{detección facial} diseñado para sistemas con recursos muy limitados, con tan solo \textbf{75856 parámetros} . Debido a los múltiples \textit{outputs} del modelo, complejos de procesar, se ha optado por utilizar la versión de OpenCV, que simplifica todo el proceso de pre y postprocesado. El modelo en formato \acrshort{onnx} se encuentra en este enlace \footnote{https://github.com/opencv/opencv\_zoo/blob/main/models/face\_detection\_yunet/face\_detection\_yunet\_2023mar.onnx}.
    \item[ArcFace] \textit{Additive Angular Margin Loss} o ArcFace \cite{deng2019arcface} es una \textbf{función de pérdida} (\gls{lossfunc}) que persigue mejorar el poder discriminativo de los modelos de \textbf{reconocimiento facial}, es decir, maximizar el margen que separa a las clases y minimizar el margen de los datos que pertenecen a una misma clase. Los \glspl{embedding} con los que opera ArcFace pertenecen al modelo ResNet-100. Se emplea el archivo de pesos de una implementación en TensorFlow Lite, accesible mediante el siguiente enlace \footnote{https://www.digidow.eu/f/datasets/arcface-tensorflowlite/model.tflite}.
    \item[YOLO] \textit{You Only Look Once} (YOLO) \cite{redmon2016you, mdpi} es una red de \textbf{detección de objetos} centrada en otorgar la mayor velocidad de procesamiento en tiempo real sin sacrificar la precisión. Existen múltiples versiones de YOLO, en \cite{andrew} se probaron las versiones 3, 5 y 8 en el tamaño nano, más ligero y veloz, pero al mismo tiempo más impreciso. En este proyecto a mayores se ha probado la versión 11 (anterior a la reciente versión 26), en su tamaño nano. El modelo se puede obtener mediante la \acrshort{api} de Python de ultralytics, que permite exportarlo con múltiples ajustes (ver apéndice \ref{sec:exports}).
    \item[OSNet] \textit{Omni-Scale Network} (OSNet) \cite{zhou2019omni} es un modelo de \textbf{reidentificación de personas} que emplea características de diferentes tamaños (\textit{omni-scale features}) para la clasificación (ejemplo: el cuerpo de una persona o el logo de su camiseta). Destaca por ser extremadamente ligero y por su elevada precisión. Se realizó una exportación a \acrshort{onnx} a partir de la implementación de PyTorch de \cite{andrew}.
\end{description}

\section{Realización de las pruebas}
\label{sec:probes}

A modo de evaluar el rendimiento en términos de precisión de los modelos, se parte de un \gls{dataset} (ver apéndice \ref{sec:datcam}) compuesto de la información extraída de un fichero \gls{rosbag}, que contiene los datos generados por las cámaras Kinect \acrshort{rgbd} y el sensor \acrshort{lidar} del Summit\_XL durante la realización de una prueba en el laboratorio de robótica del \acrshort{citic}, en el que seis personas caminan alrededor del robot y se entrecruzan durante 105 segundos (aproximadamente). El robot también se desplaza en su propio eje, generando así imágenes borrosas y cambios impredecibles en la posición de los individuos en escena.

Durante la ejecución de una prueba, la red neuronal procesa los frames \acrshort{rgb} de los dos vídeos (uno por cámara) del \gls{dataset}, de forma que al momento de terminar el procesamiento de un frame, el modelo puede continuar inmediatamente con el siguiente. La salida devuelta por la \acrshort{cnn} se contrasta con la información del \textit{\gls{dataset}} de la respectiva cámara, que devuelve un resultado final de precisión al finalizar la prueba. En cuanto a la latencia, se mide el tiempo de la \textbf{inferencia}, en el caso de los modelos de detección, y de la \textbf{inferencia más el reconocimiento} en el caso de los modelos de reconocimiento. En ambas situaciones, se emplea la función \textbf{time} del módulo \textbf{time} de Python para obtener los tiempos en cada frame. Finalmente, todas las latencias se fusionan a partir de la media. \\
\indent La latencia en el reconocimiento es el tiempo total de calcular las distancias coseno de los \glspl{embedding} de entrada contra los \glspl{embedding} de las seis personas registradas en la base de datos. Como ya se ha comentado en la sección \ref{sec:basearch}, existen varios métodos para escoger la identidad ganadora dentro de un conjunto de identidades probables, por simplicidad, siempre se escoge la primera predicción de la lista como la ganadora. \\
\indent Para los modelos de detección, el test determina una detección como coincidente si los porcentajes de intersección entre la \gls{bbox} obtenida respecto a la de referencia y viceversa superan o igualan un \textbf{umbral}. En este caso, dicho umbral se ha fijado en el \textbf{50\% de intersección} \cite{andrew}. Por el lado de los modelos de reconocimiento, se determina un reconocimiento como correcto si la predicción obtenida es la misma que la etiqueta (\textit{\gls{gt}}) recogida en el \gls{dataset}. Es importante mencionar que los modelos de reconocimiento no utilizan directamente los vídeos, sino que parten de una lista de frames \textbf{ya recortados} con las detecciones, simulando así una entrada otorgada por un modelo de detección.\\
\indent Para la ejecución de inferencias, se han empleado los \glspl{rt} de OpenVINO, TensorRT y OpenCV, este último para el caso concreto de la red YuNet. OpenVINO se ha utilizado para ejecutar las inferencias \textbf{puramente en \acrshort{cpu}}, mientras que TensorRT utiliza la \textbf{\acrshort{gpu}}. OpenCV permite realizar las inferencias \textbf{únicamente en \acrshort{cpu}} o por medio de \gls{CUDA}, que aplica la \acrshort{gpu}. Se han configurado los modelos para la inferencia en \textbf{precisión mixta \acrshort{fp}32 y \acrshort{fp}16} (y TensorFloat 32 en el caso de arquitecturas Ampere y superiores) en dichos \glspl{rt}, es decir, se ejecutan las capas de un modelo en la precisión que menor latencia reporte dentro de una lista de precisiones (ejemplo: \acrshort{fp}16 o \acrshort{fp}32) \cite{mixed}. Esta funcionalidad puede emplearse siempre que la arquitectura de los dispositivos la implemente.

\section{Resultados}
\label{sec:modelic}

\input{contenido/tables/tests/tabequips}

En la tabla \ref{tab:equips} se muestra la \textbf{latencia (en milisegundos)} y los resultados de la métrica \textbf{\textit{F1\_score}} para los modelos de detección y \textbf{\textit{precision}} para los modelos de reconocimiento (ver apéndice \ref{sec:modeleval}), además del \emph{speed up} logrado en la versión de \acrshort{gpu} de cada dispositivo. Cada resultado refleja la media y la desviación típica de \textbf{5 ejecuciones} sin contar la ejecución previa del test a modo de calentamiento para cada modelo.

En el caso del modelo YOLO11n, los 2 equipos de sobremesa y la Jetson AGX Thor se quedan por debajo de los 30 milisegundos en la versión de OpenVINO, mientras que la latencia en el resto de dispositivos es significativamente mayor. El \textbf{costoso pre y postprocesado del modelo} (ver optimización del postprocesado en el apéndice \ref{sec:postyolo}) realizado en la \acrshort{cpu} supone el principal problema para la familia Jetson. Entre el portátil y el PC del laboratorio hay \textbf{8 milisegundos de diferencia} a favor del último, ya que su \acrshort{cpu} le permite hacer \textit{overclock} a mayores frecuencias sin importar el consumo, a diferencia del portátil, pensado para otorgar un mayor ahorro energético. %siendo ambas \acrshort{cpu}s de la misma generación (Intel Core i7 12th).

Al trasladar la inferencia de YOLO11n a la \acrshort{gpu}, se logra un \emph{speed up} de hasta el \textbf{6.6} en el caso de la Jetson AGX Thor y \textbf{5.1} en el caso de la Jetson Xavier NX. Todos los dispositivos logran alcanzar el \textbf{tiempo real} (fijado a 10 \acrshort{fps}) en la ejecución del modelo. %, incluidas las Jetson Xavier NX y Orin Nano, los dispositivos con el menor consumo energético los \textbf{35 y 45 \acrshort{fps}} respectivamente, lo que hace posible la ejecución en \textbf{tiempo real} del modelo en estos dispositivos.

\begin{figure}[tbp]
    \centering
    \begin{subfigure}[t]{0.7\textwidth}
        \includegraphics[width=\textwidth]{imagenes/LAP.jpg}
        \caption{Ejecución en portátil}
        \label{fig:nsyslap}
    \end{subfigure}
    %\vspace{2cm}
    \begin{subfigure}[t]{0.7\textwidth}
        \includegraphics[width=\textwidth]{imagenes/JET.jpg}
        \caption{Ejecución en Jetson AGX Thor}
        \label{fig:nsysjet}
    \end{subfigure}
    \caption{Desglose de la ejecución de una inferencia de YOLO en Nsight Systems}
    \label{fig:nsys}
\end{figure}

La \textbf{Jetson AGX Thor} logra la \textbf{menor latencia de YOLO11n en TensorRT} (\textbf{227 \acrshort{fps}}), cuya velocidad en OpenVINO dista notablemente de los dispositivos Intel. La figura \ref{fig:nsys} muestra el reporte de Nsight Systems de la inferencia de YOLO11n en el pórtatil y en la Jetson AGX Thor. Atendiendo a las filas del reporte, \emph{CUDA API} refleja las llamadas de la \acrshort{cpu} para encolar las transferencias de memoria y \glspl{kernel} en el \gls{stream} reservado, mientras que \emph{Stream} muestra el tiempo de ejecución de las operaciones en dicho \gls{stream} por la \acrshort{gpu}. En el caso del portátil (figura \ref{fig:nsyslap}), se muestra como la operación \textbf{cuMemcpyDtoHAsync} se demora en la \acrshort{gpu} (\textbf{1.5 milisegundos}, la diferencia de latencia del modelo entre el portátil y la AGX Thor) debido a la ejecución pendiente de los \glspl{kernel}, en cierto modo arrastrados por la latencia de la operación \textbf{cuMemcpyHtoDAsync}. Por el lado de la AGX Thor (figura \ref{fig:nsysjet}), la operación \textbf{cuMemcpyDtoHAsync} se ejecuta en \acrshort{gpu} sin apenas latencia desde su emisión por la \acrshort{cpu}, debido a que las copias de memoria entre \textit{device} y \textit{host} se realizan en el \textbf{mismo chip} (\acrshort{gpu} integrada), sin tener que utilizar una conexión \acrshort{PCIe}. Otro aspecto importante es la ejecución de los \glspl{kernel} \textbf{en paralelo en múltiples \glspl{stream}} que realiza la Jetson AGX Thor, a diferencia del portátil, que parece indicar cierta saturación en el \gls{stream} reservado.

Otro aspecto implicado en las veloces transferencias se debe a que el chip de la AGX Thor soporta \textbf{coherencia de dos vías entre la \acrshort{cpu} y la \acrshort{gpu}} (\cite{zerocpy} sección 3.2), de forma que la \acrshort{cpu} puede acceder directamente a los contenidos de la caché de la \acrshort{gpu} y viceversa, sin problemas de coherencia en las lecturas.

En el caso de YuNet, la inferencia en OpenCV mediante \gls{CUDA} supone un salto \textbf{menor} respecto a TensorRT, ya que no es capaz de aprovechar todas las capacidades de la arquitectura de la \acrshort{gpu}, como los \textbf{tensor cores}. Salvo la Jetson Xavier NX, con un \emph{speed up} de 2.3, el resto de dispositivos apenas logran mejoras o, en el caso de la Jetson AGX Orin, incluso llegan a \textbf{aumentar la latencia} en la versión de la \acrshort{gpu}. Otro punto es que en la mayoría de dispositivos YuNet es el modelo más rápido en \acrshort{cpu}, por lo que existe menos margen para mejoras (esto puede deberse a que el modelo fue diseñado para su ejecución en sistemas embebidos sin \acrshort{gpu}). Al igual que YOLO, el modelo realiza un costoso postprocesado en \acrshort{cpu} (aplica el algoritmo \gls{nms}), lo que supone una de las principales causas de que los dispositivos Intel destaquen sobre las Jetson.

En el caso del modelo ArcFace, el uso de TensorRT logra \emph{speed ups} de más del doble, llegando a \textbf{quintuplicar} la velocidad en el portátil. Los pipelines de pre y postprocesado del modelo son muy simples computacionalmente, lo que permite a los dispositivos obtener latencias muy bajas. %TODO: estaría bien (no sería necesario) comentar porqué ArcFace en ARM va mejor q en x86, ChatGPT dice q esto es normal.

En la versión de \acrshort{cpu} de OSNet\_x1 la latencia aumenta en dispositivos Jetson respecto a ArcFace, a diferencia de la familia Intel, que logra reducirla. Al igual que ArcFace, los pipelines de pre y postprocesamiento no son costosos, ya que la fase de preprocesado se realiza en \acrshort{gpu} (ver apéndice \ref{sec:preosnet}) y el postprocesado consiste únicamente en aplicar la normalización euclidiana (ecuación \ref{eq:l2norm} apéndice \ref{sec:val}) a la salida.
En el caso de la Jetson AGX Thor, su \textbf{arquitectura Blackwell} de \acrshort{gpu} aplica las optimizaciones más rápidas, aunque impliquen una pérdida en la precisión. La tabla \ref{tab:equips} muestra los resultados del modelo compilado con un nivel de optimización de \textbf{cero} por medio de la herramienta \textit{trtexec} (ver apéndice \ref{seq:execflags}), es decir, sin aplicar ninguna optimización de la arquitectura y escoger la primera táctica (reparto del trabajo) que funcione correctamente. Al aumentar dicho nivel se consiguen unos excelentes \textbf{1.7 milisegundos}, a costa de una precisión en torno al \textbf{40\%}.

\section{Discusiones}
\label{sec:discuss}

Se han desarrollado implementaciones de las inferencias para seis dispositivos diferentes en dos versiones, una para la \acrshort{cpu} y la otra para la \acrshort{gpu}. En la mayoría de casos se triplica la velocidad en la versión de \acrshort{gpu}, llegando a quintuplicarlos en la familia Intel y a sextuplicarlos en la familia Jetson. Las mejoras en el rendimiento son muy similares en todos los dispositivos, lo que indica que se ha obtenido el máximo rendimiento.
La casi nula mejora en el rendimiento del modelo YuNet supone una excepción, debido a la gestión de la inferencia de OpenCV por medio de \gls{CUDA}, más ineficiente que la realizada por TensorRT.


%La Jetson Xavier NX La Jetson AGX Orin logra una ligera mejora en el rendimiento respecto a la Orin Nano con un consumo energético similar (30 W y 25 W respectivamente), al mismo tiempo que se queda atrás de los resultados logrados en la AGX Thor. Un ejemplo es la ejecución de YOLO11n en TensorRT, cuya diferencia entre ambas AGX es de \textbf{15 milisegundos} a favor de la AGX Thor. Los factores principales no se deben únicamente a la potencia (130 W) y a los recursos computacionales que la AGX Thor ofrece (ejemplo: caché L3 de 16 \acrshort{mb}s en la que cabe el modelo YOLO11n de 8 \acrshort{mb}s), sino a una mejor comprensión del hardware, que permite a herramientas como \gls{CUDA} y TensorRT explotar los recursos mediante la ejecución paralela de múltiples \glspl{stream} sin que el programador tenga que reservarlos explícitamente, entre otras técnicas como la coherencia de dos vías entre las cachés de la \acrshort{cpu} y la \acrshort{gpu}.

%Las \acrshort{gpu}s de la familia Intel rinden de una forma excelente, consiguiendo un salto de mejora equiparable al de los dispositivos Jetson, cuya principal virtud son las veloces comunicaciones entre la \acrshort{cpu} y la \acrshort{gpu}. Sin embargo, puede afirmarse que las latencias de ejecución de los modelos en dichos dispositivos son \textbf{directamente proporcionales} al volumen de los datos de entrada y de salida a transferir, a diferencia de la AGX Thor en la ejecución de YOLO11n, que logra sobrepasar la limitación de las comunicaciones entre la \acrshort{cpu} y la \acrshort{gpu}.

Estableciendo un balance entre rendimiento, consumo y precio, los equipos de la familia Intel ofrecen la mejor relación rendimiento/precio (poco más de 1000 €), a costa de un elevado consumo energético. Por otro lado, la Jetson AGX Thor rinde de una forma equiparable a los equipos Intel, con las ventajas de ser más compacta y eficiente energéticamente, pero con el inconveniente de su elevado precio (3499 \$). La Jetson AGX Orin ofrece mayor potencia respecto a la Jetson Orin Nano por un consumo similar (30 y 25 W respectivamente), aunque no compensa su precio (1999 \$ respecto a 249 \$). Por último, la Jetson Xavier NX no logra mejorar los resultados de la Orin Nano, siendo esta última más económica. En definitiva, la Jetson Orin Nano supone la mejor relación entre rendimiento, consumo y precio.

La Jetson AGX Thor consigue el mayor \emph{speed up} de todos los dispositivos y se distancia de las Jetson en cuanto a velocidad, especialmente en la ejecución de YOLO11n. Los factores principales no se deben únicamente a la potencia y recursos computacionales, sino a una mejor comprensión del hardware, que permite a herramientas como \gls{CUDA} y TensorRT explotar los recursos mediante la ejecución paralela de múltiples \glspl{stream} sin que el programador tenga que reservarlos explícitamente, entre otras técnicas como la coherencia de dos vías entre las cachés de la \acrshort{cpu} y la \acrshort{gpu}. Otro factor fundamental es su arquitectura \textbf{Blackwell}, que opta por optimizaciones más eficaces, aunque puedan implicar en pérdidas de la precisión en algunos casos.

Es importante puntualizar que las pruebas de este capítulo evalúan los modelos de forma \textbf{individual}. En el siguiente capítulo, se realizan pruebas de los modelos funcionando de forma conjunta en el sistema en tiempo real.