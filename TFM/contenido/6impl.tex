\chapter{Implementación del sistema}
\label{chap:impl}

\lettrine{E}{n} este capítulo se ahonda en los detalles de implementación de los componentes del sistema extendido. También se comentan los cambios relevantes realizados al sistema base.

\input{contenido/codes/trtskel.tex}

\input{contenido/codes/training.tex}

\input{contenido/codes/weib.tex}

\input{contenido/codes/cash.tex}

TODO: explicar en algún momento que implementamos el reconocimiento adaptativo como módulo por separado del sistema, y que realmente a excepción de la escalabilidad y el ROS 2, el sistema sigue igual.

\section{Códigos de inferencia para el \gls{rt} de TensorRT}
En la figura \ref{fig:finalsys} se muestran los 4 modelos utilizados en el nodo cámara y que se detallarán en la sección \ref{sec:models}. Dichos modelos cuentan con su implementación optimizada para OpenVINO creada en \cite{andrew}, excepto para el modelo de detección de caras (YuNet), que se ejecuta en el \gls{rt} de OpenCV. En este proyecto se han implementado nuevas versiones de las inferencias adaptadas para su uso en \acrshort{gpu} por medio de TensorRT y PyCUDA. PyCUDA ofrece una API que facilita gran parte de la interacción con CUDA, aun así, se necesita gestionar en el código temas como la creación de contextos y reserva de la memoria.

TensorRT trabaja con su propio formato de los modelos, por lo que es necesario realizar una conversión de los mismos a dicho formato mediante las herramientas expuestas en el apéndice \ref{chap:procedure}. En el proceso de conversión se aplica un proceso de selección de optimizaciones según la arquitectura de la \acrshort{gpu}, que prueba distintas tácticas para la distribución del trabajo y diferentes precisiones para quedarse con la combinación más rápida, sin comprometer la precisión del modelo.

El código \ref{coud:trtskel} muestra el esqueleto de dichas inferencias, cada vez que se crea una instancia del modelo en Python, se reservan los siguientes recursos:
\begin{itemize}
    \item \textbf{Contexto de CUDA}: en este caso se recibe como parámetro, ya que es necesario crear dicho contexto en el mismo código donde se ejecuta el bucle infinito de \acrshort{ros} (ejemplo: nodo cámara), de modo que pueda manejarse correctamente.
    \item \textbf{Contexto del modelo}: dicho contexto se crea a partir del modelo deserializado de TensorRT.
    \item \textbf{CUDA Stream}: canal que utilizará la \acrshort{gpu} para ejecutar todas las inferencias.
    \item \textbf{Device input}: memoria del \gls{tensor} de entrada reservado en la \acrshort{gpu}. Se especifica el tamaño en bytes del \gls{tensor} que representa los datos de entrada.
    \item \textbf{Device output}: memoria del \gls{tensor} de salida reservado en la \acrshort{gpu}. Se especifica el tamaño en bytes del \gls{tensor} que representa los datos de salida.
    \item \textbf{Bindings}: lista de direcciones de memoria de los \glspl{tensor} a utilizar durante la inferencia.
\end{itemize}

Dichos recursos se reservan \textbf{una sola vez}, que corresponde con el momento de creación de la instancia de la clase Python que representa al modelo.

La función \textit{infer} del código \ref{coud:trtskel} se ejecuta por cada vez que se recibe un frame de entrada a procesar. Se reserva en \acrshort{cpu} un array de NumPy para almacenar la salida de la inferencia y se ejecuta la función PREPROCESING con el frame de entrada, que representa el pipeline de preprocesado específico para cada modelo. El frame preprocesado se copia a la memoria de la \acrshort{gpu} (o \textit{device}) y se realiza la inferencia en el stream de CUDA reservado. Finalmente, los resultados se copian de vuelta a la memoria de la \acrshort{cpu} (o \textit{host}) y se ejecuta el pipeline de postprocesado del resultado definido en la función POSTPROCESING.

Las operaciones de copia de datos y la inferencia se ejecutan de forma \textbf{asíncrona}, por lo que es necesario introducir una barrera de sincronización una vez todas las operaciones se emitan, de forma que nuevos \glspl{thread} no sobrescriben la memoria de la \acrshort{gpu} mientras esta realiza inferencias.

%\section{Creación de las SVM}
%\label{sec:training}
\section{Procesamiento de video}
La implementación sigue el flujo expuesto en la sección \ref{sec:tracker}.

Para realizar una asignación óptima del método húngaro, se utilizó la función \textit{linear\_sum\_assignment} otorgada por el módulo \textit{optimize} de la librería SciPy \cite{lsa}. Dicha librería implementa el algoritmo Jonker-Volgenan, que es una variante del método húngaro con complejidad computacional $O(n^{3})$ \cite{lsa, o3}. TODO: el coste es muy elevado, estaría bien poner a partir de qué número de personas se vuelve demasiado costoso.

El cálculo del \acrshort{iou} es sencillo y no entraña apenas complejidad computacional. La distancia euclidiana se calcula como se expone en la sección \ref{subsec:reiden}, para el cálculo de la raíz cuadrada, se empleó la función \textit{sqrt} de la librería \textit{math}.

\section{Módulo de valoración}

Como se expuso en la sección \ref{sec:val}, el módulo de valoración devuelve un conjunto de puntuaciones que representan a la secuencia para todos los comités.

La implementación sigue el funcionamiento descrito en la sección \ref{sec:val}. Para calcular los percentiles y la mediana en las funciones FDF y SDF se utilizan las funciones \textit{percentile} y \textit{median} respectivamente de la librería NumPy, como se aplica en \cite{src} (función presentCandidate de tools.py).

\section{Módulo de reconocimiento}

Como se mencionó en la sección \ref{sec:reckon}, el módulo de reconocimiento determina si la entidad detectada se corresponde a un individuo previamente reclutado o a una entidad \textbf{desconocida}.

El código \ref{coud:weib}, basado en \cite{src} (función selfUpdate de tools.py), muestra la implementación de dicha funcionalidad. Se parte de la función RDF (Recognition Decision Function), que recibe el conjunto de puntuaciones ordenado y devuelve la decisión de reconocimiento.

Del conjunto de puntuaciones ordenadas, se excluye la mejor puntuación de todos los comités (es decir, la puntuación más baja) y se compone la distribución de puntuaciones no coincidentes. Se calcula para cada punto la distancia respecto a la mediana (variable v), de forma que los puntos de la distribución se distancian del mejor comité. Se obtienen los parámetros \textit{shape} y \textit{scale} a partir de la distribución, que modelan la función de Weibull. Finalmente, se calcula la probabilidad de pertenencia del mejor comité a la distribución (función weib) y se toma la decisión en base a un umbral (Tw). Si la probabilidad es inferior al umbral, se reconoce el caso como un extremo y se asigna la identidad correspondiente al sujeto (\textit{drift}), en caso contrario se devuelve como desconocido (\textit{unknown}).

Para obtener los parámetros que componen la distribución de Weibull, se utiliza la función \textit{fit} de la clase \textit{weibull\_fit} del módulo \textit{stats} de la librería SciPy.

\section{Módulo de actualización}

La implementación del módulo sigue el flujo descrito en la sección \ref{sec:reckon}. La creación de las \acrshort{svm} se realiza a partir de la función \textit{def\_svm} vista en la sección \ref{sec:init}.

\section{Módulo de limitación}
Este módulo se activa cuando se excede el límite prefijado de clasificadores para un comité.

Se invoca a la función que calcula el criterio de favorabilidad, que otorga un valor para cada \acrshort{svm} a partir de la suma de los criterios de diversidad y coherencia. Se almacenan los resultados en un array de NumPy y se ordenan los valores devueltos mediante la función \textit{argsort} de NumPy, por lo que la \acrshort{svm} a eliminar se corresponde con la primera entrada del vector (es decir, el que tenga el valor de favorabilidad más bajo).

\section{Personas registradas}
\label{sec:init}

Como se ha comentado en la sección \ref{sec:initarch}, la base de datos de usuarios se puede inicializar de un modo supervisado o no supervisado. Para que el algoritmo de clasificación y el método de reconocimiento funcionen, es necesario disponer de varias muestras representativas del individuo (a partir de ahora este parámetro se definirá como \textbf{tamaño de plantilla}).

En el modo supervisado, se cuenta con un directorio con las muestras (recortes de caras y/o cuerpos) agrupadas por subdirectorios para cada usuario. Se inicializa la base de datos con los \glspl{embedding} generados por los modelos de reconocimiento para cada muestra, tras obtener los \glspl{embedding} de todos los usuarios, se entrenan las \acrshort{svm} que compondrán los comités iniciales. El conjunto de positivos de dichos clasificadores se compone de las muestras del propio usuario, mientras que el conjunto de negativos se corresponde con una selección aleatoria de las muestras del resto de individuos.

En el modo no supervisado, se parte puramente de la información de las cámaras en el momento de ejecución del sistema, en el caso de las pruebas realizadas se utilizan los frames de los videos grabados.

El código \ref{coud:training} corresponde al proceso seguido a la hora de crear y entrenar nuevos clasificadores. De todas las muestras de individuos de la base de datos, se \textbf{excluyen las del propio sujeto} en el caso de crear una \acrshort{svm} para un comité existente. Posteriormente, se extrae un subconjunto aleatorio de estas (función random\_pick), que representará al conjunto de muestras negativas del entrenamiento.

A la hora de entrenar las \acrshort{svm}, se asigna la etiqueta 1 (variable plabels) cuando la muestra se corresponde con el conjunto de positivos (variable d1) y -1 (variable labels) cuando corresponde con el de negativos (variable samplesNegative). De esta forma, cuando el clasificador determine que las muestras pertenecen a la clase que representa, devolverá como máximo una puntuación de -1, en caso contrario devolverá un 1 TODO: no sé porqué se comporta al revés. Este comportamiento se ha aplicado simplemente por comodidad, pero la \acrshort{svm} podría entrenarse para que cause el efecto contrario.

%La función comentada se invoca durante la inicialización del sistema y cuando se llama al módulo de actualización, o lo que es lo mismo, cada vez que se necesite crear un nuevo clasificador a registrar en el sistema.

TODO: poner algún extracto de código?


\section{Escalabilidad y tolerancia a fallos de las cámaras}

Cuando se fusionan los resultados procesados por los distintos sensores (nodo integración de sensores en la figura \ref{fig:finalsys}), se requiere que todos ellos se encuentren \textbf{sincronizados} dentro de un intervalo temporal (ejemplo: 100 milisegundos), de forma que las predicciones no se realizan a partir de información desactualizada.

En \cite{andrew} se utiliza una clase del paquete \textit{message\_filters} de \acrshort{ros} llamado \textit{ApproximateTimeSynchronizer}, que utiliza un algoritmo adaptativo para emparejar mensajes a partir de su \gls{timestamp} \cite{ApproximateTime}. El problema de \textit{ApproximateTimeSynchronizer} es que espera recibir datos \textbf{de todas las fuentes en todo momento}, de modo que si uno de los sensores falla, el sistema \textbf{se congela} debido a que se dejan de recibir mensajes de dicha fuente.

Se ha decidido sustituir dicha clase por \textit{MessageFiltersCache} de la misma librería \cite{MFC}. En esta nueva implementación, cada nodo posee una caché en la que se almacenan sus mensajes, que pueden recuperarse especificando un timestamp.

El código \ref{coud:cash} implementa una función (\textit{process}) que recoge los datos de las cachés a una frecuencia fija (ejemplo: 10 Hz), de modo que se recuperan los últimos mensajes de los sensores que emitieron dentro del intervalo, sin esperar por sensores que hayan sufrido latencias o fallos.

Se otorga una ventana de 500 ms para mensajes que llegan con cierto retardo antes de su descarte. Si un nodo no procesa y envía su mensaje antes de acabar el intervalo (ejemplo: 100ms), \textbf{se sigue aplicando el último mensaje recibido de este}, siempre y cuando no se exceda la ventana de tiempo ya comentada.

\section{Migración a ROS 2}
\label{subsec:ROS2}

Con el motivo del fin de soporte de ROS 1 \cite{ROSEOL}, se ha optado por migrar el sistema para ser ejecutado en ROS 2 con el fin de mantener su continuidad.

El proceso de migración se ha llevado a cabo por medio de la guía oficial de \acrshort{ros} \cite{ros2tuto}. Muchos de los problemas encontrados en este proceso están relacionados con los nuevos archivos de configuración.

Se han migrado los nodos cámara e integrador. El proceso incluye cambiar las firmas de las funciones, sus parámetros y cambiar de paquetes. En esencia, la mayoría de paquetes de \acrshort{ros} 2 preservan las mismas funcionalidades e incluso mantienen las mismas interfaces de las funciones de \acrshort{ros} 1, lo que ha facilitado la correcta migración.

La migración del nodo \acrshort{lidar} no ha sido posible, debido a que el código depende de versiones de librerías no soportadas a partir de la versión 22.04 de Ubuntu y de muchos componentes de \acrshort{ros} 1, eliminados en \acrshort{ros} 2. Sería necesario rediseñar todo el código o optar por un paquete de \acrshort{ros} 2 con las mismas funcionalidades que el paquete \textit{hdl\_people\_tracking}, utilizado en \cite{andrew} para detectar personas en nubes de puntos \acrshort{3d}.

En ROS 2 se ha introducido la posibilidad de escribir scripts de lanzamiento en Python. El archivo de lanzamiento del sistema se ha convertido del formato \acrshort{xml} a un script de Python para aprovechar la flexibilidad que el propio lenguaje ofrece y utilizar las nuevas funcionalidades. %Ref a esto: https://docs.ros.org/en/foxy/How-To-Guides/Launch-file-different-formats.html#launch-file-examples